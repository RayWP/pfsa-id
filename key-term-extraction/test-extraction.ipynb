{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3bd324",
   "metadata": {},
   "source": [
    "# Testing on Class Entities Only\n",
    "\n",
    "This notebook loads a pre-trained BERT-style model and evaluates it on a dataset containing only class entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe404151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from sklearn.metrics import classification_report\n",
    "import evaluate\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.metrics import f1_score as seqeval_f1\n",
    "\n",
    "# Define paths\n",
    "model_path = \"BERT-Style-model/microsoft/phi-4-4-epoch-8bs-new\"\n",
    "test_data_path = \"corpus-raymond/test-full-hf-style-tokens-class-only.csv\"\n",
    "output_path = \"class-only-results\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label mappings (same as in training)\n",
    "iob_mapping = {\n",
    "    \"O\": 0,\n",
    "    \"B-class\": 1,\n",
    "    \"I-class\": 2,\n",
    "    \"B-attr\": 3,\n",
    "    \"I-attr\": 4\n",
    "}\n",
    "\n",
    "label_names = ['O', 'B-class', 'I-class', 'B-attr', 'I-attr']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label[5] = \"[PAD]\"\n",
    "label2id[\"[PAD]\"] = 5\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e10702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = pd.read_csv(test_data_path)\n",
    "\n",
    "# Process tokens and IOB tags\n",
    "test_dataset['tokens'] = test_dataset['tokens'].apply(eval)\n",
    "test_dataset['IOB_tag'] = test_dataset['IOB_tag'].apply(eval)\n",
    "\n",
    "print(f\"Loaded {len(test_dataset)} test samples\")\n",
    "print(test_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec947f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function from BERT-Style notebook\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = 5 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(5)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(df, max_length=256):\n",
    "    # Convert Pandas DataFrame to dictionary format\n",
    "    examples = df.to_dict(orient=\"list\")\n",
    "\n",
    "    # Tokenize the input tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True, \n",
    "        padding='max_length', \n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    all_labels = examples[\"IOB_tag\"]\n",
    "    rearranged_labels = []\n",
    "\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        rearranged_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = rearranged_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test dataset\n",
    "tokenized_test = tokenize_and_align_labels(test_dataset)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "test_inputs = torch.tensor(tokenized_test[\"input_ids\"])\n",
    "test_masks = torch.tensor(tokenized_test[\"attention_mask\"])\n",
    "test_labels = torch.tensor(tokenized_test[\"labels\"])\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "batch_size = 8\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "eval_loss = 0\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "    \n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    \n",
    "    eval_loss += outputs.loss.mean().item()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "eval_loss = eval_loss / len(test_dataloader)\n",
    "print(f\"Test loss: {eval_loss:.4f}\")\n",
    "\n",
    "# Convert predictions to flat list of labels\n",
    "pred_tags = [id2label[p_i] for p, l in zip(predictions, true_labels)\n",
    "                             for p_i, l_i in zip(p, l) if id2label[l_i] != \"[PAD]\"]\n",
    "test_tags = [id2label[l_i] for l in true_labels\n",
    "                         for l_i in l if id2label[l_i] != \"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate flat F1 score and classification report\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "f1 = f1_score(test_tags, pred_tags, average='micro')\n",
    "print(f\"Test F1 score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report (token-level):\")\n",
    "print(classification_report(test_tags, pred_tags, digits=4))\n",
    "\n",
    "# Save results to file\n",
    "with open(f\"{output_path}/f1_score.txt\", \"w\") as file:\n",
    "    file.write(f\"Test F1 score: {f1:.4f}\\n\\n\")\n",
    "    file.write(\"Classification Report (token-level):\\n\")\n",
    "    file.write(classification_report(test_tags, pred_tags, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002447b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions and true labels to CSV\n",
    "df_results = pd.DataFrame(list(zip(pred_tags, test_tags)),\n",
    "                      columns=['Predicted', 'True'])\n",
    "df_results.to_csv(f\"{output_path}/test-result-token-level.csv\", index=False)\n",
    "print(f\"Results saved to {output_path}/test-result-token-level.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seqeval evaluation for entity-level metrics\n",
    "seqeval_preds = []\n",
    "seqeval_true = []\n",
    "\n",
    "for pred_seq, label_seq in zip(predictions, true_labels):\n",
    "    pred_tags_seq = []\n",
    "    true_tags_seq = []\n",
    "    for p_i, l_i in zip(pred_seq, label_seq):\n",
    "        if id2label[l_i] != \"[PAD]\":\n",
    "            pred_tags_seq.append(id2label[p_i])\n",
    "            true_tags_seq.append(id2label[l_i])\n",
    "    seqeval_preds.append(pred_tags_seq)\n",
    "    seqeval_true.append(true_tags_seq)\n",
    "\n",
    "# Calculate sequence-level F1 score\n",
    "seq_f1 = seqeval_f1(seqeval_true, seqeval_preds)\n",
    "print(f\"\\nSequence-level F1 score: {seq_f1:.4f}\")\n",
    "print(\"\\nSeqeval Classification Report (entity-level):\")\n",
    "print(seqeval_report(seqeval_true, seqeval_preds))\n",
    "\n",
    "# Save sequence-level results\n",
    "with open(f\"{output_path}/f1_score.txt\", \"a\") as file:\n",
    "    file.write(f\"\\n\\nSequence-level F1 score: {seq_f1:.4f}\\n\\n\")\n",
    "    file.write(\"Seqeval Classification Report (entity-level):\\n\")\n",
    "    file.write(seqeval_report(seqeval_true, seqeval_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f6629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sequence predictions\n",
    "df_seq = pd.DataFrame({\n",
    "    'Predicted': [' '.join(pred) for pred in seqeval_preds],\n",
    "    'True': [' '.join(true) for true in seqeval_true]\n",
    "})\n",
    "df_seq.to_csv(f\"{output_path}/test-result-entity-level.csv\", index=False)\n",
    "print(f\"Entity-level results saved to {output_path}/test-result-entity-level.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e6cc0",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4878ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_tags, pred_tags, labels=label_names)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig(f\"{output_path}/confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot class-wise precision, recall, and F1\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_tags, pred_tags, labels=label_names, average=None\n",
    ")\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "}, index=label_names)\n",
    "\n",
    "# Plot the metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics_df[['Precision', 'Recall', 'F1-Score']].plot(kind='bar')\n",
    "plt.title('Class-wise Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Class')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_path}/class_metrics.png\")\n",
    "plt.show()\n",
    "\n",
    "print(metrics_df)\n",
    "metrics_df.to_csv(f\"{output_path}/class_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652344a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Show some predictions\n",
    "def display_predictions(sample_indices=5):\n",
    "    print(\"\\nSample Predictions:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Take random samples if an integer, otherwise use the provided list of indices\n",
    "    if isinstance(sample_indices, int):\n",
    "        indices = np.random.choice(len(test_dataset), sample_indices, replace=False)\n",
    "    else:\n",
    "        indices = sample_indices\n",
    "    \n",
    "    for idx in indices:\n",
    "        tokens = test_dataset.iloc[idx][\"tokens\"]\n",
    "        true_iob = [label_names[label] for label in test_dataset.iloc[idx][\"IOB_tag\"]]\n",
    "        \n",
    "        # Get predictions for this example\n",
    "        input_ids = test_inputs[idx].unsqueeze(0).to(device)\n",
    "        attention_mask = test_masks[idx].unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        pred_iob = [id2label[p] for p in np.argmax(logits[0], axis=1)[:len(tokens)]]\n",
    "        \n",
    "        # Print aligned tokens and labels\n",
    "        print(f\"Example {idx}:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"{:<20} {:<10} {:<10}\".format(\"Token\", \"True\", \"Predicted\"))\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for token, true, pred in zip(tokens, true_iob, pred_iob):\n",
    "            color = \"\\033[92m\" if true == pred else \"\\033[91m\"  # green if match, red if not\n",
    "            print(\"{:<20} {:<10} {}{}{}\".format(token, true, color, pred, \"\\033[0m\"))\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Display some random examples\n",
    "display_predictions(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
