{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:49.855955Z",
     "start_time": "2025-03-07T14:45:49.695025Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_dataset = pd.read_csv('../corpus-raymond/train-full-hf-style.csv')\n",
    "val_dataset = pd.read_csv('../corpus-raymond/val-full-hf-style.csv')\n",
    "test_dataset = pd.read_csv('../corpus-raymond/test-full-hf-style-tokens.csv')\n",
    "#take column 'tokens' as list\n",
    "train_dataset['tokens'] = train_dataset['tokens'].apply(eval)\n",
    "val_dataset['tokens'] = val_dataset['tokens'].apply(eval)\n",
    "test_dataset['tokens'] = test_dataset['tokens'].apply(eval)\n",
    "\n",
    "#take column 'IOB_tag' as list\n",
    "train_dataset['IOB_tag'] = train_dataset['IOB_tag'].apply(eval)\n",
    "val_dataset['IOB_tag'] = val_dataset['IOB_tag'].apply(eval)\n",
    "test_dataset['IOB_tag'] = test_dataset['IOB_tag'].apply(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afb5042-b1e4-4d13-b5ca-33bbbf644719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>IOB_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[operational, core, specification, ;, 5, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[every, command, must, be, acknowledge, in, a,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Menu, ., statistic, be, no, ouput, ., a, of, ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(, ., ongoing, be, call, voice, if, a, networ...</td>\n",
       "      <td>[0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, internal, (, within, the, IOC, ), implem...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>[this, ., problem, of, case, in, procedure, ma...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>[user, ., order, ascend, the, /, by, result, a...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 3, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>[the, new, user, shall, inherit, all, specify,...</td>\n",
       "      <td>[0, 0, 3, 0, 0, 0, 0, 3, 4, 0, 4, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>[flexible, scheduling, require, the, possibili...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>[this, in, turn, set, quite, stringent, requir...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>939 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tokens  \\\n",
       "0          [operational, core, specification, ;, 5, .]   \n",
       "1    [every, command, must, be, acknowledge, in, a,...   \n",
       "2    [Menu, ., statistic, be, no, ouput, ., a, of, ...   \n",
       "3    [(, ., ongoing, be, call, voice, if, a, networ...   \n",
       "4    [the, internal, (, within, the, IOC, ), implem...   \n",
       "..                                                 ...   \n",
       "934  [this, ., problem, of, case, in, procedure, ma...   \n",
       "935  [user, ., order, ascend, the, /, by, result, a...   \n",
       "936  [the, new, user, shall, inherit, all, specify,...   \n",
       "937  [flexible, scheduling, require, the, possibili...   \n",
       "938  [this, in, turn, set, quite, stringent, requir...   \n",
       "\n",
       "                                               IOB_tag  \n",
       "0                                   [0, 0, 0, 0, 0, 0]  \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, ...  \n",
       "2           [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1]  \n",
       "3    [0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 2, 2, ...  \n",
       "4        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]  \n",
       "..                                                 ...  \n",
       "934  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, ...  \n",
       "935  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 3, 0, ...  \n",
       "936  [0, 0, 3, 0, 0, 0, 0, 3, 4, 0, 4, 0, 0, 0, 0, ...  \n",
       "937  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "938  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[939 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f73d5f4016b9e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:49.896364Z",
     "start_time": "2025-03-07T14:45:49.867960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552deebe74c4634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.040212Z",
     "start_time": "2025-03-07T14:45:50.036210Z"
    }
   },
   "outputs": [],
   "source": [
    "iob_mapping = {\n",
    "    \"O\": 0,\n",
    "    \"B-class\": 1,\n",
    "    \"I-class\": 2,\n",
    "    \"B-attr\": 3,\n",
    "    \"I-attr\": 4\n",
    "}\n",
    "\n",
    "label_names = [ 'O', 'B-class', 'I-class', 'B-attr', 'I-attr' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d341879131b0e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.241226Z",
     "start_time": "2025-03-07T14:45:50.236225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-class', 2: 'I-class', 3: 'B-attr', 4: 'I-attr', 5: '[PAD]'}\n",
      "{'O': 0, 'B-class': 1, 'I-class': 2, 'B-attr': 3, 'I-attr': 4, '[PAD]': 5}\n"
     ]
    }
   ],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label[5] = \"[PAD]\"\n",
    "label2id[\"[PAD]\"] = 5\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1809149bf9d95f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.456197Z",
     "start_time": "2025-03-07T14:45:50.450198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Customer', '.', 'HTTPS', 'with', 'browser', 'web', 'button', 'account', 'customer', 'new', 'sdram', 'MB', '128', ';', 'chip', 'ram', 'Flash', ';', 'GB', ':', 'requirement', 'hardware', 'follow', 'the', 'to', 'adhere', '4', 'Intel', 'XScale', 'PXA270', 'a']\n",
      "[1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "words = train_dataset.iloc[0][\"tokens\"]\n",
    "labels = train_dataset.iloc[0][\"IOB_tag\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(words)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0ad48192d4de0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.891213Z",
     "start_time": "2025-03-07T14:45:50.623559Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DebertaV2TokenizerFast\n",
    "import os\n",
    "from pathlib import Path\n",
    "tokenizer_checkpoint = \"microsoft/phi-4\"\n",
    "model_checkpoint = \"microsoft/phi-4\" \n",
    "folder_name = \"BERT-Style-result/microsoft/phi-4-4-epoch-8bs-new\"\n",
    "model_name_save = \"BERT-Style-model/microsoft/phi-4-4-epoch-8bs-new\"\n",
    "Path(folder_name).mkdir(parents=True, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint, add_prefix_space=True )\n",
    "\n",
    "max_length = 256\n",
    "epochs = 4\n",
    "bs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239159fe7bb3e643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:47:40.251290Z",
     "start_time": "2025-03-07T14:47:40.247348Z"
    }
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = 5 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(5)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd2a750b286ecd95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:47:41.349838Z",
     "start_time": "2025-03-07T14:47:41.344368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [13084, 13, 83454, 4291, 23279, 2984, 2208, 4711, 11296, 943, 13752, 2453, 8578, 4386, 26, 30762, 2453, 24818, 26, 5494, 25, 4408, 479, 69315, 19070, 1820, 998, 329, 6881, 19, 50199, 55, 7092, 47, 60550, 10914, 64], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(train_dataset.iloc[0][\"tokens\"],truncation=True, is_split_into_words=True, padding='do_not_pad', max_length=max_length)\n",
    "labels = train_dataset.iloc[0][\"IOB_tag\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(inputs)\n",
    "# print(labels)\n",
    "# print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efd2fcfe3798a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:51.421549Z",
     "start_time": "2025-03-07T14:29:51.417726Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(df):\n",
    "    # Convert Pandas DataFrame to dictionary format (column-based)\n",
    "    examples = df.to_dict(orient=\"list\")\n",
    "\n",
    "    # Tokenize the input tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='max_length', max_length=max_length\n",
    "    )\n",
    "\n",
    "    all_labels = examples[\"IOB_tag\"]\n",
    "    rearranged_labels = []\n",
    "\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        rearranged_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = rearranged_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65afae5f293a13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:51.915266Z",
     "start_time": "2025-03-07T14:29:51.761477Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenize_and_align_labels(train_dataset)\n",
    "tokenized_val = tokenize_and_align_labels(val_dataset)\n",
    "tokenized_test = tokenize_and_align_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "849d07139f52d337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:52.481705Z",
     "start_time": "2025-03-07T14:29:52.126651Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# convert tokenized train to arrow dataset class\n",
    "train_dataset = Dataset.from_dict(tokenized_train)\n",
    "val_dataset = Dataset.from_dict(tokenized_val)\n",
    "test_dataset = Dataset.from_dict(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161be3fb74d6927f",
   "metadata": {},
   "source": [
    "# Data Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60b4279359a555e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:52.683957Z",
     "start_time": "2025-03-07T14:29:52.678957Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea37710ff2039e",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14f04c19330eeac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:54.825368Z",
     "start_time": "2025-03-07T14:29:53.158363Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a92b04db98538dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:55.027556Z",
     "start_time": "2025-03-07T14:29:55.021810Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536937e8af00143",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e201335ef1af87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:55.671299Z",
     "start_time": "2025-03-07T14:29:55.436307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50692c66bd16497ebfaaf1a9af62725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/802 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616b38e378f74919bd667667ef7971fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97f48bd5ecd42598422eafce6834efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faf3babd8544140bd3041d51a5e842b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ba27240c604ec69c51784dc6df0847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0150e39772e14941a9ff3d733019eeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bb849c61ed44e68ddf3cf7d4ccd9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b096aa6c7fdc4a89bdb57bd5236abcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c117ccb23ef2418486561bdd5febce5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForTokenClassification,AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=len(id2label),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "# weight_decay is a regularization procedure with regard to the weight matrices\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19216f4f9f0ef1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:56.596964Z",
     "start_time": "2025-03-07T14:29:55.819246Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:1\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = \"cuda\"\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438f940b2194e8a",
   "metadata": {},
   "source": [
    "# Preparing Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f110bd1254115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:57.596407Z",
     "start_time": "2025-03-07T14:29:56.798046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to torch tensors\n",
    "train_inputs = torch.tensor(train_dataset[\"input_ids\"])\n",
    "dev_inputs = torch.tensor(val_dataset[\"input_ids\"])\n",
    "test_inputs = torch.tensor(test_dataset[\"input_ids\"])\n",
    "train_tags = torch.tensor(train_dataset[\"labels\"])\n",
    "dev_tags = torch.tensor(val_dataset[\"labels\"])\n",
    "test_tags = torch.tensor(test_dataset[\"labels\"])\n",
    "train_masks = torch.tensor(train_dataset[\"attention_mask\"])\n",
    "dev_masks = torch.tensor(val_dataset[\"attention_mask\"])\n",
    "test_masks = torch.tensor(test_dataset[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e090fa3de219e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:57.824872Z",
     "start_time": "2025-03-07T14:29:57.820878Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "\n",
    "# We define the dataloaders. \n",
    "# Shuffle the data for training using RandomSampler\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "# Load dev and test data sequentially with SequentialSampler.\n",
    "dev_data = TensorDataset(dev_inputs, dev_masks, dev_tags)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=bs)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef97bc0b87326df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:58.004324Z",
     "start_time": "2025-03-07T14:29:58.000310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import a scheduler to reduce the learning rate \n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs; the BERT paper uses 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68aa2ac29568db",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96e2008ecec57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:30:09.818329800Z",
     "start_time": "2025-03-07T14:30:03.696195Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import trange\n",
    "\n",
    "# To measure execution time of this cell\n",
    "\n",
    "# Train the model for; the BERT paper uses 4\n",
    "## Store the average loss after each epoch; these values are used to plot the loss.\n",
    "loss_values, development_loss_values = [], []\n",
    "\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "    # Set the model into training mode\n",
    "    model.train()\n",
    "    # Reset the total loss for each epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Transfer batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Remove previous gradients before each backward pass\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This returns the loss (not the model output) since we have input the labels.\n",
    "        outputs = model(b_input_ids,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Get the loss\n",
    "        loss = outputs[0]\n",
    "        # Backward pass to compute the gradients\n",
    "        loss.backward()\n",
    "        # Train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "\n",
    "    # Store each loss value for plotting the learning curve afterwards\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    # After each training epoch, measure performance on development set\n",
    "\n",
    "    # Set the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the development loss for this epoch\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in dev_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # The model must not compute or save gradients, in order to save memory and speed up this step\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, compute predictions\n",
    "            # This will return the logits (logarithm of the odds), not the loss (we do not provide labels)\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Transfer logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Compute the accuracy for this batch of development sentences\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "        #data_seqeval[\"batch\"].append(str(batch))\n",
    "        #data_seqeval[\"true_tags\"].append(str(label_ids))\n",
    "        #data_seqeval[\"predicted_tags\"].append(str([list(p) for p in np.argmax(logits, axis=2)]))\n",
    "\n",
    "    #df_seqeval = pd.DataFrame(data_seqeval)\n",
    "    #wandb.log({f\"dataframe_seqeval\": wandb.Table(dataframe=df_seqeval)})\n",
    "    \n",
    "    eval_loss = eval_loss / len(dev_dataloader)\n",
    "    development_loss_values.append(eval_loss)\n",
    "    print(\"Development loss: {}\".format(eval_loss))\n",
    "    pred_tags = [id2label[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if id2label[l_i] != \"[PAD]\"]\n",
    "    dev_tags = [id2label[l_i] for l in true_labels\n",
    "                                  for l_i in l if id2label[l_i] != \"[PAD]\"]\n",
    "    f1 = f1_score(pred_tags, dev_tags, average='micro')\n",
    "\n",
    "    # Format output with 4 decimal places\n",
    "    output_text = \"train-val F1 score: {:.4f}\\n\".format(f1)\n",
    "\n",
    "    # Print to console\n",
    "    print(output_text)\n",
    "\n",
    "    # Save to a text file\n",
    "    with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
    "        file.write(output_text)\n",
    "    #print(\"Development classification report:\\n{}\".format(classification_report(pred_tags, dev_tags,digits=4)))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f51cd3e2dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(pred_tags, dev_tags)),\n",
    "               columns =['Pred', 'True'])\n",
    "df.to_csv(folder_name + '/train-val-result-bert.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5b5bf726c34c1",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa54a7b1f8075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(development_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95241329f809f83f",
   "metadata": {},
   "source": [
    "# Testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a911e1ce59aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the test set\n",
    "# Set again the model into evaluation mode\n",
    "model.eval()\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "input_ids_list = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # The model must not compute or store gradients\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate predictions.\n",
    "        outputs = model(b_input_ids,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "    # Transfer logits and labels to CPU\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    input_ids_list.extend(b_input_ids)\n",
    "    \n",
    "    # Calculate the accuracy for this batch of test sentences\n",
    "    eval_loss += outputs[0].mean().item()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "pred_tags = [id2label[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if id2label[l_i] != \"[PAD]\"]\n",
    "test_tags = [id2label[l_i] for l in true_labels\n",
    "                                  for l_i in l if id2label[l_i] != \"[PAD]\"]\n",
    "#print(str(pred_tags))\n",
    "#print(str(test_tags))\n",
    "f1 = f1_score(pred_tags, test_tags, average='micro')\n",
    "\n",
    "# Format output with 4 decimal places\n",
    "output_text = \"Test F1 score: {:.4f}\\n\".format(f1)\n",
    "\n",
    "# Print to console\n",
    "print(output_text)\n",
    "\n",
    "# Save to a text file\n",
    "with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
    "    file.write(output_text)\n",
    "#print(\"Test classification report: {}\".format(classification_report(pred_tags, test_tags,digits=4)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42cd76-91e2-4530-a62c-c3e32a937a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(pred_tags, test_tags)),\n",
    "               columns =['Pred', 'True'])\n",
    "df.to_csv(folder_name + '/test-result-bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc31b3-c79a-4066-bf98-bc1d9bc4186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(folder_name+\"/test-result-bert.csv\")\n",
    "\n",
    "# Extract true and predicted labels\n",
    "y_true = df[\"True\"]\n",
    "y_pred = df[\"Pred\"]\n",
    "\n",
    "# Define the target classes\n",
    "target_classes = [\"B-class\", \"I-class\", \"B-attr\", \"I-attr\", \"O\"]\n",
    "\n",
    "# Compute precision, recall, and F1-score for the specified classes\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=target_classes, zero_division=0)\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Create a results dictionary\n",
    "metrics = pd.DataFrame({\n",
    "    \"Class\": target_classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1-Score\": f1\n",
    "})\n",
    "\n",
    "# Add overall accuracy\n",
    "metrics.loc[len(metrics)] = [\"Overall Accuracy\", accuracy, accuracy, accuracy]\n",
    "\n",
    "# Save metrics to a text file\n",
    "with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
    "    file.write(metrics.to_string(index=False) + \"\\n\")\n",
    "\n",
    "# Display results\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94be0102f039b7",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce2b962f166c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241bd72f66c1bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name_save)\n",
    "tokenizer.save_pretrained(model_name_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ce5ff-5ff5-42b5-ac6c-95ca0b6220c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "from seqeval.scheme import IOB2  # or another scheme depending on your dataset\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "input_ids_list = []\n",
    "eval_loss = 0  # Make sure this is initialized\n",
    "\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    input_ids_list.extend(b_input_ids)\n",
    "\n",
    "    eval_loss += outputs[0].mean().item()\n",
    "\n",
    "    # Convert predictions and labels to label strings\n",
    "    batch_preds = np.argmax(logits, axis=2)\n",
    "    \n",
    "    for pred_seq, label_seq in zip(batch_preds, label_ids):\n",
    "        pred_tags_seq = []\n",
    "        true_tags_seq = []\n",
    "        for p_i, l_i in zip(pred_seq, label_seq):\n",
    "            if id2label[l_i] != \"[PAD]\":\n",
    "                pred_tags_seq.append(id2label[p_i])\n",
    "                true_tags_seq.append(id2label[l_i])\n",
    "        predictions.append(pred_tags_seq)\n",
    "        true_labels.append(true_tags_seq)\n",
    "\n",
    "# Compute F1 score using seqeval\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "output_text = \"Test F1 score (seqeval): {:.4f}\\n\".format(f1)\n",
    "\n",
    "print(output_text)\n",
    "\n",
    "# Save to a text file\n",
    "with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
    "    file.write(output_text)\n",
    "\n",
    "# Print full classification report\n",
    "print(\"Seqeval classification report:\")\n",
    "print(classification_report(true_labels, predictions, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb46c42-716e-4291-8000-228aa1e1cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(predictions, true_labels)),\n",
    "               columns =['Pred', 'True'])\n",
    "df.to_csv(folder_name + '/test-result-bert-seqeval.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
