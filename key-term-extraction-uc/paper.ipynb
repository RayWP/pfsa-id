{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83f907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RAYMOND\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RAYMOND\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Output written to output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from typing import List\n",
    "import re\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n",
    "from seqeval.metrics import classification_report, f1_score as seqeval_f1_score\n",
    "import ast\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define label names as used in BERT-Style.ipynb\n",
    "label_names = ['O', 'B-actor', 'I-actor', 'B-usecase', 'I-usecase', '[PAD]']\n",
    "pad_token_label_id = 5  # Index of [PAD] in label_names\n",
    "\n",
    "# Sample heuristic-based tagging function\n",
    "def iob_tag_sentence(sentence: str) -> (List[str], List[int]):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    iob_tags = []\n",
    "    actor_started = False\n",
    "    usecase_started = False\n",
    "\n",
    "    for word, tag in pos_tags:\n",
    "        # Heuristic patterns\n",
    "        if tag in ['NNP', 'NNPS'] and not actor_started:\n",
    "            # Begin Actor\n",
    "            iob_tags.append(1)  # B-Actor\n",
    "            actor_started = True\n",
    "            usecase_started = False\n",
    "        elif tag in ['NNP', 'NNPS'] and actor_started:\n",
    "            # Inside Actor\n",
    "            iob_tags.append(2)  # I-Actor\n",
    "        elif tag in ['VB', 'VBP', 'VBZ', 'VBD', 'VBN'] and not usecase_started:\n",
    "            # Begin Usecase\n",
    "            iob_tags.append(3)  # B-Usecase\n",
    "            actor_started = False\n",
    "            usecase_started = True\n",
    "        elif tag in ['NN', 'VB', 'VBP', 'VBG', 'VBD', 'VBN'] and usecase_started:\n",
    "            # Inside Usecase\n",
    "            iob_tags.append(4)  # I-Usecase\n",
    "        else:\n",
    "            # Outside\n",
    "            iob_tags.append(0)\n",
    "            actor_started = False\n",
    "            usecase_started = False\n",
    "\n",
    "    return tokens, iob_tags\n",
    "\n",
    "# Main processing function\n",
    "def process_sentences(sentences: List[str], output_csv='output.csv'):\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"tokens\", \"IOB_tag\"])\n",
    "        writer.writeheader()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens, tags = iob_tag_sentence(sentence)\n",
    "            writer.writerow({\n",
    "                \"tokens\": tokens,\n",
    "                \"IOB_tag\": tags\n",
    "            })\n",
    "\n",
    "    print(f\"Inference complete. Output written to {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade09e0",
   "metadata": {},
   "source": [
    "# Reading and Processing Test Data from BERT-Style Dataset\n",
    "\n",
    "This section loads the test dataset used in BERT-Style.ipynb and processes it with our rules-based system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_dataset = pd.read_csv('../corpus-raymond/usecase-test-hf.csv')\n",
    "\n",
    "# Preview the test data\n",
    "print(\"First few rows of the test dataset:\")\n",
    "print(test_dataset.head())\n",
    "print(f\"\\nTotal examples in test dataset: {len(test_dataset)}\")\n",
    "\n",
    "# Check if 'tokens' and 'IOB_tag' are already evaluated lists or need to be converted from strings\n",
    "sample_tokens = test_dataset['tokens'].iloc[0]\n",
    "if isinstance(sample_tokens, str):\n",
    "    print(\"\\nConverting string representations to Python lists...\")\n",
    "    test_dataset['tokens'] = test_dataset['tokens'].apply(ast.literal_eval)\n",
    "    test_dataset['IOB_tag'] = test_dataset['IOB_tag'].apply(ast.literal_eval)\n",
    "else:\n",
    "    print(\"\\nTokens and IOB_tags are already in list format.\")\n",
    "\n",
    "# Show an example\n",
    "print(\"\\nExample tokens and their IOB tags from the test dataset:\")\n",
    "example_idx = 0\n",
    "print(f\"Tokens: {test_dataset['tokens'].iloc[example_idx]}\")\n",
    "print(f\"True IOB tags: {test_dataset['IOB_tag'].iloc[example_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9020e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data using our rule-based tagger\n",
    "def process_test_data(test_df):\n",
    "    \"\"\"\n",
    "    Process test data with our rule-based tagger to generate predictions.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with original tokens, true IOB tags, and predicted IOB tags\n",
    "    - List of token-level true tags and predicted tags for evaluation\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    token_level_true = []\n",
    "    token_level_pred = []\n",
    "    sequence_level_true = []\n",
    "    sequence_level_pred = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        true_tags = row['IOB_tag']\n",
    "        \n",
    "        # Reconstruct sentence from tokens (simple space-joining)\n",
    "        sentence = \" \".join(tokens)\n",
    "        \n",
    "        # Get predicted tags using our rule-based system\n",
    "        _, pred_tags = iob_tag_sentence(sentence)\n",
    "        \n",
    "        # Make sure pred_tags is the same length as tokens\n",
    "        # (in case tokenization differs)\n",
    "        if len(pred_tags) > len(tokens):\n",
    "            pred_tags = pred_tags[:len(tokens)]\n",
    "        elif len(pred_tags) < len(tokens):\n",
    "            pred_tags.extend([0] * (len(tokens) - len(pred_tags)))\n",
    "        \n",
    "        # Save results\n",
    "        results.append({\n",
    "            'tokens': tokens,\n",
    "            'true_tags': true_tags,\n",
    "            'pred_tags': pred_tags\n",
    "        })\n",
    "        \n",
    "        # Collect token-level tags for evaluation\n",
    "        token_level_true.extend([label_names[tag] for tag in true_tags])\n",
    "        token_level_pred.extend([label_names[tag] for tag in pred_tags])\n",
    "        \n",
    "        # Collect sequence-level tags for seqeval evaluation\n",
    "        seq_true = [label_names[tag] for tag in true_tags]\n",
    "        seq_pred = [label_names[tag] for tag in pred_tags]\n",
    "        sequence_level_true.append(seq_true)\n",
    "        sequence_level_pred.append(seq_pred)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, token_level_true, token_level_pred, sequence_level_true, sequence_level_pred\n",
    "\n",
    "# Process the test data\n",
    "results_df, token_true, token_pred, seq_true, seq_pred = process_test_data(test_dataset)\n",
    "\n",
    "# Preview results\n",
    "print(\"Preview of our rule-based tagger results:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Save results to CSV\n",
    "output_path = \"rule_based_results.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378756ab",
   "metadata": {},
   "source": [
    "# Evaluation of Rule-Based Tagger\n",
    "\n",
    "This section evaluates our rule-based tagger using the same metrics as in BERT-Style.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e44786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token-level evaluation metrics\n",
    "token_f1 = f1_score(token_true, token_pred, average='micro')\n",
    "print(f\"Token-level F1 score: {token_f1:.4f}\")\n",
    "\n",
    "# Compute precision, recall and f1 for each class\n",
    "target_classes = [\"B-actor\", \"I-actor\", \"B-usecase\", \"I-usecase\", \"O\"]\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    token_true, token_pred, labels=target_classes, zero_division=0\n",
    ")\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = accuracy_score(token_true, token_pred)\n",
    "\n",
    "# Create a results DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": target_classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1-Score\": f1\n",
    "})\n",
    "\n",
    "# Add overall accuracy\n",
    "metrics_df.loc[len(metrics_df)] = [\"Overall Accuracy\", accuracy, accuracy, accuracy]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nToken-level classification report:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Save to a text file\n",
    "with open(\"rule_based_metrics.txt\", \"w\") as file:\n",
    "    file.write(f\"Token-level F1 score: {token_f1:.4f}\\n\\n\")\n",
    "    file.write(metrics_df.to_string(index=False) + \"\\n\")\n",
    "\n",
    "# Compute sequence-level evaluation using seqeval\n",
    "print(\"\\nSequence-level evaluation using seqeval:\")\n",
    "seq_f1 = seqeval_f1_score(seq_true, seq_pred)\n",
    "print(f\"Seqeval F1 score: {seq_f1:.4f}\")\n",
    "\n",
    "# Print seqeval classification report\n",
    "print(\"\\nSeqeval classification report:\")\n",
    "seqeval_report = classification_report(seq_true, seq_pred, digits=4)\n",
    "print(seqeval_report)\n",
    "\n",
    "# Append to the metrics file\n",
    "with open(\"rule_based_metrics.txt\", \"a\") as file:\n",
    "    file.write(f\"\\nSeqeval F1 score: {seq_f1:.4f}\\n\\n\")\n",
    "    file.write(\"Seqeval classification report:\\n\")\n",
    "    file.write(seqeval_report)\n",
    "\n",
    "print(f\"Metrics saved to rule_based_metrics.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c43c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with BERT-Style results \n",
    "# You can load the saved results from BERT-Style.ipynb for direct comparison\n",
    "\n",
    "def load_bert_results(result_path):\n",
    "    try:\n",
    "        bert_results = pd.read_csv(result_path)\n",
    "        print(f\"Loaded BERT-Style results from {result_path}\")\n",
    "        return bert_results\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load BERT-Style results: {e}\")\n",
    "        return None\n",
    "\n",
    "# Path to BERT-Style results (adjust as needed)\n",
    "bert_result_path = \"BERT-Style-result/microsoft/deberta-v3-base-4-epoch-8bs-new/test-result-bert.csv\"\n",
    "bert_results = load_bert_results(bert_result_path)\n",
    "\n",
    "if bert_results is not None:\n",
    "    # Compare F1 scores\n",
    "    bert_f1 = f1_score(bert_results['True'], bert_results['Pred'], average='micro')\n",
    "    print(f\"\\nBERT-Style F1 score: {bert_f1:.4f}\")\n",
    "    print(f\"Rule-based F1 score: {token_f1:.4f}\")\n",
    "    print(f\"Difference: {bert_f1 - token_f1:.4f}\")\n",
    "    \n",
    "    # Save comparison \n",
    "    with open(\"model_comparison.txt\", \"w\") as file:\n",
    "        file.write(f\"BERT-Style F1 score: {bert_f1:.4f}\\n\")\n",
    "        file.write(f\"Rule-based F1 score: {token_f1:.4f}\\n\")\n",
    "        file.write(f\"Difference: {bert_f1 - token_f1:.4f}\\n\")\n",
    "    \n",
    "    print(\"Comparison saved to model_comparison.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbed2af",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "This section analyzes where our rule-based system made errors compared to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bf61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze errors\n",
    "def analyze_errors(results_df):\n",
    "    error_examples = []\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        true_tags = row['true_tags']\n",
    "        pred_tags = row['pred_tags']\n",
    "        \n",
    "        # Check if there's any mismatch in the tags\n",
    "        has_error = False\n",
    "        for t, p in zip(true_tags, pred_tags):\n",
    "            if t != p:\n",
    "                has_error = True\n",
    "                break\n",
    "                \n",
    "        if has_error:\n",
    "            # Format the error example\n",
    "            token_tag_pairs = []\n",
    "            for token, true_tag, pred_tag in zip(tokens, true_tags, pred_tags):\n",
    "                is_error = true_tag != pred_tag\n",
    "                tag_info = f\"{token} ({label_names[true_tag]} -> {label_names[pred_tag]})\"\n",
    "                if is_error:\n",
    "                    tag_info = f\"**{tag_info}**\"  # Mark errors with bold\n",
    "                token_tag_pairs.append(tag_info)\n",
    "            \n",
    "            error_examples.append({\n",
    "                'example_id': idx,\n",
    "                'tokens_with_errors': token_tag_pairs,\n",
    "                'sentence': \" \".join(tokens)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(error_examples)\n",
    "\n",
    "# Analyze errors\n",
    "error_df = analyze_errors(results_df)\n",
    "\n",
    "# Save top errors to a file\n",
    "with open(\"error_analysis.txt\", \"w\") as file:\n",
    "    file.write(f\"Total examples with errors: {len(error_df)} out of {len(results_df)} ({len(error_df)/len(results_df)*100:.2f}%)\\n\\n\")\n",
    "    \n",
    "    # Print the first 10 errors\n",
    "    for i, row in error_df.head(10).iterrows():\n",
    "        file.write(f\"Example {row['example_id']}:\\n\")\n",
    "        file.write(f\"Sentence: {row['sentence']}\\n\")\n",
    "        file.write(\"Tokens with errors (true -> predicted):\\n\")\n",
    "        for tag_info in row['tokens_with_errors']:\n",
    "            file.write(f\"  {tag_info}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "print(f\"Error analysis saved to error_analysis.txt\")\n",
    "print(f\"Total examples with errors: {len(error_df)} out of {len(results_df)} ({len(error_df)/len(results_df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17facce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    example_sentences = [\n",
    "        \"The doctor schedules an appointment for the patient.\",\n",
    "        \"The system notifies the user of new messages.\",\n",
    "        \"Admin updates the records weekly.\"\n",
    "    ]\n",
    "    process_sentences(example_sentences, \"example_output.csv\")\n",
    "    print(\"\\nExample predictions on sample sentences:\")\n",
    "    \n",
    "    for sentence in example_sentences:\n",
    "        tokens, tags = iob_tag_sentence(sentence)\n",
    "        tag_names = [label_names[tag] for tag in tags]\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Tags: {tag_names}\")\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
