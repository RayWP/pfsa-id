{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-07T06:07:46.048270Z",
     "start_time": "2025-03-07T06:07:45.842988Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "train_dataset = pd.read_csv('../data/train-full-hf-style.csv')\n",
    "val_dataset = pd.read_csv('../data/val-full-hf-style.csv')\n",
    "test_dataset = pd.read_csv('../data/test-full-hf-style.csv')\n",
    "#take column 'tokens' as list\n",
    "train_dataset['tokens'] = train_dataset['tokens'].apply(eval)\n",
    "val_dataset['tokens'] = val_dataset['tokens'].apply(eval)\n",
    "test_dataset['tokens'] = test_dataset['tokens'].apply(eval)\n",
    "\n",
    "#take column 'IOB_tag' as list\n",
    "train_dataset['IOB_tag'] = train_dataset['IOB_tag'].apply(eval)\n",
    "val_dataset['IOB_tag'] = val_dataset['IOB_tag'].apply(eval)\n",
    "test_dataset['IOB_tag'] = test_dataset['IOB_tag'].apply(eval)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:07:46.087264Z",
     "start_time": "2025-03-07T06:07:46.056359Z"
    }
   },
   "cell_type": "code",
   "source": "!python --version",
   "id": "28f73d5f4016b9e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:07:46.174976Z",
     "start_time": "2025-03-07T06:07:46.170121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "iob_mapping = {\n",
    "    \"O\": 0,\n",
    "    \"B-Class\": 1,\n",
    "    \"I-Class\": 2,\n",
    "    \"B-attr\": 3,\n",
    "    \"I-attr\": 4\n",
    "}\n",
    "\n",
    "label_names = [ 'O', 'B-Class', 'I-Class', 'B-attr', 'I-attr' ]"
   ],
   "id": "552deebe74c4634",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:07:57.442131Z",
     "start_time": "2025-03-07T06:07:57.438248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = train_dataset.iloc[0][\"tokens\"]\n",
    "labels = train_dataset.iloc[0][\"IOB_tag\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ],
   "id": "c1809149bf9d95f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". HTTPS with browser web button account customer new sdram MB 128 ; chip ram Flash ; GB : requirement hardware follow the to adhere 4 Intel XScale PXA270 a Register \n",
      "O O     O    O       O   O      I-attr  I-attr   O   O     O  O   O O    O   O     O O  O O           O        O      O   O  O      O O     O      O      O O        \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:09:05.912486Z",
     "start_time": "2025-03-07T06:07:57.706988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "id": "ca0ad48192d4de0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "X:\\language\\python\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:09:06.609271Z",
     "start_time": "2025-03-07T06:09:06.597216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ],
   "id": "239159fe7bb3e643",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:09:06.710248Z",
     "start_time": "2025-03-07T06:09:06.684204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(val_dataset.iloc[0][\"tokens\"], is_split_into_words=True)\n",
    "labels = val_dataset.iloc[0][\"IOB_tag\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ],
   "id": "fd2a750b286ecd95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:09:06.760737Z",
     "start_time": "2025-03-07T06:09:06.752632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_and_align_labels(df):\n",
    "    # Convert Pandas DataFrame to dictionary format (column-based)\n",
    "    examples = df.to_dict(orient=\"list\")\n",
    "\n",
    "    # Tokenize the input tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = examples[\"IOB_tag\"]\n",
    "    rearranged_labels = []\n",
    "\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        rearranged_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = rearranged_labels\n",
    "    return tokenized_inputs\n"
   ],
   "id": "5efd2fcfe3798a38",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:09:07.170343Z",
     "start_time": "2025-03-07T06:09:06.800008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_train = tokenize_and_align_labels(train_dataset)\n",
    "tokenized_val = tokenize_and_align_labels(val_dataset)\n",
    "tokenized_test = tokenize_and_align_labels(test_dataset)"
   ],
   "id": "e65afae5f293a13b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:12:46.272856Z",
     "start_time": "2025-03-07T06:12:46.208134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "# convert tokenized train to arrow dataset class\n",
    "train_dataset = Dataset.from_dict(tokenized_train)\n",
    "val_dataset = Dataset.from_dict(tokenized_val)\n",
    "test_dataset = Dataset.from_dict(tokenized_test)"
   ],
   "id": "849d07139f52d337",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Collation",
   "id": "161be3fb74d6927f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:12:47.888412Z",
     "start_time": "2025-03-07T06:12:47.885215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ],
   "id": "e60b4279359a555e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:13:24.935598Z",
     "start_time": "2025-03-07T06:13:24.894429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = data_collator([train_dataset[i] for i in range(2)])\n",
    "batch"
   ],
   "id": "cacb378ce04a4861",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   119,   145, 20174, 10197,  1114, 19089,  5127,  6324,  3300,\n",
       "          8132,  1207,   188,  7412,  1306, 19443, 11965,   132, 11451, 26084,\n",
       "         12346,   132, 17909,   131,  8875,  8172,  2812,  1103,  1106,  8050,\n",
       "         12807,   125, 15397,   161,  1708, 20532,   153,  3190,  1592, 24458,\n",
       "          1568,   170,  4273,   102],\n",
       "        [  101,   119,  1888,  1104,  6453,  1137,  1888,  2747,   170, 12726,\n",
       "          1106,  2222,  1165,  4795,  1106,  1103,  1106,  1329,  1129,  1209,\n",
       "          1449,  1103,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,   -1,   -1,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0, -100],\n",
       "        [-100,    0,   -1,   -1,   -1,    0,   -1,    0,    0,    0,    0,    0,\n",
       "            0,   -1,    0,    0,    0,    0,    0,    0,   -1,    0, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Metrics",
   "id": "e2ea37710ff2039e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ],
   "id": "a14f04c19330eeac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:15:38.014566Z",
     "start_time": "2025-03-07T06:15:38.010152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ],
   "id": "1a92b04db98538dd",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the model",
   "id": "a536937e8af00143"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T06:16:12.610093Z",
     "start_time": "2025-03-07T06:16:12.605346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label)\n",
    "print(label2id)"
   ],
   "id": "4d341879131b0e94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Class', 2: 'I-Class', 3: 'B-attr', 4: 'I-attr'}\n",
      "{'O': 0, 'B-Class': 1, 'I-Class': 2, 'B-attr': 3, 'I-attr': 4}\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ],
   "id": "17e201335ef1af87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine Tuning",
   "id": "d438f940b2194e8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-base-cased-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ],
   "id": "49db9c317883dc03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ],
   "id": "ba87bbb4fa4c7227"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "d8c929361e86478d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
