{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction And Attribution Of Public Figures Statements For Journalism In Indonesia Using Deep Learning\n",
        "\n",
        "## Objectives\n",
        "To develop a single pipeline model for the extraction and attribution of public figures statement in Indonesian news articles using deep learning algorithms"
      ],
      "metadata": {
        "id": "mf73iaRVXL_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXP at Tesla P4 - Weight and Bias with More Data 70-30 V2"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Python Library"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "import gensim\n",
        "import torch\n",
        "import torchcrf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torchtext.data import Field, NestedField, BucketIterator\n",
        "from torchtext.datasets import SequenceTaggingDataset\n",
        "from torchtext.vocab import Vocab\n",
        "from collections import Counter\n",
        "from spacy.lang.id import Indonesian\n",
        "from sklearn.metrics import f1_score, classification_report, precision_recall_fscore_support\n",
        "from torch_lr_finder import LRFinder"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:29.834905Z",
          "start_time": "2025-02-27T08:18:29.820425Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "OQ2wlfYDGPiA",
        "outputId": "cf45e908-446f-4cf4-aea4-415b784e6942",
        "gather": {
          "logged": 1740733765520
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking GPU Availability"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "available_gpu = torch.cuda.is_available()\n",
        "if available_gpu:\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(1)}\")\n",
        "    use_device = torch.device(\"cuda:1\")\n",
        "else:\n",
        "    use_device = torch.device(\"cpu\")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:29.865802Z",
          "start_time": "2025-02-27T08:18:29.848619Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EC-uewkVGSrx",
        "outputId": "51fa1257-6624-42fe-fea9-63a54d606b8e",
        "gather": {
          "logged": 1740733765781
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_device"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "device(type='cpu')"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:29.896800Z",
          "start_time": "2025-02-27T08:18:29.890799Z"
        },
        "gather": {
          "logged": 1740733765962
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weight and Bias Integration to Track the Experiments"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "wandb_api_key = os.getenv(\"WANDB_API_KEY\", \"\")\n",
        "wandb.login(key = wandb_api_key)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() without arguments from jupyter should prompt you for an api key.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:29.959855Z",
          "start_time": "2025-02-27T08:18:29.936847Z"
        },
        "gather": {
          "logged": 1740733766197
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"pfsa-id-teslap4-7030-v2\",entity=\"r76127011-wkdl\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://app.wandb.ai/r76127011-wkdl/pfsa-id-teslap4-7030-v2\" target=\"_blank\">https://app.wandb.ai/r76127011-wkdl/pfsa-id-teslap4-7030-v2</a><br/>\n                Run page: <a href=\"https://app.wandb.ai/r76127011-wkdl/pfsa-id-teslap4-7030-v2/runs/qvc80idw\" target=\"_blank\">https://app.wandb.ai/r76127011-wkdl/pfsa-id-teslap4-7030-v2/runs/qvc80idw</a><br/>\n            "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.19.7 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "W&B Run: https://app.wandb.ai/r76127011-wkdl/pfsa-id-teslap4-7030-v2/runs/qvc80idw"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.275660Z",
          "start_time": "2025-02-27T08:18:30.062948Z"
        },
        "gather": {
          "logged": 1740733772301
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_metrics_train=[\"model\",\"epoch\",\"train_loss\",\"train_p\",\"train_r\",\"train_f1\",\"val_loss\",\"val_p\",\"val_r\",\"val_f1\"]\n",
        "data_table_metrics_train = wandb.Table(columns=columns_metrics_train)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.369682Z",
          "start_time": "2025-02-27T08:18:36.352663Z"
        },
        "gather": {
          "logged": 1740733772526
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_metrics_test=[\"model\",\"test_loss\",\"test_p\",\"test_r\",\"test_f1\",\"best_epoch\",\"best_val_f1\",\"ellapse_train_times\"]\n",
        "data_table_metrics_test = wandb.Table(columns=columns_metrics_test)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.496038Z",
          "start_time": "2025-02-27T08:18:36.482036Z"
        },
        "gather": {
          "logged": 1740733772712
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Random Seed"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "if available_gpu:\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.enabled = False "
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.527554Z",
          "start_time": "2025-02-27T08:18:36.514555Z"
        },
        "gather": {
          "logged": 1740733772878
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus Loader\n",
        "\n",
        "Setting the path location of corpus and class declaration that used for loading the corpus and converting the field into vocabulary list."
      ],
      "metadata": {
        "id": "iYkejIkFF1Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "DRIVE_ROOT = \"corpus-raymond\"\n",
        "if DRIVE_ROOT not in sys.path:\n",
        "    sys.path.append(DRIVE_ROOT)\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.558559Z",
          "start_time": "2025-02-27T08:18:36.545554Z"
        },
        "id": "uz245NlJV9_7",
        "gather": {
          "logged": 1740733773027
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "class Corpus(object):\n",
        "\n",
        "    def __init__(self, input_folder, min_word_freq, batch_size, wv_file=None):\n",
        "        # list all the fields\n",
        "        self.word_field = Field(lower=True)  # [sent len, batch_size]\n",
        "        self.tag_field = Field(\n",
        "            \n",
        "            unk_token=None)  # [sent len, batch_size]\n",
        "        # Character-level input\n",
        "        self.char_nesting_field = Field(tokenize=list)\n",
        "        self.char_field = NestedField(self.char_nesting_field)  # [batch_size, sent len, max len char]\n",
        "        # create dataset using built-in parser from torchtext\n",
        "        self.train_dataset, self.val_dataset, self.test_dataset = SequenceTaggingDataset.splits(\n",
        "            path=input_folder,\n",
        "            train=\"train-full.txt\",\n",
        "            validation=\"validation-full.txt\",\n",
        "            test=\"validation-full.txt\",\n",
        "            fields=(\n",
        "                ((\"word\", \"char\"), (self.word_field, self.char_field)),\n",
        "                (\"tag\", self.tag_field)\n",
        "            )\n",
        "        )\n",
        "        # convert fields to vocabulary list\n",
        "        if wv_file:\n",
        "            self.wv_model = gensim.models.word2vec.Word2Vec.load(wv_file)\n",
        "            self.embedding_dim = self.wv_model.vector_size\n",
        "            word_freq = {word: self.wv_model.wv.vocab[word].count for word in self.wv_model.wv.vocab}\n",
        "            word_counter = Counter(word_freq)\n",
        "            self.word_field.vocab = Vocab(word_counter, min_freq=min_word_freq)\n",
        "            vectors = []\n",
        "            for word, idx in self.word_field.vocab.stoi.items():\n",
        "                if word in self.wv_model.wv.vocab.keys():\n",
        "                    vectors.append(torch.as_tensor(self.wv_model.wv[word].tolist()))\n",
        "                else:\n",
        "                    vectors.append(torch.zeros(self.embedding_dim))\n",
        "            self.word_field.vocab.set_vectors(\n",
        "                stoi=self.word_field.vocab.stoi,\n",
        "                vectors=vectors,\n",
        "                dim=self.embedding_dim\n",
        "            )\n",
        "        else:\n",
        "            self.wv_model = api.load(\"word2vec-google-news-300\")\n",
        "            self.embedding_dim = self.wv_model.vector_size\n",
        "            word_freq = {word: self.wv_model.wv.vocab[word].count for word in self.wv_model.wv.vocab}\n",
        "            word_counter = Counter(word_freq)\n",
        "            self.word_field.vocab = Vocab(word_counter, min_freq=min_word_freq)\n",
        "            vectors = []\n",
        "            for word, idx in self.word_field.vocab.stoi.items():\n",
        "                if word in self.wv_model.wv.vocab.keys():\n",
        "                    vectors.append(torch.as_tensor(self.wv_model.wv[word].tolist()))\n",
        "                else:\n",
        "                    vectors.append(torch.zeros(self.embedding_dim))\n",
        "            self.word_field.vocab.set_vectors(\n",
        "                stoi=self.word_field.vocab.stoi,\n",
        "                vectors=vectors,\n",
        "                dim=self.embedding_dim\n",
        "            )\n",
        "\n",
        "        # build vocab for tag and characters\n",
        "        self.char_field.build_vocab(self.train_dataset.char)\n",
        "        self.tag_field.build_vocab(self.train_dataset.tag)\n",
        "        # create iterator for batch input\n",
        "        self.train_iter, self.val_iter, self.test_iter = BucketIterator.splits(\n",
        "            datasets=(self.train_dataset, self.val_dataset, self.test_dataset),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        # prepare padding index to be ignored during model training/evaluation\n",
        "        self.word_pad_idx = self.word_field.vocab.stoi[self.word_field.pad_token]\n",
        "        self.char_pad_idx = self.char_field.vocab.stoi[self.char_field.pad_token]\n",
        "        self.tag_pad_idx = self.tag_field.vocab.stoi[self.tag_field.pad_token]"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:25:08.419106Z",
          "start_time": "2025-02-27T08:25:08.392936Z"
        },
        "id": "BkSJCLrVWIc2",
        "gather": {
          "logged": 1740733773191
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = Corpus(\n",
        "    input_folder=f\"{DRIVE_ROOT}\",\n",
        "    min_word_freq=3,\n",
        "    batch_size=64\n",
        "    # wv_file=f\"{DRIVE_ROOT}/id.bin\"\n",
        ")\n",
        "print(f\"Train set: {len(corpus.train_dataset)} sentences\")\n",
        "print(f\"Val set: {len(corpus.val_dataset)} sentences\")\n",
        "print(f\"Test set: {len(corpus.test_dataset)} sentences\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[==================================================] 100.0% 1662.8/1662.8MB downloaded\nTrain set: 3130 sentences\nVal set: 145 sentences\nTest set: 145 sentences\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/custom_37/lib/python3.7/site-packages/ipykernel_launcher.py:46: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n/anaconda/envs/custom_37/lib/python3.7/site-packages/ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n/anaconda/envs/custom_37/lib/python3.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-02-27T08:25:09.204895Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPN8mgL5WLcd",
        "jupyter": {
          "is_executing": true
        },
        "outputId": "f8677156-3216-45ca-fa47-02fa66c8fd19",
        "gather": {
          "logged": 1740734411239
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Components\n",
        "\n",
        "### Embedding Layer"
      ],
      "metadata": {
        "id": "lw7AEpNmGqbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 word_input_dim,\n",
        "                 word_emb_dim,\n",
        "                 word_emb_pretrained,\n",
        "                 word_emb_dropout,\n",
        "                 word_emb_froze,\n",
        "                 use_char_emb,\n",
        "                 char_input_dim,\n",
        "                 char_emb_dim,\n",
        "                 char_emb_dropout,\n",
        "                 char_cnn_filter_num,\n",
        "                 char_cnn_kernel_size,\n",
        "                 char_cnn_dropout,\n",
        "                 word_pad_idx,\n",
        "                 char_pad_idx,\n",
        "                 device\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.word_pad_idx = word_pad_idx\n",
        "        self.char_pad_idx = char_pad_idx\n",
        "        # Word Embedding\n",
        "        # initialize embedding with pretrained weights if given\n",
        "        if word_emb_pretrained is not None:\n",
        "            self.word_emb = nn.Embedding.from_pretrained(\n",
        "                embeddings=torch.as_tensor(word_emb_pretrained),\n",
        "                padding_idx=self.word_pad_idx,\n",
        "                freeze=word_emb_froze\n",
        "            )\n",
        "        else:\n",
        "            self.word_emb = nn.Embedding(\n",
        "                num_embeddings=word_input_dim,\n",
        "                embedding_dim=word_emb_dim,\n",
        "                padding_idx=self.word_pad_idx\n",
        "            )\n",
        "            self.word_emb.weight.data[self.word_pad_idx] = torch.zeros(word_emb_dim)\n",
        "        self.word_emb_dropout = nn.Dropout(word_emb_dropout)\n",
        "        self.output_dim = word_emb_dim\n",
        "        # Char Embedding\n",
        "        self.use_char_emb = use_char_emb\n",
        "        if self.use_char_emb:\n",
        "            self.char_emb_dim = char_emb_dim\n",
        "            self.char_emb = nn.Embedding(\n",
        "                num_embeddings=char_input_dim,\n",
        "                embedding_dim=char_emb_dim,\n",
        "                padding_idx=char_pad_idx\n",
        "            )\n",
        "            # initialize embedding for char padding as zero\n",
        "            self.char_emb.weight.data[self.char_pad_idx] = torch.zeros(self.char_emb_dim)\n",
        "            self.char_emb_dropout = nn.Dropout(char_emb_dropout)\n",
        "            # Char CNN\n",
        "            self.char_cnn = nn.Conv1d(\n",
        "                in_channels=char_emb_dim,\n",
        "                out_channels=char_emb_dim * char_cnn_filter_num,\n",
        "                kernel_size=char_cnn_kernel_size,\n",
        "                groups=char_emb_dim  # different 1d conv for each embedding dim\n",
        "            )\n",
        "            self.char_cnn_dropout = nn.Dropout(char_cnn_dropout)\n",
        "            self.output_dim += char_emb_dim * char_cnn_filter_num\n",
        "\n",
        "    def forward(self, words, chars):\n",
        "        # words = [sentence length, batch size]\n",
        "        # chars = [batch size, sentence length, word length)\n",
        "        # tags = [sentence length, batch size]\n",
        "        # embedding_out = [sentence length, batch size, embedding dim]\n",
        "        embedding_out = self.word_emb_dropout(self.word_emb(words))\n",
        "        if not self.use_char_emb: return embedding_out\n",
        "        # character cnn layer forward\n",
        "        # reference: https://github.com/achernodub/targer/blob/master/src/layers/layer_char_cnn.py\n",
        "        # char_emb_out = [batch size, sentence length, word length, char emb dim]\n",
        "        char_emb_out = self.char_emb_dropout(self.char_emb(chars))\n",
        "        batch_size, sent_len, word_len, char_emb_dim = char_emb_out.shape\n",
        "        char_cnn_max_out = torch.zeros(batch_size, sent_len, self.char_cnn.out_channels, device=self.device)\n",
        "        for sent_i in range(sent_len):\n",
        "            # sent_char_emb = [batch size, word length, char emb dim]\n",
        "            sent_char_emb = char_emb_out[:, sent_i, :, :]\n",
        "            # sent_char_emb_p = [batch size, char emb dim, word length]\n",
        "            sent_char_emb_p = sent_char_emb.permute(0, 2, 1)\n",
        "            # char_cnn_sent_out = [batch size, out channels * char emb dim, word length - kernel size + 1]\n",
        "            char_cnn_sent_out = self.char_cnn(sent_char_emb_p)\n",
        "            char_cnn_max_out[:, sent_i, :], _ = torch.max(char_cnn_sent_out, dim=2)\n",
        "        char_cnn = self.char_cnn_dropout(char_cnn_max_out)\n",
        "        # concat word and char embedding\n",
        "        # char_cnn_p = [sentence length, batch size, char emb dim * num filter]\n",
        "        char_cnn_p = char_cnn.permute(1, 0, 2)\n",
        "        word_features = torch.cat((embedding_out, char_cnn_p), dim=2)\n",
        "        return word_features\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.839285Z",
          "start_time": "2025-02-27T08:18:36.825118Z"
        },
        "id": "T8wLgemGWZJ7",
        "gather": {
          "logged": 1740734412181
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM Layer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMAttn(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 lstm_hidden_dim,\n",
        "                 lstm_layers,\n",
        "                 lstm_dropout,\n",
        "                 word_pad_idx,\n",
        "                 attn_heads=None,\n",
        "                 attn_dropout=None\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.word_pad_idx = word_pad_idx\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=lstm_hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            bidirectional=True,\n",
        "            dropout=lstm_dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "        self.attn_heads = attn_heads\n",
        "        if self.attn_heads:\n",
        "            self.attn = nn.MultiheadAttention(\n",
        "                embed_dim=lstm_hidden_dim * 2,\n",
        "                num_heads=attn_heads,\n",
        "                dropout=attn_dropout\n",
        "            )\n",
        "\n",
        "    def forward(self, words, word_features):\n",
        "        lstm_out, _ = self.lstm(word_features)\n",
        "        if not self.attn_heads: return lstm_out\n",
        "        # create masking for paddings\n",
        "        key_padding_mask = torch.as_tensor(words == self.word_pad_idx).permute(1, 0)\n",
        "        attn_out, _ = self.attn(lstm_out, lstm_out, lstm_out, key_padding_mask=key_padding_mask)\n",
        "        return attn_out"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.870403Z",
          "start_time": "2025-02-27T08:18:36.856893Z"
        },
        "gather": {
          "logged": 1740734412868
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CRF Layer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class CRF(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 fc_dropout,\n",
        "                 word_pad_idx,\n",
        "                 tag_names,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.word_pad_idx = word_pad_idx\n",
        "        # Fully-connected\n",
        "        self.fc_dropout = nn.Dropout(fc_dropout)\n",
        "        self.fc = nn.Linear(input_dim, len(tag_names))\n",
        "        # CRF\n",
        "        self.crf = torchcrf.CRF(num_tags=len(tag_names))\n",
        "        self.init_crf_transitions(tag_names)\n",
        "\n",
        "    def forward(self, words, word_features, tags):\n",
        "        # fc_out = [sentence length, batch size, output dim]\n",
        "        fc_out = self.fc(self.fc_dropout(word_features))\n",
        "        crf_mask = words != self.word_pad_idx\n",
        "        crf_out = self.crf.decode(fc_out, mask=crf_mask)\n",
        "        crf_loss = -self.crf(fc_out, tags=tags, mask=crf_mask) if tags is not None else None\n",
        "        return crf_out, crf_loss\n",
        "\n",
        "    def init_crf_transitions(self, tag_names, imp_value=-100):\n",
        "        num_tags = len(tag_names)\n",
        "        for i in range(num_tags):\n",
        "            tag_name = tag_names[i]\n",
        "            # I and L and <pad> impossible as a start\n",
        "            if tag_name[0] in (\"I\", \"L\") or tag_name == \"<pad>\":\n",
        "                torch.nn.init.constant_(self.crf.start_transitions[i], imp_value)\n",
        "            # B and I impossible as an end\n",
        "            if tag_name[0] in (\"B\", \"I\"):\n",
        "                torch.nn.init.constant_(self.crf.end_transitions[i], imp_value)\n",
        "        # init impossible transitions between positions\n",
        "        tag_is = {}\n",
        "        for tag_position in (\"B\", \"I\", \"O\", \"U\", \"L\"):\n",
        "            tag_is[tag_position] = [i for i, tag in enumerate(tag_names) if tag[0] == tag_position]\n",
        "        tag_is[\"P\"] = [i for i, tag in enumerate(tag_names) if tag == \"tag\"]\n",
        "        impossible_transitions_position = {\n",
        "            \"B\": \"BOUP\",\n",
        "            \"I\": \"BOUP\",\n",
        "            \"O\": \"IL\",\n",
        "            \"U\": \"IL\"\n",
        "        }\n",
        "        for from_tag, to_tag_list in impossible_transitions_position.items():\n",
        "            to_tags = list(to_tag_list)\n",
        "            for from_tag_i in tag_is[from_tag]:\n",
        "                for to_tag in to_tags:\n",
        "                    for to_tag_i in tag_is[to_tag]:\n",
        "                        torch.nn.init.constant_(\n",
        "                            self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
        "                        )\n",
        "        # init impossible B and I transitions to different entity types\n",
        "        impossible_transitions_tags = {\n",
        "            \"B\": \"IL\",\n",
        "            \"I\": \"IL\"\n",
        "        }\n",
        "        for from_tag, to_tag_list in impossible_transitions_tags.items():\n",
        "            to_tags = list(to_tag_list)\n",
        "            for from_tag_i in tag_is[from_tag]:\n",
        "                for to_tag in to_tags:\n",
        "                    for to_tag_i in tag_is[to_tag]:\n",
        "                        if tag_names[from_tag_i].split(\"-\")[1] != tag_names[to_tag_i].split(\"-\")[1]:\n",
        "                            torch.nn.init.constant_(\n",
        "                                self.crf.transitions[from_tag_i, to_tag_i], imp_value\n",
        "                            )\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.901767Z",
          "start_time": "2025-02-27T08:18:36.887666Z"
        },
        "gather": {
          "logged": 1740734413533
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Layer for Transformer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=10000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.932432Z",
          "start_time": "2025-02-27T08:18:36.918117Z"
        },
        "gather": {
          "logged": 1740734414216
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Layer"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 attn_heads,\n",
        "                 attn_dropout,\n",
        "                 trf_layers,\n",
        "                 fc_hidden,\n",
        "                 word_pad_idx\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.word_pad_idx = word_pad_idx\n",
        "        self.position_encoder = PositionalEncoding(\n",
        "            d_model=input_dim\n",
        "        )\n",
        "        layers = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim,\n",
        "            nhead=attn_heads,\n",
        "            activation=\"relu\",\n",
        "            dropout=attn_dropout\n",
        "        )\n",
        "        self.trf = nn.TransformerEncoder(\n",
        "            encoder_layer=layers,\n",
        "            num_layers=trf_layers\n",
        "        )\n",
        "        # 2-layers fully-connected with GELU activation in-between\n",
        "        self.fc = nn.Linear(\n",
        "            in_features=input_dim,\n",
        "            out_features=fc_hidden\n",
        "        )\n",
        "        self.fc_gelu = nn.GELU()\n",
        "        self.fc_norm = nn.LayerNorm(fc_hidden)\n",
        "        self.output_dim = fc_hidden\n",
        "\n",
        "    def forward(self, words, word_features):\n",
        "        # Transformer\n",
        "        key_padding_mask = torch.as_tensor(words == self.word_pad_idx).permute(1, 0)\n",
        "        # pos_out = [sentence length, batch size, embedding dim + char emb dim * num filter]\n",
        "        pos_out = self.position_encoder(word_features)\n",
        "        # enc_out = [sentence length, batch size, embedding dim + char emb dim * num filter]\n",
        "        trf_out = self.trf(pos_out, src_key_padding_mask=key_padding_mask)\n",
        "        # fc_out = [sentence length, batch size, fc hidden]\n",
        "        fc_out = self.fc_norm(self.fc_gelu(self.fc(trf_out)))\n",
        "        return fc_out\n"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.964566Z",
          "start_time": "2025-02-27T08:18:36.950426Z"
        },
        "gather": {
          "logged": 1740734414872
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture for Public Figures Statements For Journalism In Indonesia using NER Approach"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class NERModel(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 word_input_dim,\n",
        "                 word_pad_idx,\n",
        "                 char_pad_idx,\n",
        "                 tag_names,\n",
        "                 device,\n",
        "                 name=\"\",\n",
        "                 model_arch=\"bilstm\",\n",
        "                 word_emb_dim=300,\n",
        "                 word_emb_pretrained=None,\n",
        "                 word_emb_dropout=0.5,\n",
        "                 word_emb_froze=False,\n",
        "                 use_char_emb=False,\n",
        "                 char_input_dim=None,\n",
        "                 char_emb_dim=None,\n",
        "                 char_emb_dropout=None,\n",
        "                 char_cnn_filter_num=None,\n",
        "                 char_cnn_kernel_size=None,\n",
        "                 char_cnn_dropout=None,\n",
        "                 lstm_hidden_dim=64,\n",
        "                 lstm_layers=2,\n",
        "                 lstm_dropout=0.1,\n",
        "                 attn_heads=None,\n",
        "                 attn_dropout=None,\n",
        "                 trf_layers=None,\n",
        "                 fc_hidden=None,\n",
        "                 fc_dropout=0.25\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        # Embeddings\n",
        "        self.embeddings = Embeddings(\n",
        "            word_input_dim=word_input_dim,\n",
        "            word_emb_dim=word_emb_dim,\n",
        "            word_emb_pretrained=word_emb_pretrained,\n",
        "            word_emb_dropout=word_emb_dropout,\n",
        "            word_emb_froze=word_emb_froze,\n",
        "            use_char_emb=use_char_emb,\n",
        "            char_input_dim=char_input_dim,\n",
        "            char_emb_dim=char_emb_dim,\n",
        "            char_emb_dropout=char_emb_dropout,\n",
        "            char_cnn_filter_num=char_cnn_filter_num,\n",
        "            char_cnn_kernel_size=char_cnn_kernel_size,\n",
        "            char_cnn_dropout=char_cnn_dropout,\n",
        "            word_pad_idx=word_pad_idx,\n",
        "            char_pad_idx=char_pad_idx,\n",
        "            device=device\n",
        "        )\n",
        "        if model_arch.lower() == \"bilstm\":\n",
        "            # LSTM-Attention\n",
        "            self.encoder = LSTMAttn(\n",
        "                 input_dim=self.embeddings.output_dim,\n",
        "                 lstm_hidden_dim=lstm_hidden_dim,\n",
        "                 lstm_layers=lstm_layers,\n",
        "                 lstm_dropout=lstm_dropout,\n",
        "                 word_pad_idx=word_pad_idx,\n",
        "                 attn_heads=attn_heads,\n",
        "                 attn_dropout=attn_dropout\n",
        "            )\n",
        "            encoder_output_dim = lstm_hidden_dim * 2\n",
        "        elif model_arch.lower() == \"transformer\":\n",
        "            # Transformer\n",
        "            self.encoder = Transformer(\n",
        "                input_dim=self.embeddings.output_dim,\n",
        "                attn_heads=attn_heads,\n",
        "                attn_dropout=attn_dropout,\n",
        "                trf_layers=trf_layers,\n",
        "                fc_hidden=fc_hidden,\n",
        "                word_pad_idx=word_pad_idx\n",
        "            )\n",
        "            encoder_output_dim = self.encoder.output_dim\n",
        "        else:\n",
        "            raise ValueError(\"param `model_arch` must be either 'bilstm' or 'transformer'\")\n",
        "        # CRF\n",
        "        self.crf = CRF(\n",
        "            input_dim=encoder_output_dim,\n",
        "            fc_dropout=fc_dropout,\n",
        "            word_pad_idx=word_pad_idx,\n",
        "            tag_names=tag_names\n",
        "        )\n",
        "\n",
        "    def forward(self, words, chars, tags=None):\n",
        "        word_features = self.embeddings(words, chars)\n",
        "        # lstm_out = [sentence length, batch size, hidden dim * 2]\n",
        "        encoder_out = self.encoder(words, word_features)\n",
        "        # fc_out = [sentence length, batch size, output dim]\n",
        "        crf_out, crf_loss = self.crf(words, encoder_out, tags)\n",
        "        \n",
        "        return crf_out, crf_loss\n",
        "\n",
        "    def save_state(self, path):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load_state(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:36.996049Z",
          "start_time": "2025-02-27T08:18:36.982037Z"
        },
        "gather": {
          "logged": 1740734415481
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training\n",
        "\n",
        "The training sequence (`train()`) using the LR reduction and early stopping. For reducing LR, the pytorch built-in implementation `ReduceLROnPlateau` is used, while the early stopping is simply executed with a counter based on the F1 score. \n",
        "\n",
        "Note that the new score has to be at least 1% relatively better than the previous best score to be considered as a significant improvement."
      ],
      "metadata": {
        "id": "bIEiKwF4Gxuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "\n",
        "    def __init__(self, model, data, optimizer, device, checkpoint_path=None, model_name=None):\n",
        "        self.device = device\n",
        "        self.model = model.to(self.device)\n",
        "        self.data = data\n",
        "        self.optimizer = optimizer\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.model_name = model_name\n",
        "\n",
        "    @staticmethod\n",
        "    def epoch_time(start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "    def f1_positive(self, preds, y, full_report=False):\n",
        "        index_o = self.data.tag_field.vocab.stoi[\"O\"]\n",
        "        # take all labels except padding and \"O\"\n",
        "        positive_labels = [i for i in range(len(self.data.tag_field.vocab.itos))\n",
        "                           if i not in (self.data.tag_pad_idx, index_o)]\n",
        "        # make the prediction one dimensional to follow sklearn f1 score input param\n",
        "        flatten_preds = [pred for sent_pred in preds for pred in sent_pred]\n",
        "        # remove prediction for padding and \"O\"\n",
        "        positive_preds = [pred for pred in flatten_preds\n",
        "                          if pred not in (self.data.tag_pad_idx, index_o)]\n",
        "        # make the true tags one dimensional to follow sklearn f1 score input param\n",
        "        flatten_y = [tag for sent_tag in y for tag in sent_tag]\n",
        "        if full_report:\n",
        "            # take all names except padding and \"O\"\n",
        "            positive_names = [self.data.tag_field.vocab.itos[i]\n",
        "                              for i in range(len(self.data.tag_field.vocab.itos))\n",
        "                              if i not in (self.data.tag_pad_idx, index_o)]\n",
        "            print(classification_report(\n",
        "                y_true=flatten_y,\n",
        "                y_pred=flatten_preds,\n",
        "                labels=positive_labels,\n",
        "                target_names=positive_names\n",
        "            ))\n",
        "            \n",
        "        # average \"micro\" means we take weighted average of the class f1 score\n",
        "        # weighted based on the number of support\n",
        "        return precision_recall_fscore_support(\n",
        "            y_true=flatten_y,\n",
        "            y_pred=flatten_preds,\n",
        "            labels=positive_labels,\n",
        "            average=\"micro\"\n",
        "        ) if len(positive_preds) > 0 else 0\n",
        "\n",
        "    def epoch(self):\n",
        "        epoch_loss = 0\n",
        "        true_tags_epoch = []\n",
        "        pred_tags_epoch = []\n",
        "        self.model.train()\n",
        "        for batch in self.data.train_iter:\n",
        "            # words = [sent len, batch size]\n",
        "            words = batch.word.to(self.device)\n",
        "            # chars = [batch size, sent len, char len]\n",
        "            chars = batch.char.to(self.device)\n",
        "            # tags = [sent len, batch size]\n",
        "            true_tags = batch.tag.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            pred_tags_list, batch_loss = self.model(words, chars, true_tags)\n",
        "            pred_tags_epoch += pred_tags_list\n",
        "            # to calculate the loss and f1, we flatten true tags\n",
        "            true_tags_epoch += [\n",
        "                [tag for tag in sent_tag if tag != self.data.tag_pad_idx]\n",
        "                for sent_tag in true_tags.permute(1, 0).tolist()\n",
        "            ]\n",
        "            batch_loss.backward()\n",
        "            self.optimizer.step()\n",
        "        \n",
        "            epoch_loss += batch_loss.item()\n",
        "        \n",
        "        epoch_score = self.f1_positive(pred_tags_epoch, true_tags_epoch)\n",
        "        return epoch_loss / len(self.data.train_iter), epoch_score\n",
        " \n",
        "    def evaluate(self, iterator, full_report=False):\n",
        "        epoch_loss = 0\n",
        "        true_tags_epoch = []\n",
        "        pred_tags_epoch = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            # similar to epoch() but model is in evaluation mode and no backprop\n",
        "            for batch in iterator:\n",
        "                words = batch.word.to(self.device)\n",
        "                chars = batch.char.to(self.device)\n",
        "                true_tags = batch.tag.to(self.device)\n",
        "                pred_tags, batch_loss = self.model(words, chars, true_tags)\n",
        "                pred_tags_epoch += pred_tags\n",
        "                true_tags_epoch += [\n",
        "                    [tag for tag in sent_tag if tag != self.data.tag_pad_idx]\n",
        "                    for sent_tag in true_tags.permute(1, 0).tolist()\n",
        "                ]          \n",
        "            \n",
        "                epoch_loss += batch_loss.item()\n",
        "                \n",
        "        epoch_score = self.f1_positive(pred_tags_epoch, true_tags_epoch, full_report)\n",
        "        return epoch_loss / len(iterator), epoch_score\n",
        "\n",
        "    ### BEGIN MODIFIED SECTION: LEARNING RATE ###\n",
        "    def train(self, max_epochs, no_improvement=None):\n",
        "        history = {\n",
        "            \"num_params\": self.model.count_parameters(),\n",
        "            \"train_loss\": [],\n",
        "            \"train_f1\": [],\n",
        "            \"train_p\": [],\n",
        "            \"train_r\": [],\n",
        "            \"val_loss\": [],\n",
        "            \"val_f1\": [],\n",
        "            \"val_p\": [],\n",
        "            \"val_r\": [],\n",
        "            \"test_loss\": [],\n",
        "            \"test_f1\": [],\n",
        "            \"test_p\": [],\n",
        "            \"test_r\": [],           \n",
        "        }\n",
        "        data_train = {\n",
        "            \"models\": [],\n",
        "            \"epoch\": [],\n",
        "            \"train_loss\": [],\n",
        "            \"train_f1\": [],\n",
        "            \"train_p\": [],\n",
        "            \"train_r\": [],\n",
        "            \"val_loss\": [],\n",
        "            \"val_f1\": [],\n",
        "            \"val_p\": [],\n",
        "            \"val_r\": [],\n",
        "            \"elapsed_train_time\": [],\n",
        "        }\n",
        "        data_test = {\n",
        "            \"models\": [],\n",
        "            \"best_epoch\": [],\n",
        "            \"best_train_f1\": [],\n",
        "            \"best_val_f1\": [],\n",
        "            \"test_loss\": [],\n",
        "            \"test_f1\": [],\n",
        "            \"test_p\": [],\n",
        "            \"test_r\": [],  \n",
        "        }\n",
        "        df_train = None\n",
        "        df_test = None\n",
        "        \n",
        "        elapsed_train_time = 0\n",
        "        best_val_f1 = 0\n",
        "        best_train_f1 = 0\n",
        "        best_epoch = None\n",
        "        # scheduler object from pytorch\n",
        "        # reduce learning rate by a factor of 0.3 if there is no performance\n",
        "        # improvement after 3 epochs\n",
        "        lr_scheduler = ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer,\n",
        "            patience=3,\n",
        "            factor=0.3,\n",
        "            mode=\"max\",\n",
        "            verbose=True\n",
        "        )\n",
        "        epoch = 1\n",
        "        n_stagnant = 0  # preparation for early stopping\n",
        "        stop = False\n",
        "        while not stop:\n",
        "            start_time = time.time()\n",
        "            train_loss, train_metrics = self.epoch()\n",
        "            end_time = time.time()\n",
        "            elapsed_train_time += end_time - start_time\n",
        "            \n",
        "            history[\"train_loss\"].append(train_loss)\n",
        "            history[\"train_p\"].append(train_metrics[0])\n",
        "            history[\"train_r\"].append(train_metrics[1])\n",
        "            history[\"train_f1\"].append(train_metrics[2])\n",
        "            \n",
        "            data_train[\"models\"].append(str(self.model_name))\n",
        "            data_train[\"epoch\"].append(epoch)\n",
        "            data_train[\"elapsed_train_time\"].append(elapsed_train_time)\n",
        "            data_train[\"train_loss\"].append(train_loss)\n",
        "            data_train[\"train_p\"].append(train_metrics[0])\n",
        "            data_train[\"train_r\"].append(train_metrics[1])\n",
        "            data_train[\"train_f1\"].append(train_metrics[2])\n",
        "            \n",
        "            val_loss, val_metrics = self.evaluate(self.data.val_iter)\n",
        "            \n",
        "            lr_scheduler.step(val_metrics[2])  # inform the scheduler\n",
        "            # take the current model if it is at least 1% better than the previous best F1\n",
        "            if self.checkpoint_path and val_metrics[2] > (1.01 * best_val_f1):\n",
        "                print(f\"Epoch {epoch:5d}: found better Val F1: {val_metrics[2]:.4f} (Train F1: {train_metrics[2]:.4f}), saving model...\")\n",
        "                self.model.save_state(self.checkpoint_path)   \n",
        "                best_val_f1 = val_metrics[2]\n",
        "                best_train_f1 = train_metrics[2]\n",
        "                best_epoch = epoch\n",
        "                n_stagnant = 0\n",
        "            else:\n",
        "                n_stagnant += 1\n",
        "            \n",
        "            history[\"val_loss\"].append(val_loss)\n",
        "            history[\"val_p\"].append(val_metrics[0])\n",
        "            history[\"val_r\"].append(val_metrics[1])\n",
        "            history[\"val_f1\"].append(val_metrics[2])\n",
        "            \n",
        "            data_train[\"val_loss\"].append(val_loss)\n",
        "            data_train[\"val_p\"].append(val_metrics[0])\n",
        "            data_train[\"val_r\"].append(val_metrics[1])\n",
        "            data_train[\"val_f1\"].append(val_metrics[2])\n",
        "            \n",
        "            if epoch >= max_epochs:\n",
        "                print(f\"Reach maximum number of epoch: {epoch}, stop training.\")\n",
        "                stop = True\n",
        "            elif no_improvement is not None and n_stagnant >= no_improvement:\n",
        "                print(f\"No improvement after {n_stagnant} epochs, stop training.\")\n",
        "                stop = True\n",
        "            else:\n",
        "                epoch += 1\n",
        "        \n",
        "        df_train = pd.DataFrame(data_train)\n",
        "        #wandb.log({f\"dataframe_train_{self.model_name}\": wandb.Table(dataframe=df_train)})\n",
        "                \n",
        "        if self.checkpoint_path and best_val_f1 > 0:\n",
        "            self.model.load_state(self.checkpoint_path)\n",
        "        test_loss, test_metrics = self.evaluate(self.data.test_iter)\n",
        "                    \n",
        "        history[\"best_val_f1\"] = best_val_f1\n",
        "        history[\"best_epoch\"] = best_epoch        \n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_p\"].append(test_metrics[0])\n",
        "        history[\"test_r\"].append(test_metrics[1])\n",
        "        history[\"test_f1\"].append(test_metrics[2])\n",
        "        \n",
        "        data_test[\"models\"].append(str(self.model_name))\n",
        "        data_test[\"best_train_f1\"].append(best_train_f1)\n",
        "        data_test[\"best_val_f1\"].append(best_val_f1)\n",
        "        data_test[\"best_epoch\"].append(best_epoch)\n",
        "        data_test[\"test_loss\"].append(test_loss)\n",
        "        data_test[\"test_p\"].append(test_metrics[0])\n",
        "        data_test[\"test_r\"].append(test_metrics[1])\n",
        "        data_test[\"test_f1\"].append(test_metrics[2])        \n",
        "        \n",
        "        df_test = pd.DataFrame(data_test)\n",
        "        #wandb.log({f\"dataframe_test_{self.model_name}\": wandb.Table(dataframe=df_test)})\n",
        "        \n",
        "        history[\"elapsed_train_time\"] = elapsed_train_time\n",
        "        return history, df_train, df_test\n",
        "    ### END MODIFIED SECTION ###\n",
        "\n",
        "    def infer(self, docs, index=None, true_tags=None):        \n",
        "        data_infer = {\n",
        "            \"models\": [],\n",
        "            \"tokens\": [],\n",
        "            \"unks\": [],\n",
        "            \"predicted_tags\": [],\n",
        "            \"true_tags\": [], \n",
        "        }\n",
        "        data_seqeval = {\n",
        "            \"sentences\": [],\n",
        "            \"tokens_length\": [],\n",
        "            \"tokens\": [],\n",
        "            \"predicted_tags\": [],\n",
        "            \"true_tags\": [],\n",
        "        }\n",
        "        df_infer = None\n",
        "        df_seqeval = None\n",
        "        self.model.eval()\n",
        "        # tokenize sentence\n",
        "        nlp = Indonesian()\n",
        "        #nlp.max_length = 2000000\n",
        "        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "        doc = nlp(docs)\n",
        "        tokens_len = 0\n",
        "        for sent in doc.sents:\n",
        "            data_seqeval[\"sentences\"].append(sent.text)\n",
        "            tokens = [token.text for token in nlp(sent.text)]\n",
        "            data_seqeval[\"tokens\"].append(tokens)\n",
        "            max_word_len = max([len(token) for token in tokens])\n",
        "            # transform to indices based on corpus vocab\n",
        "            numericalized_tokens = [self.data.word_field.vocab.stoi[token.lower()] for token in tokens]\n",
        "            numericalized_chars = []\n",
        "            char_pad_id = self.data.char_pad_idx\n",
        "            for token in tokens:\n",
        "                numericalized_chars.append(\n",
        "                    [self.data.char_field.vocab.stoi[char] for char in token]\n",
        "                    + [char_pad_id for _ in range(max_word_len - len(token))]\n",
        "                )\n",
        "            # find unknown words\n",
        "            unk_idx = self.data.word_field.vocab.stoi[self.data.word_field.unk_token]\n",
        "            unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "            # begin prediction\n",
        "            token_tensor = torch.as_tensor(numericalized_tokens)\n",
        "\n",
        "            token_tensor = token_tensor.unsqueeze(-1).to(self.device)\n",
        "            char_tensor = torch.as_tensor(numericalized_chars)\n",
        "            char_tensor = char_tensor.unsqueeze(0).to(self.device)\n",
        "            predictions, _ = self.model(token_tensor, char_tensor)\n",
        "            # convert results to tags\n",
        "            predicted_tags = [self.data.tag_field.vocab.itos[t] for t in predictions[0]]\n",
        "            # print inferred tags\n",
        "            max_len_token = max([len(token) for token in tokens] + [len('word')])\n",
        "            max_len_tag = max([len(tag) for tag in predicted_tags] + [len('pred')])           \n",
        "            #print(\n",
        "            #    f\"{'word'.ljust(max_len_token)}\\t{'unk'.ljust(max_len_token)}\\t{'pred tag'.ljust(max_len_tag)}\"\n",
        "            #    + (\"\\ttrue tag\" if true_tags else \"\")\n",
        "            #)\n",
        "            endpos = tokens_len+len(tokens)\n",
        "            t_tags = true_tags[tokens_len:endpos]\n",
        "            p_tags = []\n",
        "            for i, token in enumerate(tokens):\n",
        "                is_unk = \"unk\" if token in unks else \"-\"\n",
        "                data_infer[\"models\"].append(str(self.model_name))\n",
        "                data_infer[\"tokens\"].append(str(token))\n",
        "                data_infer[\"unks\"].append(str(is_unk))\n",
        "                data_infer[\"predicted_tags\"].append(str(predicted_tags[i]))\n",
        "                data_infer[\"true_tags\"].append(str(true_tags[i]))\n",
        "                #t_tags.append(str(true_tags[i+tokens_len]).strip())\n",
        "                p_tags.append(str(predicted_tags[i]).strip())     \n",
        "                #print(\n",
        "                #    f\"{token.ljust(max_len_token)}\\t{is_unk.ljust(max_len_token)}\\t{predicted_tags[i].ljust(max_len_tag)}\"\n",
        "                #    + (f\"\\t{true_tags[i]}\" if true_tags else \"-\")\n",
        "                #)\n",
        "           \n",
        "            \n",
        "            data_seqeval[\"true_tags\"].append(str(t_tags))\n",
        "            data_seqeval[\"predicted_tags\"].append(str(p_tags))\n",
        "            tokens_len = tokens_len + len(tokens)\n",
        "            data_seqeval[\"tokens_length\"].append(str(len(tokens)))   \n",
        " \n",
        "        df_infer = pd.DataFrame(data_infer)\n",
        "        wandb.log({f\"dataframe_infer_{self.model_name}\": wandb.Table(dataframe=df_infer)})  \n",
        "        df_infer.to_csv(f\"{DRIVE_ROOT}/models-wandb-7030-v2/infer/{model_name}/df_infer_{model_name}_{index}.csv\")\n",
        "\n",
        "        df_seqeval = pd.DataFrame(data_seqeval)\n",
        "        wandb.log({f\"dataframe_seqeval_{self.model_name}\": wandb.Table(dataframe=df_seqeval)})\n",
        "        df_seqeval.to_csv(f\"{DRIVE_ROOT}/models-wandb-7030-v2/seqeval/{model_name}/df_seqeval_{model_name}_{index}.csv\")\n",
        "        return tokens, predicted_tags, unks\n",
        "        #return tokens, predicted_tags, unks, df_infer, df_seqeval"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:37.057073Z",
          "start_time": "2025-02-27T08:18:37.014038Z"
        },
        "id": "0wIC2GQ9GuRz",
        "gather": {
          "logged": 1740734416107
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "<__main__.Corpus at 0x7fdba2f1ba10>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1740734416905
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LR Finder for each of the architecture config. The plot shown for each model represents the loss at every learning rate value. The rule of thumb is to take the value on which the loss has been steadily decreasing and not yet reach the minimum."
      ],
      "metadata": {
        "id": "UpFcgCoTg-B_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configurations building block\n",
        "base = {\n",
        "    \"word_input_dim\": len(corpus.word_field.vocab),\n",
        "    \"char_pad_idx\": corpus.char_pad_idx,\n",
        "    \"word_pad_idx\": corpus.word_pad_idx,\n",
        "    \"tag_names\": corpus.tag_field.vocab.itos,\n",
        "    \"device\": use_device\n",
        "}\n",
        "w2v = {\n",
        "    \"word_emb_pretrained\": corpus.word_field.vocab.vectors if corpus.wv_model else None\n",
        "}\n",
        "cnn = {\n",
        "    \"use_char_emb\": True,\n",
        "    \"char_input_dim\": len(corpus.char_field.vocab),\n",
        "    \"char_emb_dim\": 37,\n",
        "    \"char_emb_dropout\": 0.25,\n",
        "    \"char_cnn_filter_num\": 4,\n",
        "    \"char_cnn_kernel_size\": 3,\n",
        "    \"char_cnn_dropout\": 0.25\n",
        "}\n",
        "attn = {\n",
        "    \"attn_heads\": 16,\n",
        "    \"attn_dropout\": 0.25\n",
        "}\n",
        "transformer = {\n",
        "    \"model_arch\": \"transformer\",\n",
        "    \"trf_layers\": 2,\n",
        "    \"fc_hidden\": 256,\n",
        "}\n",
        "configs = {\n",
        "    \"bilstm\": base,\n",
        "    \"bilstm+w2v\": {**base, **w2v},\n",
        "    \"bilstm+w2v+cnn\": {**base, **w2v, **cnn},\n",
        "    \"bilstm+w2v+cnn+attn\": {**base, **w2v, **cnn, **attn},\n",
        "    \"transformer+w2v+cnn\": {**base, **transformer, **w2v, **cnn, **attn}\n",
        "}\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-27T08:18:37.088300Z",
          "start_time": "2025-02-27T08:18:37.074299Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I1a_GGkGG5Xm",
        "outputId": "3ef771dc-9e87-4cfd-81eb-82cf4e15a59f",
        "gather": {
          "logged": 1740734417583
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_space = {\n",
        "    \"bilstm\": (1e-5, 10),\n",
        "    \"bilstm+w2v\": (1e-5, 10),\n",
        "    \"bilstm+w2v+cnn\": (1e-5, 2),\n",
        "    \"bilstm+w2v+cnn+attn\": (1e-5, 0.2),\n",
        "    \"transformer+w2v+cnn\": (1e-6, 0.2)\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1740734418192
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts = [getattr(example, 'word') for example in batch]\n",
        "    labels = [getattr(example, 'tag') for example in batch]  # Pastikan sudah numerik\n",
        "\n",
        "    # Tokenisasi\n",
        "    encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    return encoded_texts.input_ids, torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    corpus.train_iter.dataset, \n",
        "    batch_size=corpus.train_iter.batch_size, \n",
        "    shuffle=True, \n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    corpus.val_iter.dataset, \n",
        "    batch_size=corpus.val_iter.batch_size, \n",
        "    shuffle=False, \n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "\n",
        "for model_name in configs:\n",
        "    print(f\"Begin LR Finder for model: {model_name}\")\n",
        "    model = NERModel(**configs[model_name])\n",
        "    start_lr, end_lr = search_space[model_name]\n",
        "    lr_finder = LRFinder(model, Adam(model.parameters(), lr=start_lr, weight_decay=1e-2), nn.CrossEntropyLoss(), device=use_device)\n",
        "    lr_finder.range_test(train_loader, val_loader, end_lr=end_lr, num_iter=55, step_mode=\"exp\", diverge_th=3)\n",
        "    data_table_lr = wandb.Table(dataframe=lr_finder.data_table)\n",
        "    wandb.log({f\"key_table_lr_{model_name}\":data_table_lr})\n",
        "    lr_finder.plot(skip_start=10, skip_end=0, suggest_lr=True,model_name=model_name)\n",
        "    data_table_lr_suggest = wandb.Table(dataframe=lr_finder.data_table_img)\n",
        "    wandb.log({f\"key_table_lr_suggest_{model_name}\":data_table_lr_suggest})\n",
        "    lr_finder.reset()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Begin LR Finder for model: bilstm\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/55 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eab53b81d8d4475788794cdc4bf217a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many dimensions 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9da2b1674a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_space\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"exp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiverge_th\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mdata_table_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34mf\"key_table_lr_{model_name}\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata_table_lr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36mrange_test\u001b[0;34m(self, train_loader, val_loader, start_lr, end_lr, num_iter, step_mode, smooth_f, diverge_th, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mnon_blocking_transfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking_transfer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m             )\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36m_train_batch\u001b[0;34m(self, train_iter, accumulation_steps, non_blocking_transfer)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m             inputs, labels = self._move_to_device(\n\u001b[1;32m    401\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking_transfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs_labels_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-9da2b1674a01>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Konversi ke tensor (disesuaikan dengan data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Sesuaikan dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1740735952798
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Experiments"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# New initial learning rate\n",
        "lrs = {\n",
        "    \"bilstm\": 7.74e-03,\n",
        "    \"bilstm+w2v\": 7.74e-03,\n",
        "    \"bilstm+w2v+cnn\": 5.61e-03,\n",
        "    \"bilstm+w2v+cnn+attn\": 5.11e-03,\n",
        "    \"transformer+w2v+cnn\": 3.72e-05\n",
        "}\n",
        "max_epochs = 50\n",
        "no_improvement = 10\n",
        "histories = {}\n",
        "df_train_metrics = None\n",
        "df_test_metrics = None"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ovpuM6NXG_iF",
        "outputId": "d4628277-d5cf-468b-f9e9-b79ffffc217a",
        "gather": {
          "logged": 1740734424203
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in configs:\n",
        "    print(f\"Start Training: {model_name}\")\n",
        "    model = NERModel(**configs[model_name])\n",
        "    wandb.watch(model,log=\"all\")\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        data=corpus,\n",
        "        optimizer=Adam(model.parameters(), lr=lrs[model_name], weight_decay=1e-2),  # add weight decay for Adam\n",
        "        device=use_device,\n",
        "        checkpoint_path=f\"{DRIVE_ROOT}/models-wandb-7030-v2/{model_name}.pt\",\n",
        "        model_name=f\"{model_name}\"\n",
        "    )\n",
        "    \n",
        "    histories[model_name], df_train_metrics, df_test_metrics = trainer.train(max_epochs, no_improvement)\n",
        "    data_table_train = wandb.Table(dataframe=df_train_metrics)\n",
        "    wandb.log({f\"key_table_train_val_metrics_{model_name}\":data_table_train})\n",
        "    data_table_test = wandb.Table(dataframe=df_test_metrics)\n",
        "    wandb.log({f\"key_table_test_metrics_{model_name}\":data_table_test})\n",
        "    print(f\"Done Training: {model_name}\")\n",
        "    print(f\"Model Information:\")\n",
        "    print(model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740734424267
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Information"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "columns_info=[\"model\",\"num_params\",\"training_time\"]\n",
        "data_model_info = wandb.Table(columns=columns_info)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740734424342
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_model_name = max([len(m) for m in histories])\n",
        "print(f\"{'MODEL NAME'.ljust(max_len_model_name)}\\t{'NUM PARAMS'.ljust(10)}\\tTRAINING TIME\")\n",
        "for model_name, history in histories.items():\n",
        "    print(f\"{model_name.ljust(max_len_model_name)}\\t{history['num_params']:,}\\t{int(history['elapsed_train_time']//60)}m {int(history['elapsed_train_time'] % 60)}s\")\n",
        "    data_model_info.add_data(model_name,history['num_params'],f\"{int(history['elapsed_train_time']//60)}m {int(history['elapsed_train_time'] % 60)}s\")\n",
        "    wandb.log({\"key_table_model_info\":data_model_info})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "46s9a4xYu5CU",
        "outputId": "5c5f885c-4c28-49e1-c9f3-3e9d2c95effe",
        "gather": {
          "logged": 1740734424401
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bilstm\"\n",
        "val_loss1 = histories[model_name][\"val_loss\"]\n",
        "val_loss2 = histories[\"bilstm+w2v\"][\"val_loss\"]\n",
        "fig, axs = plt.subplots(2, 1, figsize=(15, 12))\n",
        "for model_name in histories:\n",
        "    axs[0].plot(histories[model_name][\"val_loss\"], dashes=[6, 2], label=model_name)\n",
        "    axs[0].plot(histories[model_name][\"train_loss\"], label=model_name)\n",
        "    axs[1].plot(histories[model_name][\"val_f1\"], dashes=[6, 2], label=model_name)\n",
        "    axs[1].plot(histories[model_name][\"train_f1\"], label=model_name)\n",
        "_ = axs[0].set_title(\"Val and Train Loss\")\n",
        "_ = axs[1].set_title(\"Val dan Train F1\")\n",
        "_ = axs[1].set_xlabel(\"epochs\")\n",
        "_ = axs[0].set_ylabel(\"loss\")\n",
        "_ = axs[1].set_ylabel(\"F1\")\n",
        "_ = axs[0].legend(loc=\"upper right\")\n",
        "_ = axs[1].legend(loc=\"lower right\")\n",
        "\n",
        "wandb.log({\"train_val_loss_f1_chart\":wandb.Image(fig)})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "id": "Xwdd9D6Bl9GS",
        "outputId": "d7e14640-b129-40e4-e8ac-529f8389f09e",
        "gather": {
          "logged": 1740734424472
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_test_f1 = [(m, histories[m][\"test_f1\"]) for m in histories]\n",
        "model_test_f1_sorted = sorted(model_test_f1, key=lambda m: m[1])\n",
        "model_names = [m[0] for m in model_test_f1_sorted]\n",
        "y_pos = list(range(len(model_names)))\n",
        "f1_scores = [m[1] for m in model_test_f1_sorted]\n",
        "flatten_f1 =  [element for sublist in f1_scores for element in sublist]\n",
        "print(str(flatten_f1))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.barh(y_pos, flatten_f1, align='center')\n",
        "_ = ax.set_yticks(y_pos)\n",
        "_ = ax.set_yticklabels(model_names)\n",
        "_ = ax.set_title(\"Test F1\")\n",
        "\n",
        "wandb.log({\"test_f1_chart\":wandb.Image(fig)})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "U4ti17nHl_1T",
        "outputId": "7be5a563-0f24-4c98-ac47-a908247ab374",
        "gather": {
          "logged": 1740734424549
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Testing"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "from tqdm import tqdm"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740734424628
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_csv(\"/home/jupyter-pfsa-id/corpus/data_paragraph.csv\",converters={'tags': literal_eval})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740734424690
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740734424756
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in configs:\n",
        "    print(f\"Sample inferences for model: {model_name}\")\n",
        "\n",
        "    model = NERModel(**configs[model_name]).to(use_device)\n",
        "    tester = Trainer(\n",
        "        model=model,\n",
        "        data=corpus,\n",
        "        optimizer=Adam(model.parameters(), lr=lrs[model_name], weight_decay=1e-2),  # add weight decay for Adam\n",
        "        device=use_device,\n",
        "        checkpoint_path=f\"{DRIVE_ROOT}/models-wandb-7030-v2/{model_name}.pt\",\n",
        "        model_name=f\"{model_name}\"\n",
        "    )\n",
        "    tester.model.load_state(f\"{DRIVE_ROOT}/models-wandb-7030-v2/{model_name}.pt\")\n",
        "\n",
        "    for index, row in tqdm(data_test.iterrows(),total=data_test.shape[0]):\n",
        "        words, infer_tags, unknown_tokens = tester.infer(docs=row[\"paragraph\"], index=index, true_tags=row[\"tags\"])\n",
        "    \n",
        "    print()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1740734424830
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740734424899
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1740734424980
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iYkejIkFF1Wh",
        "lw7AEpNmGqbD"
      ],
      "name": "8 - Optimizing BiLSTM and Transformer-based model for Indonesian NER",
      "provenance": []
    },
    "kernelspec": {
      "name": "custom_37",
      "language": "python",
      "display_name": "Python_37"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "custom_37"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}