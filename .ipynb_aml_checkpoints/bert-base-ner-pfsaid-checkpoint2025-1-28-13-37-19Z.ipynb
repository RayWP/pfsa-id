{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Already done\n",
        "# pip install transformers==2.9\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "125562b3-559c-46d6-a17e-fd827757b1aa"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1740749263445
        }
      },
      "id": "6de8e206-d2fa-4cfa-9596-414c48965711"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = []\n",
        "labels_train = []\n",
        "\n",
        "tokens = []\n",
        "token_labels = []\n",
        "unique_labels_train = set()\n",
        "\n",
        "with open(\"corpus-raymond/train-full.txt\", newline = '') as lines:                                                                                          \n",
        "  \n",
        "    line_reader = csv.reader(lines, delimiter='\\t')\n",
        "\n",
        "    for line in line_reader:\n",
        "        \n",
        "        if line == []:\n",
        "\n",
        "            sentences_train.append(tokens)\n",
        "            labels_train.append(token_labels)           \n",
        "    \n",
        "            tokens = []\n",
        "            token_labels = []        \n",
        "\n",
        "        else: \n",
        "            #print(str(line[0]))\n",
        "            tokens.append(line[0])\n",
        "            token_labels.append(line[1])\n",
        "\n",
        "            unique_labels_train.add(line[1])"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1740749389702
        }
      },
      "id": "d56252cd-d718-4c80-8ccc-bc8ba88bc41a"
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "sentences_dev = []\n",
        "labels_dev = []\n",
        "unique_labels_dev = set()\n",
        "\n",
        "tokens = []\n",
        "token_labels = []\n",
        "\n",
        "with open(\"corpus-raymond/validation-full.txt\", newline='', encoding='utf-8') as file:\n",
        "    line_reader = csv.reader(file, delimiter='\\t')\n",
        "\n",
        "    for line in line_reader:\n",
        "        # Remove empty spaces and ensure valid parsing\n",
        "        line = [x.strip() for x in line if x.strip()]  # Strip whitespace & ignore empty columns\n",
        "        \n",
        "        if not line:  # If it's an empty line, treat it as a sentence separator\n",
        "            if tokens:  # Avoid adding empty lists\n",
        "                sentences_dev.append(tokens)\n",
        "                labels_dev.append(token_labels)\n",
        "                tokens, token_labels = [], []  # Reset for next sentence\n",
        "        else:\n",
        "            if len(line) == 1 and line[0] == '\\\"':  # Handle single double-quote case\n",
        "                tokens.append('\\\"')\n",
        "                token_labels.append('O')  # Assuming label should be 'O' if unknown\n",
        "            elif len(line) >= 2:  # Normal case (word, label)\n",
        "                tokens.append(line[0])\n",
        "                token_labels.append(line[1])\n",
        "                unique_labels_dev.add(line[1])\n",
        "\n",
        "# Ensure last collected sentence is added\n",
        "if tokens:\n",
        "    sentences_dev.append(tokens)\n",
        "    labels_dev.append(token_labels)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['the', 'O']\n['clinic', 'O']\n['basically', 'O']\n['schedule', 'O']\n['patient', 'B-class']\n[',', 'O']\n['provide', 'O']\n['service', 'B-class']\n['for', 'O']\n['they', 'O']\n[',', 'O']\n['and', 'O']\n['bill', 'O']\n['they', 'O']\n['for', 'O']\n['those', 'O']\n['service', 'B-class']\n['.', 'O']\n['new', 'O']\n['patient', 'B-class']\n['fill', 'O']\n['out', 'O']\n['a', 'O']\n['form', 'O']\n['list', 'O']\n['their', 'O']\n['name', 'B-attr']\n[',', 'O']\n['address', 'B-attr']\n[',', 'O']\n['telephone', 'B-attr']\n['number', 'I-attr']\n[',', 'O']\n['allergy', 'B-attr']\n[',', 'O']\n['and', 'O']\n['state', 'B-attr']\n['of', 'I-attr']\n['mind', 'I-attr']\n['prior', 'O']\n['to', 'O']\n['schedule', 'O']\n['their', 'O']\n['first', 'O']\n['appointment', 'B-class']\n['.', 'O']\n['exist', 'O']\n['patient', 'B-class']\n['be', 'O']\n['normally', 'O']\n['schedule', 'O']\n['for', 'O']\n['their', 'O']\n['next', 'O']\n['appointment', 'B-class']\n['as', 'O']\n['they', 'O']\n['depart', 'O']\n['from', 'O']\n['their', 'O']\n['current', 'O']\n['appointment', 'B-class']\n['.', 'O']\n['when', 'O']\n['the', 'O']\n['office', 'B-class']\n['staff', 'I-class']\n['forget', 'O']\n['to', 'O']\n['do', 'O']\n['this', 'O']\n[',', 'O']\n['a', 'O']\n['desk', 'B-class']\n['worker', 'I-class']\n['have', 'O']\n['to', 'O']\n['call', 'O']\n['the', 'O']\n['patient', 'B-class']\n['to', 'O']\n['set', 'O']\n['up', 'O']\n['a', 'O']\n['date', 'B-attr']\n['.', 'O']\n['schedule', 'B-class']\n['be', 'O']\n['enter', 'O']\n['into', 'O']\n['a', 'O']\n['central', 'B-class']\n['appointment', 'I-class']\n['book', 'I-class']\n[';', 'O']\n['patient', 'B-class']\n['record', 'I-class']\n['(', 'O']\n['include', 'O']\n['contact', 'B-attr']\n['information', 'I-attr']\n[')', 'O']\n['be', 'O']\n['keep', 'O']\n['in', 'O']\n['paper', 'O']\n['file', 'O']\n['.', 'O']\n['appointment', 'B-class']\n['be', 'O']\n['for', 'O']\n['one', 'O']\n['of', 'O']\n['three', 'O']\n['procedure', 'B-class']\n[':', 'O']\n['dental', 'B-class']\n['hygiene', 'I-class']\n[',', 'O']\n['cavity', 'B-class']\n['and', 'I-class']\n['filling', 'I-class']\n[',', 'O']\n['and', 'O']\n['oral', 'B-class']\n['surgery', 'I-class']\n['(', 'O']\n['include', 'O']\n['root', 'O']\n['canal', 'O']\n['and', 'O']\n['tooth', 'O']\n['extraction', 'O']\n[')', 'O']\n['.', 'O']\n['for', 'O']\n['each', 'O']\n['procedure', 'B-class']\n['the', 'O']\n['patient', 'B-class']\n['need', 'O']\n['to', 'O']\n['be', 'O']\n['prepare', 'O']\n['and', 'O']\n['supply', 'B-attr']\n['need', 'O']\n['to', 'O']\n['be', 'O']\n['collect', 'O']\n['(', 'O']\n['e.g.', 'O']\n[',', 'O']\n['probe', 'O']\n[',', 'O']\n['drill', 'O']\n['bit', 'O']\n[',', 'O']\n['cement', 'O']\n[',', 'O']\n['resin', 'O']\n[',', 'O']\n['etc', 'O']\n['.', 'O']\n[')', 'O']\n['.', 'O']\n['for', 'O']\n['a', 'O']\n['hygienist', 'O']\n[\"'s\", 'O']\n['appointment', 'B-class']\n[',', 'O']\n['preparation', 'O']\n['could', 'O']\n['be', 'O']\n['as', 'O']\n['simple', 'O']\n['as', 'O']\n['seat', 'O']\n['the', 'O']\n['patient', 'O']\n['in', 'O']\n['dental', 'O']\n['chair', 'O']\n['and', 'O']\n['put', 'O']\n['a', 'O']\n['bib', 'O']\n['around', 'O']\n['his', 'O']\n['or', 'O']\n['her', 'O']\n['neck', 'O']\n['.', 'O']\n['for', 'O']\n['oral', 'O']\n['surgery', 'O']\n[',', 'O']\n['anesthesia', 'O']\n['of', 'O']\n['various', 'O']\n['strength', 'O']\n['be', 'O']\n['normally', 'O']\n['administer', 'O']\n['prior', 'O']\n['to', 'O']\n['operation', 'O']\n['.', 'O']\n['only', 'O']\n['for', 'O']\n['oral', 'O']\n['surgery', 'O']\n['procedure', 'B-class']\n['be', 'O']\n['it', 'O']\n['necessary', 'O']\n['to', 'O']\n['ask', 'O']\n['the', 'O']\n['patient', 'B-class']\n['to', 'O']\n['wait', 'O']\n['for', 'O']\n['up', 'O']\n['to', 'O']\n['twenty', 'O']\n['minute', 'O']\n['before', 'O']\n['perform', 'O']\n['a', 'O']\n['post', 'O']\n['-', 'O']\n['operative', 'O']\n['check', 'O']\n['.', 'O']\n['billing', 'O']\n['be', 'O']\n['always', 'O']\n['do', 'O']\n['by', 'O']\n['the', 'O']\n['month', 'O']\n[',', 'O']\n['and', 'O']\n['bill', 'B-class']\n['be', 'O']\n['always', 'O']\n['send', 'O']\n['by', 'O']\n['mail', 'O']\n['to', 'O']\n['patient', 'B-class']\n[\"'\", 'O']\n['contact', 'B-attr']\n['address', 'I-attr']\n['.', 'O']\n['check', 'O']\n['be', 'O']\n['receive', 'O']\n['by', 'O']\n['mail', 'O']\n['.', 'O']\n['HMO', 'B-class']\n['-', 'I-class']\n['fund', 'I-class']\n['patient', 'I-class']\n['be', 'O']\n['ask', 'O']\n['to', 'O']\n['make', 'O']\n['a', 'O']\n['copayment', 'O']\n['at', 'O']\n['the', 'O']\n['time', 'O']\n['that', 'O']\n['they', 'O']\n['leave', 'O']\n['the', 'O']\n['office', 'O']\n['.', 'O']\n['each', 'O']\n['patient', 'B-class']\n['also', 'O']\n['generate', 'O']\n['a', 'O']\n['reimbursement', 'B-class']\n['request', 'I-class']\n['to', 'O']\n['an', 'O']\n['insurance', 'B-class']\n['company', 'I-class']\n['.', 'O']\n['insurance', 'B-class']\n['company', 'I-class']\n['and', 'O']\n['hmo', 'B-class']\n['send', 'O']\n['their', 'O']\n['check', 'O']\n['by', 'O']\n['mail', 'O']\n['three', 'O']\n['month', 'O']\n['after', 'O']\n['receive', 'O']\n['a', 'O']\n['reimbursement', 'B-class']\n['request', 'I-class']\n['.', 'O']\n['the', 'O']\n['clinic', 'B-class']\n['maintain', 'O']\n['a', 'O']\n['supply', 'B-class']\n['inventory', 'I-class']\n['file', 'O']\n['that', 'O']\n['a', 'O']\n['worker', 'B-class']\n['fill', 'O']\n['out', 'O']\n['once', 'O']\n['a', 'O']\n['week', 'O']\n['by', 'O']\n['physically', 'O']\n['inspect', 'O']\n['each', 'O']\n['of', 'O']\n['the', 'O']\n['three', 'O']\n['procedure', 'B-class']\n['room', 'I-class']\n['.', 'O']\n['supply', 'B-class']\n['and', 'O']\n['tool', 'B-class']\n['be', 'O']\n['store', 'O']\n['in', 'O']\n['a', 'O']\n['standard', 'O']\n['layout', 'O']\n['in', 'O']\n['each', 'O']\n['room', 'B-class']\n['.', 'O']\n['geological', 'B-class']\n['sample', 'I-class']\n['be', 'O']\n['retrieve', 'O']\n['from', 'O']\n['the', 'O']\n['field', 'O']\n['and', 'O']\n['then', 'O']\n['process', 'O']\n['in', 'O']\n['the', 'O']\n['laboratory', 'B-class']\n['to', 'O']\n['determine', 'O']\n['various', 'O']\n['property', 'O']\n[',', 'O']\n['include', 'O']\n['chemistry', 'B-attr']\n[',', 'O']\n['mineralogy', 'B-attr']\n[',', 'O']\n['age', 'B-attr']\n[',', 'O']\n['and', 'O']\n['petrophysical', 'O']\n['property', 'O']\n['like', 'O']\n['density', 'B-attr']\n[',', 'O']\n['porosity', 'B-attr']\n[',', 'O']\n['permeability', 'B-attr']\n['.', 'O']\n['sample', 'B-class']\n['obtain', 'O']\n['as', 'O']\n['part', 'O']\n['of', 'O']\n['economic', 'O']\n['activity', 'O']\n[',', 'O']\n['such', 'O']\n['as', 'O']\n['mineral', 'O']\n['exploration', 'O']\n[',', 'O']\n['be', 'O']\n['usually', 'O']\n['process', 'O']\n['in', 'O']\n['commercial', 'B-class']\n['assay', 'I-class']\n['and', 'I-class']\n['chemistry', 'I-class']\n['lab', 'I-class']\n['.', 'O']\n['for', 'O']\n['QA', 'O']\n['/', 'O']\n['QC', 'O']\n['purpose', 'O']\n[',', 'O']\n['each', 'O']\n['batch', 'O']\n['of', 'O']\n['sample', 'B-class']\n['will', 'O']\n['have', 'O']\n['a', 'O']\n['number', 'O']\n['of', 'O']\n['control', 'B-class']\n['sample', 'B-class']\n['insert', 'O']\n[',', 'O']\n['for', 'O']\n['which', 'O']\n['the', 'O']\n['concentration', 'B-attr']\n['of', 'B-attr']\n['particular', 'B-attr']\n['chemical', 'B-attr']\n['specie', 'B-attr']\n['be', 'O']\n['already', 'O']\n['know', 'O']\n['.', 'O']\n['for', 'O']\n['confidentiality', 'O']\n['reason', 'O']\n['the', 'O']\n['location', 'B-attr']\n['information', 'I-attr']\n['associate', 'O']\n['with', 'O']\n['each', 'O']\n['sample', 'B-class']\n['be', 'O']\n['not', 'O']\n['provide', 'O']\n['to', 'O']\n['the', 'O']\n['lab', 'B-class']\n[',', 'O']\n['but', 'O']\n['must', 'O']\n['be', 'O']\n['re', 'O']\n['-', 'O']\n['attach', 'O']\n['during', 'O']\n['the', 'O']\n['interpretation', 'O']\n['phase', 'O']\n['.', 'O']\n['during', 'O']\n['processing', 'O']\n[',', 'O']\n['many', 'O']\n['derive', 'B-class']\n['sample', 'I-class']\n['will', 'O']\n['be', 'O']\n['generate', 'O']\n['by', 'O']\n['various', 'O']\n['physical', 'O']\n['and', 'O']\n['chemical', 'O']\n['procedure', 'O']\n['.', 'O']\n['in', 'O']\n['some', 'O']\n['case', 'O']\n['the', 'O']\n['derive', 'B-class']\n['sample', 'I-class']\n['be', 'O']\n['strict', 'O']\n['sub', 'B-class']\n['-', 'I-class']\n['sample', 'I-class']\n[',', 'O']\n['whose', 'O']\n['intensive', 'O']\n['property', 'O']\n['be', 'O']\n['intend', 'O']\n['to', 'O']\n['be', 'O']\n['the', 'O']\n['same', 'O']\n['as', 'O']\n['the', 'O']\n['parent', 'O']\n['.', 'O']\n['in', 'O']\n['other', 'O']\n['case', 'O']\n[',', 'O']\n['the', 'O']\n['split', 'O']\n['be', 'O']\n[\"'\", 'O']\n['biased', 'O']\n[\"'\", 'O']\n[',', 'O']\n['with', 'O']\n['the', 'O']\n['derive', 'B-class']\n['sample', 'I-class']\n['intend', 'O']\n['to', 'O']\n['select', 'O']\n['a', 'O']\n['specific', 'O']\n['sub', 'B-class']\n['-', 'I-class']\n['sample', 'I-class']\n[',', 'O']\n['define', 'O']\n['by', 'O']\n['a', 'O']\n['particular', 'O']\n['particle', 'B-attr']\n['size', 'I-attr']\n[',', 'O']\n['density', 'B-attr']\n[',', 'O']\n['magnetic', 'B-attr']\n['property', 'I-attr']\n[',', 'O']\n['etc', 'O']\n['.', 'O']\n['the', 'O']\n['link', 'O']\n['from', 'O']\n['the', 'O']\n['derived', 'B-class']\n['sample', 'I-class']\n['to', 'O']\n['the', 'O']\n['parent', 'B-class']\n['sample', 'I-class']\n['must', 'O']\n['be', 'O']\n['preserve', 'O']\n[',', 'O']\n['and', 'O']\n['the', 'O']\n['link', 'O']\n['from', 'O']\n['the', 'O']\n['parent', 'B-class']\n['to', 'O']\n['the', 'O']\n['location', 'B-attr']\n['from', 'O']\n['which', 'O']\n['it', 'O']\n['be', 'O']\n['obtain', 'O']\n['also', 'O']\n['.', 'O']\n['in', 'O']\n['some', 'O']\n['case', 'O']\n['the', 'O']\n['location', 'B-attr']\n['be', 'O']\n['associate', 'O']\n['with', 'O']\n['another', 'O']\n['sample', 'B-class']\n['artifact', 'I-class']\n[',', 'O']\n['such', 'O']\n['as', 'O']\n['a', 'O']\n['drill', 'O']\n['-', 'O']\n['hole', 'O']\n['or', 'O']\n['traverse', 'O']\n['or', 'O']\n['cruise', 'O']\n[',', 'O']\n['with', 'O']\n['the', 'O']\n['latter', 'O']\n['carry', 'O']\n['the', 'O']\n['detailed', 'B-attr']\n['location', 'I-attr']\n['information', 'I-attr']\n['.', 'O']\n['in', 'O']\n['a', 'O']\n['research', 'O']\n['context', 'O']\n['some', 'O']\n['sample', 'B-class']\n['have', 'O']\n['a', 'O']\n['particularly', 'O']\n['high', 'O']\n['-', 'O']\n['value', 'B-attr']\n[',', 'O']\n['having', 'O']\n['be', 'O']\n['obtain', 'O']\n['by', 'O']\n['an', 'O']\n['expensive', 'O']\n['process', 'O']\n['(', 'O']\n['involve', 'O']\n['drilling', 'O']\n['or', 'O']\n['ship', 'O']\n['or', 'O']\n['spacecraft', 'O']\n[')', 'O']\n['or', 'O']\n['from', 'O']\n['a', 'O']\n['location', 'B-attr']\n['that', 'O']\n['be', 'O']\n['hard', 'O']\n['to', 'O']\n['visit', 'O']\n['(', 'O']\n['remote', 'O']\n[',', 'O']\n['offshore', 'O']\n[',', 'O']\n['in', 'O']\n['space', 'O']\n[')', 'O']\n['.', 'O']\n['these', 'O']\n['sample', 'O']\n['be', 'O']\n['sometimes', 'O']\n['sub', 'O']\n['-', 'O']\n['divided', 'O']\n['and', 'O']\n['distribute', 'O']\n['to', 'O']\n['multiple', 'O']\n['research', 'O']\n['team', 'O']\n['or', 'O']\n['lab', 'B-class']\n['for', 'O']\n['different', 'O']\n['specialized', 'O']\n['observation', 'B-class']\n['.', 'O']\n['each', 'O']\n['lab', 'B-class']\n['will', 'O']\n['run', 'O']\n['its', 'O']\n['own', 'O']\n['lims', 'O']\n['system', 'O']\n[',', 'O']\n['which', 'O']\n['will', 'O']\n['usually', 'O']\n['assign', 'O']\n['a', 'O']\n['local', 'B-attr']\n['identifier', 'I-attr']\n['for', 'O']\n['the', 'O']\n['sample', 'B-class']\n['.', 'O']\n['when', 'O']\n['the', 'O']\n['result', 'O']\n['of', 'O']\n['these', 'O']\n['observation', 'B-class']\n['be', 'O']\n['report', 'O']\n[',', 'O']\n['it', 'O']\n['be', 'O']\n['necessary', 'O']\n['that', 'O']\n['observation', 'B-class']\n['from', 'O']\n['different', 'O']\n['lab', 'O']\n['can', 'O']\n['be', 'O']\n['correlate', 'O']\n['with', 'O']\n['each', 'O']\n['other', 'O']\n[',', 'O']\n['so', 'O']\n['that', 'O']\n['the', 'O']\n['complete', 'O']\n['picture', 'O']\n['around', 'O']\n['each', 'O']\n['sample', 'B-class']\n['can', 'O']\n['be', 'O']\n['assemble', 'O']\n['.', 'O']\n['these', 'O']\n['story', 'O']\n['focus', 'O']\n['on', 'O']\n['sense', 'O']\n['application', 'O']\n['involve', 'O']\n['ex', 'O']\n['-', 'O']\n['situ', 'O']\n['sampling', 'O']\n[',', 'O']\n['where', 'O']\n['a', 'O']\n['location', 'B-attr']\n['be', 'O']\n['visit', 'O']\n['and', 'O']\n['a', 'O']\n['speciman', 'B-class']\n['obtain', 'O']\n['use', 'O']\n['some', 'O']\n['sampling', 'O']\n['process', 'O']\n[',', 'O']\n['then', 'O']\n['transport', 'O']\n['to', 'O']\n['one', 'O']\n['or', 'O']\n['more', 'O']\n['laboratory', 'B-class']\n['where', 'O']\n['it', 'O']\n['be', 'O']\n['process', 'O']\n['into', 'O']\n['one', 'O']\n['or', 'O']\n['more', 'O']\n['sub', 'B-class']\n['-', 'B-class']\n['sample', 'B-class']\n['and', 'O']\n['various', 'O']\n['observation', 'B-class']\n['make', 'O']\n['.', 'O']\n['Sample', 'B-class']\n['identity', 'I-class']\n['be', 'O']\n['usually', 'O']\n['key', 'O']\n[',', 'O']\n['and', 'O']\n['the', 'O']\n['relationship', 'O']\n['between', 'O']\n['sample', 'B-class']\n[',', 'O']\n['between', 'O']\n['sample', 'B-class']\n['and', 'O']\n['other', 'O']\n['artifact', 'B-class']\n['of', 'O']\n['the', 'O']\n['sampling', 'O']\n['process', 'O']\n[',', 'O']\n['and', 'O']\n['also', 'O']\n['with', 'O']\n['other', 'O']\n['geographic', 'B-attr']\n['feature', 'I-attr']\n['or', 'O']\n['location', 'B-attr']\n['.', 'O']\n['the', 'O']\n['sample', 'B-attr']\n['time', 'I-attr']\n['and', 'O']\n['analysis', 'B-attr']\n['and', 'O']\n['reporting', 'B-attr']\n['time', 'I-attr']\n['be', 'O']\n['all', 'O']\n['different', 'O']\n['.', 'O']\n['similar', 'O']\n['process', 'O']\n['apply', 'O']\n['to', 'O']\n['botanical', 'O']\n['sampling', 'O']\n[',', 'O']\n['and', 'O']\n['to', 'O']\n['environmental', 'O']\n['sampling', 'O']\n['(', 'O']\n['water', 'O']\n[',', 'O']\n['air', 'O']\n[',', 'O']\n['dust', 'O']\n[')', 'O']\n['.', 'O']\n['analyst', 'O']\n[':', 'O']\n['do', 'O']\n['you', 'O']\n['run', 'O']\n['into', 'O']\n['any', 'O']\n['challenge', 'O']\n['with', 'O']\n['international', 'B-class']\n['address', 'I-class']\n[',', 'O']\n['give', 'O']\n['the', 'O']\n['wide', 'O']\n['variation', 'O']\n['in', 'O']\n['address', 'B-attr']\n['format', 'I-attr']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['actually', 'O']\n[',', 'O']\n['that', 'O']\n['be', 'O']\n['a', 'O']\n['constant', 'O']\n['source', 'O']\n['of', 'O']\n['confusion', 'O']\n['and', 'O']\n['pain', 'O']\n['.', 'O']\n['we', 'O']\n['have', 'O']\n['have', 'O']\n['several', 'O']\n['situation', 'O']\n['where', 'O']\n[',', 'O']\n['due', 'O']\n['to', 'O']\n['an', 'O']\n['incorrect', 'O']\n['address', 'O']\n['format', 'O']\n[',', 'O']\n['the', 'O']\n['client', 'B-class']\n['do', 'O']\n['not', 'O']\n['receive', 'O']\n['correspondence', 'B-class']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['can', 'O']\n['you', 'O']\n['give', 'O']\n['I', 'O']\n['some', 'O']\n['example', 'O']\n['of', 'O']\n['this', 'O']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['yes', 'O']\n[',', 'O']\n['one', 'O']\n['happen', 'O']\n['recently', 'O']\n['.', 'O']\n['sometimes', 'O']\n[',', 'O']\n['address', 'B-class']\n['start', 'O']\n['with', 'O']\n['a', 'O']\n['house', 'B-attr']\n['name', 'I-attr']\n['or', 'O']\n['number', 'B-attr']\n[',', 'O']\n['but', 'O']\n['some', 'O']\n['overseas', 'B-class']\n['client', 'I-class']\n['put', 'O']\n['the', 'O']\n['city', 'B-attr']\n['or', 'O']\n['town', 'B-attr']\n['first', 'O']\n['.', 'O']\n['imagine', 'O']\n['the', 'O']\n['confusion', 'O']\n['of', 'O']\n['send', 'O']\n['something', 'O']\n['to', 'O']\n['Paris', 'O']\n['which', 'O']\n['we', 'O']\n['think', 'O']\n['be', 'O']\n['the', 'O']\n['city', 'B-attr']\n['but', 'O']\n['be', 'O']\n['in', 'O']\n['fact', 'O']\n['the', 'O']\n['house', 'B-attr']\n['name', 'I-attr']\n['!', 'O']\n['the', 'O']\n['other', 'O']\n['issue', 'O']\n['be', 'O']\n['that', 'O']\n['overseas', 'O']\n['zip', 'B-attr']\n['code', 'I-attr']\n['or', 'I-attr']\n['postal', 'I-attr']\n['code', 'I-attr']\n[',', 'O']\n['as', 'O']\n['they', 'O']\n['be', 'O']\n['sometimes', 'O']\n['call', 'O']\n[',', 'O']\n['be', 'O']\n['not', 'O']\n['always', 'O']\n['numeric', 'O']\n['—', 'O']\n['they', 'O']\n['can', 'O']\n['be', 'O']\n['a', 'O']\n['combination', 'O']\n['of', 'O']\n['number', 'O']\n['and', 'O']\n['letter', 'O']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['tell', 'O']\n['I', 'O']\n['about', 'O']\n['the', 'O']\n['process', 'O']\n['when', 'O']\n['you', 'O']\n['take', 'O']\n['on', 'O']\n['a', 'O']\n['new', 'O']\n['client', 'B-class']\n['.', 'O']\n['what', 'O']\n['information', 'O']\n['do', 'O']\n['you', 'O']\n['initially', 'O']\n['record', 'O']\n['about', 'O']\n['that', 'O']\n['person', 'O']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['remember', 'O']\n[',', 'O']\n['it', 'O']\n['’', 'O']\n['not', 'O']\n['always', 'O']\n['an', 'O']\n['individual', 'B-attr']\n[';', 'O']\n['it', 'O']\n['could', 'O']\n['be', 'O']\n['an', 'O']\n['organization', 'B-attr']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['when', 'O']\n['it', 'O']\n['’', 'O']\n['an', 'O']\n['organization', 'O']\n[',', 'O']\n['do', 'O']\n['you', 'O']\n['always', 'O']\n['need', 'O']\n['to', 'O']\n['know', 'O']\n['the', 'O']\n['contact', 'B-attr']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['yes', 'O']\n[',', 'O']\n['and', 'O']\n['sometimes', 'O']\n['there', 'O']\n['be', 'O']\n['more', 'O']\n['than', 'O']\n['one', 'O']\n['.', 'O']\n['for', 'O']\n['each', 'O']\n['client', 'B-class']\n['or', 'O']\n['representative', 'B-class']\n['of', 'I-class']\n['an', 'I-class']\n['organization', 'I-class']\n[',', 'O']\n['I', 'O']\n['need', 'O']\n['to', 'O']\n['write', 'O']\n['down', 'O']\n['their', 'O']\n['full', 'B-attr']\n['name', 'I-attr']\n['and', 'O']\n['how', 'O']\n['they', 'O']\n['prefer', 'O']\n['their', 'O']\n['honorific', 'B-attr']\n[':', 'O']\n['Mr.', 'O']\n[',', 'O']\n['Ms.', 'O']\n[',', 'O']\n['Mrs.', 'O']\n[',', 'O']\n['Dr.', 'O']\n[',', 'O']\n['etc', 'O']\n['.', 'O']\n['and', 'O']\n['of', 'O']\n['course', 'O']\n[',', 'O']\n['their', 'O']\n['email', 'B-attr']\n['address', 'I-attr']\n[',', 'O']\n['phone', 'B-attr']\n['number', 'I-attr']\n[',', 'O']\n['and', 'O']\n['postal', 'B-attr']\n['address', 'I-attr']\n[',', 'O']\n['and', 'O']\n['sometimes', 'O']\n['we', 'O']\n['have', 'O']\n['a', 'O']\n['primary', 'B-attr']\n['mailing', 'I-attr']\n['address', 'I-attr']\n['that', 'O']\n['could', 'O']\n['be', 'O']\n['different', 'O']\n['from', 'O']\n['the', 'O']\n['billing', 'B-attr']\n['address', 'I-attr']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['could', 'O']\n['you', 'O']\n['have', 'O']\n['a', 'O']\n['situation', 'O']\n['where', 'O']\n['multiple', 'O']\n['people', 'O']\n['have', 'O']\n['the', 'O']\n['same', 'O']\n['address', 'B-attr']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['this', 'O']\n['do', 'O']\n['happen', 'O']\n[',', 'O']\n['for', 'O']\n['example', 'O']\n[',', 'O']\n['when', 'O']\n['we', 'O']\n['contact', 'O']\n['different', 'O']\n['employee', 'O']\n['in', 'O']\n['the', 'O']\n['same', 'O']\n['office', 'O']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['what', 'O']\n['be', 'O']\n['your', 'O']\n['other', 'O']\n['responsibility', 'O']\n[',', 'O']\n['apart', 'O']\n['from', 'O']\n['create', 'O']\n['case', 'O']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['as', 'O']\n['a', 'O']\n['case', 'B-class']\n['progress', 'O']\n[',', 'O']\n['I', 'O']\n['need', 'O']\n['to', 'O']\n['record', 'O']\n['all', 'O']\n['the', 'O']\n['individual', 'B-class']\n['and', 'O']\n['organization', 'B-class']\n['that', 'O']\n['take', 'O']\n['part', 'O']\n['in', 'O']\n['the', 'O']\n['case', 'O']\n['activity', 'O']\n['and', 'O']\n['the', 'O']\n['specific', 'O']\n['role', 'B-attr']\n['they', 'O']\n['play', 'O']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['be', 'O']\n['it', 'O']\n['possible', 'O']\n['for', 'O']\n['an', 'O']\n['individual', 'O']\n['or', 'O']\n['organization', 'O']\n['to', 'O']\n['participate', 'O']\n['in', 'O']\n['multiple', 'O']\n['action', 'B-attr']\n['or', 'O']\n['event', 'B-attr']\n['in', 'O']\n['different', 'O']\n['case', 'B-class']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['yes', 'O']\n['.', 'O']\n['not', 'O']\n['only', 'O']\n['that', 'O']\n[',', 'O']\n['but', 'O']\n['some', 'O']\n['may', 'O']\n['play', 'O']\n['different', 'O']\n['role', 'B-attr']\n['within', 'O']\n['the', 'O']\n['same', 'O']\n['case', 'B-class']\n['.', 'O']\n['for', 'O']\n['example', 'O']\n[',', 'O']\n['the', 'O']\n['same', 'O']\n['party', 'B-class']\n['can', 'O']\n['be', 'O']\n['both', 'O']\n['the', 'O']\n['defendant', 'B-attr']\n['and', 'O']\n['witness', 'B-attr']\n['in', 'O']\n['the', 'O']\n['same', 'O']\n['case', 'B-class']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['so', 'O']\n[',', 'O']\n['what', 'O']\n['be', 'O']\n['these', 'O']\n['different', 'O']\n['type', 'O']\n['of', 'O']\n['role', 'O']\n['a', 'O']\n['party', 'O']\n['can', 'O']\n['play', 'O']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['Plaintiff', 'O']\n[',', 'O']\n['witness', 'O']\n[',', 'O']\n['defendant', 'O']\n[',', 'O']\n['judge', 'O']\n[',', 'O']\n['an', 'O']\n['expert', 'O']\n['in', 'O']\n['some', 'O']\n['field', 'O']\n[',', 'O']\n['or', 'O']\n['an', 'O']\n['attorney', 'O']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['can', 'O']\n['your', 'O']\n['firm', 'O']\n['’s', 'O']\n['attorney', 'O']\n['or', 'O']\n['judge', 'O']\n['be', 'O']\n['consider', 'O']\n['as', 'O']\n['party', 'B-class']\n['to', 'O']\n['the', 'O']\n['case', 'B-class']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['absolutely', 'O']\n['.', 'O']\n['and', 'O']\n['I', 'O']\n['must', 'O']\n['record', 'O']\n['the', 'O']\n['information', 'O']\n['about', 'O']\n['their', 'O']\n['involvement', 'O']\n['as', 'O']\n['well', 'O']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['please', 'O']\n['tell', 'O']\n['I', 'O']\n['about', 'O']\n['event', 'B-class']\n['that', 'O']\n['occur', 'O']\n['.', 'O']\n['what', 'O']\n['information', 'O']\n['do', 'O']\n['you', 'O']\n['record', 'O']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['all', 'O']\n['attorney', 'O']\n['and', 'O']\n['legal', 'O']\n['assistant', 'O']\n['record', 'O']\n['their', 'O']\n['own', 'O']\n['activity', 'B-class']\n[',', 'O']\n['which', 'O']\n['include', 'O']\n['the', 'O']\n['date', 'B-attr']\n['and', 'O']\n['time', 'B-attr']\n['when', 'O']\n['an', 'O']\n['activity', 'B-class']\n['occur', 'O']\n[',', 'O']\n['a', 'O']\n['short', 'B-attr']\n['description', 'I-attr']\n[',', 'O']\n['and', 'O']\n['a', 'O']\n['duration', 'B-attr']\n[',', 'O']\n['and', 'O']\n['in', 'O']\n['the', 'O']\n['case', 'O']\n['of', 'O']\n['witness', 'O']\n[',', 'O']\n['defendant', 'O']\n[',', 'O']\n['and', 'O']\n['judge', 'O']\n[',', 'O']\n['a', 'O']\n['list', 'B-attr']\n['of', 'I-attr']\n['who', 'I-attr']\n['be', 'I-attr']\n['involve', 'I-attr']\n['.', 'O']\n['we', 'O']\n['also', 'O']\n['need', 'O']\n['to', 'O']\n['indicate', 'O']\n['if', 'O']\n['this', 'O']\n['event', 'O']\n['be', 'O']\n['billable', 'B-attr']\n['or', 'O']\n['not', 'O']\n['.', 'O']\n['Analyst', 'O']\n[':', 'O']\n['be', 'O']\n['there', 'O']\n['anything', 'O']\n['else', 'O']\n['you', 'O']\n['record', 'O']\n['for', 'O']\n['case', 'B-class']\n['?', 'O']\n['AP', 'O']\n[':', 'O']\n['yes', 'O']\n[',', 'O']\n['we', 'O']\n['need', 'O']\n['to', 'O']\n['know', 'O']\n['which', 'O']\n['document', 'B-attr']\n['be', 'O']\n['use', 'O']\n['in', 'O']\n['a', 'O']\n['case', 'B-class']\n['.', 'O']\n['the', 'O']\n['right', 'O']\n['-', 'O']\n['way', 'O']\n['Rental', 'O']\n['Truck', 'O']\n['Company', 'O']\n['rent', 'O']\n['small', 'O']\n['move', 'O']\n['truck', 'O']\n['and', 'O']\n['trailer', 'O']\n['for', 'O']\n['local', 'O']\n['and', 'O']\n['one', 'O']\n['-', 'O']\n['way', 'O']\n['usage', 'O']\n['.', 'O']\n['we', 'O']\n['have', 'O']\n['347', 'O']\n['rental', 'B-class']\n['office', 'I-class']\n['across', 'O']\n['the', 'O']\n['western', 'O']\n['United', 'O']\n['States', 'O']\n['.', 'O']\n['our', 'O']\n['rental', 'B-class']\n['stock', 'I-class']\n['include', 'O']\n['a', 'O']\n['total', 'O']\n['of', 'O']\n['5,78', 'O']\n['vehicle', 'B-class']\n['include', 'O']\n['various', 'O']\n['type', 'O']\n['of', 'O']\n['truck', 'B-class']\n['and', 'O']\n['trailer', 'B-class']\n['.', 'O']\n['we', 'O']\n['need', 'O']\n['to', 'O']\n['implement', 'O']\n['a', 'O']\n['system', 'O']\n['to', 'O']\n['track', 'O']\n['our', 'O']\n['rental', 'B-class']\n['agreement', 'I-class']\n['and', 'O']\n['our', 'O']\n['vehicle', 'B-class']\n['assignment', 'I-class']\n['.', 'O']\n['each', 'O']\n['rental', 'B-class']\n['office', 'I-class']\n['rent', 'O']\n['vehicle', 'B-class']\n['that', 'O']\n['they', 'O']\n['have', 'O']\n['in', 'O']\n['stock', 'O']\n['to', 'O']\n['customer', 'B-class']\n['ready', 'O']\n['to', 'O']\n['take', 'O']\n['possession', 'O']\n['of', 'O']\n['the', 'O']\n['vehicle', 'B-class']\n['.', 'O']\n['we', 'O']\n['do', 'O']\n['not', 'O']\n['take', 'O']\n['reservation', 'B-class']\n[',', 'O']\n['or', 'O']\n['speculate', 'O']\n['on', 'O']\n['when', 'O']\n['the', 'O']\n['customer', 'B-class']\n['will', 'O']\n['return', 'O']\n['rent', 'O']\n['vehicle', 'B-class']\n['.', 'O']\n['the', 'O']\n['central', 'B-class']\n['office', 'I-class']\n['oversee', 'O']\n['the', 'O']\n['vehicle', 'O']\n['distribution', 'O']\n[',', 'O']\n['and', 'O']\n['direct', 'O']\n['transfer', 'O']\n['of', 'O']\n['vehicle', 'O']\n['from', 'O']\n['one', 'O']\n['rental', 'B-class']\n['office', 'I-class']\n['to', 'O']\n['another', 'O']\n['.', 'O']\n['each', 'O']\n['rental', 'B-class']\n['office', 'I-class']\n['have', 'O']\n['an', 'O']\n['office', 'B-attr']\n['name', 'I-attr']\n['like', 'O']\n['\\tO\\nLittleton\\tO\\nright\\tO\\n-\\tO\\nway\\tO\\n', 'O']\n['.', 'O']\n['each', 'O']\n['office', 'B-class']\n['also', 'O']\n['have', 'O']\n['a', 'O']\n['unique', 'B-attr']\n['three', 'I-attr']\n['digit', 'I-attr']\n['office', 'I-attr']\n['number', 'I-attr']\n['.', 'O']\n['we', 'O']\n['also', 'O']\n['keep', 'O']\n['each', 'O']\n['office', 'O']\n['’s', 'O']\n['address', 'B-attr']\n['.', 'O']\n['each', 'O']\n['office', 'B-class']\n['be', 'O']\n['a', 'O']\n['home', 'O']\n['office', 'O']\n['for', 'O']\n['some', 'O']\n['of', 'O']\n['our', 'O']\n['vehicle', 'B-class']\n[',', 'O']\n['and', 'O']\n['each', 'O']\n['vehicle', 'B-class']\n['be', 'O']\n['base', 'O']\n['out', 'O']\n['of', 'O']\n['a', 'O']\n['single', 'O']\n['home', 'O']\n['office', 'B-class']\n['.', 'O']\n['each', 'O']\n['vehicle', 'B-class']\n['have', 'O']\n['a', 'O']\n['vehicle', 'B-attr']\n['I', 'I-attr']\n['d', 'I-attr']\n[',', 'O']\n['state', 'B-attr']\n['of', 'I-attr']\n['registration', 'I-attr']\n[',', 'O']\n['and', 'O']\n['a', 'O']\n['license', 'B-attr']\n['plate', 'I-attr']\n['registration', 'I-attr']\n['number', 'I-attr']\n['.', 'O']\n['we', 'O']\n['have', 'O']\n['five', 'O']\n['different', 'O']\n['type', 'O']\n['of', 'O']\n['vehicle', 'B-class']\n[':', 'O']\n['36', 'B-class']\n['truck', 'I-class']\n[',', 'O']\n['24', 'B-class']\n[\"'\", 'I-class']\n['truck', 'I-class']\n[',', 'O']\n['10', 'B-class']\n[\"'\", 'I-class']\n['truck', 'I-class']\n[',', 'O']\n['8', 'B-class']\n[\"'\", 'I-class']\n['cover', 'I-class']\n['trailer', 'I-class']\n[',', 'O']\n['and', 'O']\n['6', 'B-class']\n[\"'\", 'I-class']\n['open', 'I-class']\n['trailer', 'I-class']\n['.', 'O']\n['yes', 'O']\n[',', 'O']\n['we', 'O']\n['do', 'O']\n['have', 'O']\n['a', 'O']\n['vehicle', 'B-attr']\n['type', 'I-attr']\n['code', 'I-attr']\n['.', 'O']\n['for', 'O']\n['all', 'O']\n['our', 'O']\n['vehicle', 'B-class']\n[',', 'O']\n['we', 'O']\n['need', 'O']\n['to', 'O']\n['track', 'O']\n['the', 'O']\n['last', 'B-attr']\n['maintenance', 'I-attr']\n['date', 'I-attr']\n[',', 'O']\n['and', 'O']\n['expiration', 'B-attr']\n['date', 'I-attr']\n['of', 'I-attr']\n['its', 'I-attr']\n['registration', 'I-attr']\n['.', 'O']\n['for', 'O']\n['our', 'O']\n['truck', 'B-class']\n[',', 'O']\n['we', 'O']\n['need', 'O']\n['to', 'O']\n['know', 'O']\n['the', 'O']\n['current', 'B-attr']\n['odometer', 'I-attr']\n['reading', 'I-attr']\n[',', 'O']\n['the', 'O']\n['gas', 'B-attr']\n['tank', 'I-attr']\n['capacity', 'I-attr']\n[',', 'O']\n['and', 'O']\n['whether', 'O']\n['or', 'O']\n['not', 'O']\n['it', 'O']\n['have', 'O']\n['a', 'O']\n['work', 'B-attr']\n['radio', 'I-attr']\n['.', 'O']\n['for', 'O']\n['long', 'O']\n['move', 'O']\n[',', 'O']\n['customer', 'B-class']\n['really', 'O']\n['prefer', 'O']\n['a', 'O']\n['radio', 'O']\n['.', 'O']\n['we', 'O']\n['log', 'O']\n['the', 'O']\n['current', 'B-attr']\n['mileage', 'I-attr']\n['just', 'O']\n['before', 'O']\n['we', 'O']\n['rent', 'O']\n['a', 'O']\n['truck', 'B-class']\n[',', 'O']\n['and', 'O']\n['then', 'O']\n['again', 'O']\n['when', 'O']\n['it', 'O']\n['be', 'O']\n['return', 'O']\n['.', 'O']\n['Most', 'O']\n['of', 'O']\n['our', 'O']\n['rental', 'B-class']\n['agreement', 'I-class']\n['be', 'O']\n['for', 'O']\n['individual', 'B-class']\n['customer', 'I-class']\n[',', 'O']\n['but', 'O']\n['a', 'O']\n['rental', 'B-class']\n['agreement', 'I-class']\n['can', 'O']\n['either', 'O']\n['be', 'O']\n['for', 'O']\n['an', 'O']\n['individual', 'B-class']\n['or', 'O']\n['for', 'O']\n['a', 'O']\n['company', 'B-class']\n['.', 'O']\n['we', 'O']\n['do', 'O']\n['rent', 'O']\n['a', 'O']\n['small', 'O']\n['percentage', 'O']\n['of', 'O']\n['our', 'O']\n['truck', 'B-class']\n['to', 'O']\n['company', 'B-class']\n['.', 'O']\n['we', 'O']\n['assign', 'O']\n['each', 'O']\n['company', 'B-class']\n['an', 'O']\n['identify', 'B-attr']\n['company', 'I-attr']\n['number', 'I-attr']\n['and', 'O']\n['track', 'O']\n['the', 'O']\n['company', 'O']\n['’s', 'O']\n['name', 'B-attr']\n['and', 'O']\n['address', 'B-attr']\n['.', 'O']\n['no', 'O']\n[',', 'O']\n['we', 'O']\n['do', 'O']\n['not', 'O']\n['need', 'O']\n['to', 'O']\n['worry', 'O']\n['about', 'O']\n['any', 'O']\n['additional', 'O']\n['information', 'O']\n['about', 'O']\n['a', 'O']\n['company', 'O']\n['.', 'O']\n['our', 'O']\n['corporate', 'O']\n['sale', 'O']\n['group', 'O']\n['handle', 'O']\n['all', 'O']\n['that', 'O']\n['information', 'O']\n['separately', 'O']\n['.', 'O']\n['for', 'O']\n['each', 'O']\n['individual', 'B-class']\n['customer', 'I-class']\n[',', 'O']\n['we', 'O']\n['record', 'O']\n['the', 'O']\n['customer', 'O']\n['’s', 'O']\n['name', 'B-attr']\n[',', 'O']\n['home', 'B-attr']\n['phone', 'I-attr']\n[',', 'O']\n['address', 'B-attr']\n[',', 'O']\n['and', 'O']\n['driver', 'B-attr']\n['’s', 'I-attr']\n['license', 'I-attr']\n['state', 'I-attr']\n[',', 'O']\n['number', 'B-attr']\n[',', 'O']\n['and', 'O']\n['expiration', 'B-attr']\n['date', 'I-attr']\n['.', 'O']\n['we', 'O']\n['like', 'O']\n['to', 'O']\n['keep', 'O']\n['track', 'O']\n['of', 'O']\n['all', 'O']\n['our', 'O']\n['customer', 'B-class']\n['.', 'O']\n['if', 'O']\n['a', 'O']\n['customer', 'B-class']\n['damage', 'O']\n['a', 'O']\n['vehicle', 'B-class']\n[',', 'O']\n['abandon', 'O']\n['it', 'O']\n[',', 'O']\n['or', 'O']\n['do', 'O']\n['not', 'O']\n['fully', 'O']\n['pay', 'O']\n['the', 'O']\n['bill', 'O']\n[',', 'O']\n['then', 'O']\n['we', 'O']\n['tag', 'O']\n['the', 'O']\n['customer', 'B-class']\n['as', 'O']\n['a', 'O']\n['poor', 'B-attr']\n['risk', 'I-attr']\n[',', 'O']\n['and', 'O']\n['will', 'O']\n['not', 'O']\n['rent', 'O']\n['to', 'O']\n['that', 'O']\n['customer', 'B-class']\n['again', 'O']\n['.', 'O']\n['we', 'O']\n['only', 'O']\n['allow', 'O']\n['a', 'O']\n['single', 'O']\n['individual', 'B-class']\n['or', 'O']\n['company', 'B-class']\n['for', 'O']\n['a', 'O']\n['give', 'O']\n['rental', 'B-class']\n['agreement', 'I-class']\n[',', 'O']\n['and', 'O']\n['we', 'O']\n['write', 'O']\n['a', 'O']\n['separate', 'O']\n['rental', 'B-class']\n['agreement', 'I-class']\n['for', 'O']\n['each', 'O']\n['vehicle', 'B-class']\n['.', 'O']\n['yes', 'O']\n[',', 'O']\n['we', 'O']\n['do', 'O']\n['have', 'O']\n['customer', 'B-class']\n['rent', 'O']\n['two', 'O']\n['or', 'O']\n['more', 'O']\n['vehicle', 'B-class']\n['at', 'O']\n['the', 'O']\n['same', 'O']\n['time', 'O']\n['.', 'O']\n['each', 'O']\n['rental', 'B-class']\n['agreement', 'I-class']\n['be', 'O']\n['identify', 'O']\n['by', 'O']\n['the', 'O']\n['originate', 'B-attr']\n['rental', 'I-attr']\n['office', 'I-attr']\n['number', 'I-attr']\n['and', 'O']\n['a', 'O']\n['rental', 'B-attr']\n['agreement', 'I-attr']\n['number', 'I-attr']\n['.', 'O']\n['we', 'O']\n['also', 'O']\n['need', 'O']\n['to', 'O']\n['track', 'O']\n['the', 'O']\n['rental', 'B-attr']\n['date', 'I-attr']\n[',', 'O']\n['the', 'O']\n['anticipate', 'B-attr']\n['duration', 'I-attr']\n['of', 'I-attr']\n['the', 'I-attr']\n['rental', 'I-attr']\n[',', 'O']\n['the', 'O']\n['originate', 'B-attr']\n['rental', 'I-attr']\n['office', 'I-attr']\n[',', 'O']\n['the', 'O']\n['drop', 'B-attr']\n['-', 'I-attr']\n['off', 'I-attr']\n['rental', 'I-attr']\n['office', 'I-attr']\n[',', 'O']\n['the', 'O']\n['amount', 'B-attr']\n['of', 'I-attr']\n['the', 'I-attr']\n['deposit', 'I-attr']\n['pay', 'I-attr']\n[',', 'O']\n['the', 'O']\n['quote', 'I-attr']\n['daily', 'I-attr']\n['rental', 'I-attr']\n['rate', 'I-attr']\n[',', 'O']\n['and', 'O']\n['the', 'O']\n['quoted', 'B-attr']\n['rate', 'I-attr']\n['per', 'I-attr']\n['mile', 'I-attr']\n['.', 'O']\n['of', 'O']\n['course', 'O']\n['for', 'O']\n['the', 'O']\n['trailer', 'B-class']\n[',', 'O']\n['there', 'O']\n['be', 'O']\n['not', 'O']\n['a', 'O']\n['mileage', 'O']\n['charge', 'O']\n['.', 'O']\n['no', 'O']\n[',', 'O']\n['we', 'O']\n['do', 'O']\n['not', 'O']\n['need', 'O']\n['to', 'O']\n['automate', 'O']\n['the', 'O']\n['financial', 'O']\n['side', 'O']\n['of', 'O']\n['our', 'O']\n['business', 'O']\n[',', 'O']\n['just', 'O']\n['our', 'O']\n['rental', 'O']\n['agreement', 'O']\n['tracking', 'O']\n['and', 'O']\n['vehicle', 'O']\n['assignment', 'O']\n['function', 'O']\n['.', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n['be', 'O']\n['the', 'O']\n['fine', 'O']\n['italian', 'O']\n['restaurant', 'O']\n['in', 'O']\n['the', 'O']\n['city', 'O']\n['.', 'O']\n['unless', 'O']\n['you', 'O']\n['be', 'O']\n['a', 'O']\n['celebrity', 'O']\n['or', 'O']\n['a', 'O']\n['good', 'O']\n['friend', 'O']\n['of', 'O']\n['Romano', 'O']\n['you', 'O']\n['will', 'O']\n['need', 'O']\n['a', 'O']\n['reservation', 'B-class']\n['.', 'O']\n['a', 'O']\n['reservation', 'B-class']\n['be', 'O']\n['make', 'O']\n['for', 'O']\n['a', 'O']\n['specific', 'O']\n['time', 'B-attr']\n[',', 'O']\n['date', 'B-attr']\n['and', 'O']\n['number', 'B-attr']\n['of', 'I-attr']\n['people', 'I-attr']\n['.', 'O']\n['the', 'O']\n['reservation', 'B-class']\n['also', 'O']\n['capture', 'O']\n['the', 'O']\n['name', 'B-attr']\n['and', 'O']\n['phone', 'B-attr']\n['number', 'I-attr']\n['of', 'O']\n['the', 'O']\n['person', 'B-class']\n['make', 'O']\n['the', 'O']\n['reservation', 'B-class']\n['.', 'O']\n['each', 'O']\n['reservation', 'B-class']\n['be', 'O']\n['assign', 'O']\n['a', 'O']\n['unique', 'O']\n['reservation', 'B-attr']\n['number', 'I-attr']\n['.', 'O']\n['there', 'O']\n['be', 'O']\n['two', 'O']\n['category', 'O']\n['of', 'O']\n['reservation', 'B-class']\n['at', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n[':', 'O']\n['individual', 'B-class']\n['reservation', 'I-class']\n['and', 'O']\n['banquet', 'B-class']\n['reservation', 'I-class']\n['.', 'O']\n['additional', 'O']\n['reservation', 'O']\n['information', 'O']\n['capture', 'O']\n['when', 'O']\n['an', 'O']\n['individual', 'O']\n['make', 'O']\n['a', 'O']\n['reservation', 'B-class']\n['include', 'O']\n['seat', 'B-attr']\n['preference', 'I-attr']\n['(', 'O']\n['inside', 'O']\n['or', 'O']\n['patio', 'O']\n[')', 'O']\n['and', 'O']\n['smoke', 'B-attr']\n['preference', 'I-attr']\n['(', 'O']\n['smoking', 'O']\n['or', 'O']\n['nonsmoke', 'O']\n[')', 'O']\n['.', 'O']\n['additional', 'O']\n['reservation', 'O']\n['information', 'O']\n['capture', 'O']\n['for', 'O']\n['banquet', 'B-class']\n['reservation', 'I-class']\n['include', 'O']\n['the', 'O']\n['group', 'B-attr']\n['name', 'I-attr']\n['and', 'O']\n['the', 'O']\n['method', 'B-attr']\n['of', 'I-attr']\n['payment', 'I-attr']\n['.', 'O']\n['seating', 'O']\n['at', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n['be', 'O']\n['limited', 'O']\n['.', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n['have', 'O']\n['a', 'O']\n['fix', 'O']\n['number', 'O']\n['of', 'O']\n['table', 'O']\n['.', 'O']\n['each', 'O']\n['table', 'B-class']\n['be', 'O']\n['identify', 'O']\n['by', 'O']\n['a', 'O']\n['unique', 'O']\n['table', 'B-attr']\n['number', 'I-attr']\n['.', 'O']\n['each', 'O']\n['of', 'O']\n['the', 'O']\n['table', 'B-class']\n['be', 'O']\n['far', 'O']\n['describe', 'O']\n['by', 'O']\n['a', 'O']\n['unique', 'O']\n['free', 'B-attr']\n['form', 'I-attr']\n['description', 'I-attr']\n['such', 'O']\n['as', 'O']\n['\\tO\\nlocate\\tO\\nby\\tO\\nthe\\tO\\nnorth\\tO\\nwindow\\tO\\n', 'O']\n[',', 'O']\n['\\tO\\nlocate\\tO\\nin\\tO\\nfront\\tO\\nof\\tO\\nthe\\tO\\nfountain\\tO\\n', 'O']\n[',', 'O']\n['\\tO\\nby\\tO\\nthe\\tO\\nkitchen\\tO\\ndoor\\tO\\n', 'O']\n['.', 'O']\n['each', 'O']\n['table', 'B-class']\n['be', 'O']\n['classify', 'O']\n['as', 'O']\n['a', 'O']\n['2', 'B-attr']\n['-', 'I-attr']\n['person', 'I-attr']\n[',', 'O']\n['4', 'B-attr']\n['-', 'I-attr']\n['person', 'I-attr']\n['or', 'O']\n['6', 'B-attr']\n['-', 'I-attr']\n['person', 'I-attr']\n['table', 'I-attr']\n['.', 'O']\n['when', 'O']\n['a', 'O']\n['reservation', 'B-class']\n['be', 'O']\n['make', 'O']\n[',', 'O']\n['Romano', 'O']\n['associate', 'O']\n['a', 'O']\n['specific', 'O']\n['number', 'B-attr']\n['to', 'O']\n['the', 'O']\n['reservation', 'B-class']\n['.', 'O']\n['a', 'O']\n['table', 'B-class']\n['can', 'O']\n['be', 'O']\n['utilize', 'O']\n['many', 'O']\n['time', 'O']\n['over', 'O']\n['the', 'O']\n['evening', 'O']\n['by', 'O']\n['many', 'O']\n['reservation', 'B-class']\n['.', 'O']\n['Romano', 'O']\n['tend', 'O']\n['to', 'O']\n['overbook', 'O']\n['table', 'B-class']\n['.', 'O']\n['therefore', 'O']\n[',', 'O']\n['there', 'O']\n['can', 'O']\n['be', 'O']\n['overlap', 'O']\n['table', 'B-class']\n['reservation', 'I-class']\n['.', 'O']\n['the', 'O']\n['management', 'O']\n['structure', 'O']\n['at', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n['be', 'O']\n['hierarchical', 'O']\n['.', 'O']\n['there', 'O']\n['be', 'O']\n['several', 'O']\n['restaurant', 'B-class']\n['manager', 'I-class']\n['who', 'O']\n['report', 'O']\n['to', 'O']\n['Romano', 'B-class']\n['.', 'O']\n['the', 'O']\n['manager', 'B-class']\n['be', 'O']\n['responsible', 'O']\n['for', 'O']\n['manage', 'O']\n['the', 'O']\n[\"Maitre'd\", 'B-class']\n['and', 'O']\n['the', 'O']\n['chef', 'B-class']\n['as', 'O']\n['well', 'O']\n['as', 'O']\n['ensure', 'O']\n['that', 'O']\n['the', 'O']\n['guest', 'B-class']\n['have', 'O']\n['a', 'O']\n['pleasant', 'O']\n['dining', 'O']\n['experience', 'O']\n['.', 'O']\n['the', 'O']\n[\"Maitre'd\", 'B-class']\n['be', 'O']\n['responsible', 'O']\n['for', 'O']\n['manage', 'O']\n['the', 'O']\n['waiter', 'B-class']\n[',', 'O']\n['bartender', 'B-class']\n['and', 'O']\n['bus', 'B-class']\n['personnel', 'I-class']\n['.', 'O']\n['the', 'O']\n['Chefs', 'B-class']\n['be', 'O']\n['responsible', 'O']\n['for', 'O']\n['manage', 'O']\n['the', 'O']\n['cook', 'B-class']\n['and', 'O']\n['dishwasher', 'B-class']\n['.', 'O']\n['each', 'O']\n['person', 'B-class']\n['work', 'O']\n['for', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n['must', 'O']\n['be', 'O']\n['classify', 'O']\n['as', 'O']\n['either', 'O']\n['a', 'O']\n['manager', 'B-attr']\n[',', 'O']\n[\"Maitre'd\", 'B-attr']\n[',', 'O']\n['waiter', 'B-attr']\n[',', 'O']\n['bartender', 'B-attr']\n[',', 'O']\n['chef', 'B-attr']\n[',', 'O']\n['cook', 'B-attr']\n[',', 'O']\n['bus', 'B-attr']\n['person', 'I-attr']\n['or', 'O']\n['dishwasher', 'B-attr']\n['.', 'O']\n['additional', 'O']\n['information', 'O']\n['maintain', 'O']\n['by', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n['for', 'O']\n['each', 'O']\n['person', 'B-class']\n['include', 'O']\n['the', 'O']\n['person', 'O']\n['name', 'B-attr']\n[',', 'O']\n['date', 'B-attr']\n['of', 'I-attr']\n['birth', 'I-attr']\n['and', 'O']\n['driver', 'B-attr']\n['license', 'I-attr']\n['number', 'I-attr']\n['.', 'O']\n['when', 'O']\n['the', 'O']\n['reservation', 'O']\n['party', 'O']\n['arrive', 'O']\n['at', 'O']\n['Romano', 'O']\n[\"'s\", 'O']\n['the', 'O']\n['reservation', 'B-class']\n['be', 'O']\n['assign', 'O']\n['to', 'O']\n['one', 'O']\n['waiter', 'B-class']\n['.', 'O']\n['a', 'O']\n['waiter', 'B-class']\n['can', 'O']\n['be', 'O']\n['assign', 'O']\n['to', 'O']\n['many', 'O']\n['reservation', 'B-class']\n['during', 'O']\n['the', 'O']\n['course', 'O']\n['of', 'O']\n['the', 'O']\n['evening', 'O']\n[\"\\tO\\n.\\tO\\nthe\\tO\\n\\nmenu\\tB-class\\nat\\tO\\nRomano\\tO\\n's\\tO\\nbe\\tO\\nexquisite\\tO\\n.\\tO\\nthere\\tO\\n\\nbe\\tO\\nmany\\tO\\nexciting\\tO\\nand\\tO\\nexotic\\tO\\nitem\\tB-class\\n.\\tO\\neach\\tO\\n\\nmenu\\tB-class\\nitem\\tI-class\\nbe\\tO\\nidentify\\tO\\nby\\tO\\na\\tO\\nunique\\tO\\nmenu\\tB-attr\\nitem\\tI-attr\\nnumber\\tI-attr\\n.\\tO\\ninformation\\tO\\n\\nmaintain\\tO\\nby\\tO\\nRomano\\tO\\n's\\tO\\nfor\\tO\\neach\\tO\\nmenu\\tB-class\\nitem\\tI-class\\ninclude\\tO\\nan\\tO\\nitem\\tB-attr\\ndescription\\tI-attr\\nof\\tO\\n(\\tO\\ne.g.\\tO\\n\", 'O']\n['chicken', 'O']\n['marsala', 'O']\n['\\tO\\n,\\tO\\n', 'O']\n['fish', 'O']\n['soup', 'O']\n['\\tO\\n,\\tO\\n', 'O']\n['endive', 'O']\n['salad\",\"1988', 'O']\n['merlot', 'O']\n['wine', 'O']\n[\"\\tO\\n,\\tO\\netc\\tO\\n.\\tO\\n)\\tO\\n,\\tO\\nand\\tO\\nitem\\tB-attr\\nprep\\tI-attr\\ntime\\tI-attr\\n.\\tO\\neach\\tO\\n\\nmenu\\tB-class\\nitem\\tI-class\\nbe\\tO\\nclassify\\tO\\nby\\tO\\nRomano\\tO\\n's\\tO\\nas\\tO\\n\", 'O']\n['appetizer', 'B-attr']\n['\\tO\\n,\\tO\\n', 'O']\n['entree', 'B-attr']\n['\\tO\\n,\\tO\\n', 'O']\n['dessert', 'B-attr']\n['\\tO\\nor\\tO\\n', 'O']\n['beverage', 'B-attr']\n[\"\\tO\\n.\\tO\\nthe\\tO\\n\\nprice\\tB-attr\\nof\\tO\\neach\\tO\\nmenu\\tB-class\\nitem\\tI-class\\ncan\\tO\\nvary\\tO\\nbase\\tO\\non\\tO\\nthe\\tO\\ntime\\tO\\nof\\tO\\nday\\tO\\n.\\tO\\nfor\\tO\\n\\nexample\\tO\\n,\\tO\\nsome\\tO\\nof\\tO\\nthe\\tO\\nmenu\\tB-class\\nitem\\tI-class\\nhave\\tO\\ndifferent\\tO\\nlunch\\tB-attr\\nand\\tO\\ndinner\\tB-attr\\nprice\\tI-attr\\n.\\tO\\nsome\\tO\\n\\nof\\tO\\nthe\\tO\\nmenu\\tB-class\\nitem\\tI-class\\nchange\\tO\\nprice\\tB-attr\\nfor\\tI-attr\\nhappy\\tI-attr\\nhour\\tI-attr\\n.\\tO\\nin\\tO\\n\\norder\\tO\\nto\\tO\\ncalculate\\tO\\nthe\\tO\\ncheck\\tB-class\\nat\\tO\\nthe\\tO\\nend\\tO\\nof\\tO\\nthe\\tO\\ndinner\\tO\\n,\\tO\\nthe\\tO\\nwaiter\\tB-class\\nmaintain\\tO\\na\\tO\\nlist\\tB-class\\n,\\tO\\nby\\tO\\nreservation\\tB-attr\\nnumber\\tI-attr\\n,\\tO\\nof\\tO\\nthe\\tO\\nmenu\\tB-class\\nitem\\tI-class\\norder\\tO\\nand\\tO\\nthe\\tO\\ntime\\tB-attr\\nthat\\tI-attr\\nthe\\tI-attr\\nmenu\\tI-attr\\nitem\\tI-attr\\nbe\\tI-attr\\norder\\tI-attr\\n.\\tO\\nin\\tO\\n\\nother\\tO\\nword\\tO\\n,\\tO\\neach\\tO\\nreservation\\tB-class\\ncan\\tO\\nbe\\tO\\nassociate\\tO\\nwith\\tO\\nmany\\tO\\nmenu\\tB-class\\nitem\\tI-class\\nand\\tO\\na\\tO\\nmenu\\tB-class\\nitem\\tI-class\\ncan\\tO\\nbe\\tO\\nassociate\\tO\\nwith\\tO\\nmany\\tO\\nreservation\\tB-class\\n.\\tO\\nin\\tO\\n\\naddition\\tO\\nto\\tO\\nmenu\\tB-class\\nitem\\tI-class\\n,\\tO\\nRomano\\tO\\n's\\tO\\nmaintain\\tO\\na\\tO\\nlist\\tB-class\\nof\\tO\\nthe\\tO\\nfood\\tB-class\\nitem\\tI-class\\nthat\\tO\\nbe\\tO\\nutilize\\tO\\nby\\tO\\nthe\\tO\\nrestaurant\\tO\\nsuch\\tO\\nas\\tO\\nchicken\\tO\\n,\\tO\\nmushroom\\tO\\n,\\tO\\nbread\\tO\\nstick\\tO\\n,\\tO\\nred\\tO\\nsauce\\tO\\n,\\tO\\ncream\\tO\\nsauce\\tO\\n,\\tO\\netc\\tO\\n.\\tO\\nfood\\tB-class\\n\\nitem\\tI-class\\nbe\\tO\\nutilize\\tO\\nin\\tO\\nthe\\tO\\npreparation\\tO\\nof\\tO\\nmenu\\tB-class\\nitem\\tI-class\\n.\\tO\\neach\\tO\\n\\nfood\\tB-class\\nitem\\tI-class\\nbe\\tO\\nidentify\\tO\\nby\\tO\\na\\tO\\nunique\\tO\\nfood\\tB-attr\\nitem\\tI-attr\\nnumber\\tI-attr\\n.\\tO\\n\"]\n"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8b15e731077e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mtoken_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0munique_labels_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1740749742541
        }
      },
      "id": "570990f1-c1d2-4db0-bfd4-de6c18e39c75"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_test = []\n",
        "labels_test = []\n",
        "\n",
        "tokens = []\n",
        "token_labels = []\n",
        "unique_labels_test = set()\n",
        "\n",
        "with open(\"/home/jupyter-pfsa-id/corpus/test-data-bert.txt\", newline = '') as lines:                                                                                          \n",
        "  \n",
        "    line_reader = csv.reader(lines, delimiter='\\t')\n",
        "\n",
        "    for line in line_reader:\n",
        "        \n",
        "        if line == []:\n",
        "\n",
        "            sentences_test.append(tokens)\n",
        "            labels_test.append(token_labels)           \n",
        "    \n",
        "            tokens = []\n",
        "            token_labels = []        \n",
        "\n",
        "        else: \n",
        "            #print(str(line[0]))\n",
        "            tokens.append(line[0])\n",
        "            token_labels.append(line[1])\n",
        "\n",
        "            unique_labels_test.add(line[1])"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {},
      "id": "af4f8038-a683-4249-bda7-4ef05387acba"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_test[0][:10] # First 10 elements of sentence 1"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "['JAKARTA',\n ',',\n 'KOMPAS',\n '—',\n 'Seorang',\n 'perempuan',\n '34',\n 'tahun',\n 'berinisial',\n 'RCD']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {},
      "id": "059392ef-d305-4d6c-9aec-9172d08d38b1"
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels_test[0][:10]) # First 10 labels of sentence 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['O', 'O', 'O', 'O', 'B-ISSUE', 'I-ISSUE', 'I-ISSUE', 'I-ISSUE', 'I-ISSUE', 'I-ISSUE']\n"
        }
      ],
      "execution_count": 7,
      "metadata": {},
      "id": "819f9bc0-2329-47c6-b5e0-925c1d2089f0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Label values\n",
        "tag_values = list(unique_labels_train)\n",
        "tag_values.append(\"PAD\")\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {},
      "id": "69da810f-3060-4e7d-bf63-f780fb4e3e8d"
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare data for BERT\n",
        "# Import pytorch and transformers library\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "\n",
        "from tensorflow import keras \n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "torch.__version__"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "'1.5.1'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {},
      "id": "730e0067-57e8-43b2-a585-95a2e10cddd5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define BERT parameters\n",
        "# Sentence length\n",
        "MAX_LEN = 175\n",
        "# Batch size\n",
        "bs = 32 "
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {},
      "id": "d691ae5e-60ad-4639-a3c7-31a923461d8e"
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA device (GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "# Print state of GPU\n",
        "!nvidia-smi"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Mon Sep 19 04:37:10 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P4            Off  | 00000000:17:00.0 Off |                    0 |\n| N/A   36C    P0    23W /  75W |      2MiB /  7611MiB |      1%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n"
        }
      ],
      "execution_count": 11,
      "metadata": {},
      "id": "ba9e890b-5ba4-44a7-b580-480ec4481420"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BERT tokenizer\n",
        "# Use the BETO model (BERT for Spanish), available in the Transformers library  \n",
        "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased',use_fast=False)\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {},
      "id": "c7f3b613-0e01-4c10-8861-45f4b1815be0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize and preserve labels\n",
        "def tokenize_and_keep_labels(sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize each word and count number of its subwords\n",
        "        # We force conversion to string to avoid errors with float elements\n",
        "        tokenized_word = tokenizer.tokenize(str(word))\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # The tokenized word is added to the resulting tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # The same label is added to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {},
      "id": "2580c6f5-06b8-4b79-b453-5f90599f9b6f"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_and_labels_train = [\n",
        "    tokenize_and_keep_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_train, labels_train)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {},
      "id": "2ea005fd-0f8e-4661-ab2e-444d33a3fe95"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_and_labels_dev = [\n",
        "    tokenize_and_keep_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_dev, labels_dev)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {},
      "id": "98153f0b-0980-46cf-bd8e-707fff75f16e"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_and_labels_test = [\n",
        "    tokenize_and_keep_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_test, labels_test)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {},
      "id": "d670d60e-805a-4369-ba34-21888f059303"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_train = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_train]\n",
        "labels_train = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_train]\n",
        "\n",
        "tokenized_texts_dev = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_dev]\n",
        "labels_dev = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_dev]\n",
        "\n",
        "tokenized_texts_test = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_test]\n",
        "labels_test = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_test]"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {},
      "id": "a4c5cbc6-c1f0-4a4e-9c20-1a7c0c7510b7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding of sentences according to desired input length\n",
        "input_ids_train = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_train],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids_dev = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_dev],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids_test = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {},
      "id": "9e31ec4b-d77a-441d-8782-5a5cc2c18f67"
    },
    {
      "cell_type": "code",
      "source": [
        "# Paddding of labels with regard to input length\n",
        "tags_train = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_train],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "tags_dev = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_dev],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "tags_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_test],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {},
      "id": "6b24cdb0-e082-436c-aa3d-96bd71e46c59"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the attention mask to ignore the padded elements in the sequences during training, development and testing\n",
        "attention_masks_train = [[float(i != 0.0) for i in ii] for ii in input_ids_train]\n",
        "attention_masks_dev = [[float(i != 0.0) for i in ii] for ii in input_ids_dev]\n",
        "attention_masks_test = [[float(i != 0.0) for i in ii] for ii in input_ids_test]"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {},
      "id": "6e8eb718-1a6a-424d-9f2b-6722a0ff4346"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset to torch tensors\n",
        "train_inputs = torch.tensor(input_ids_train)\n",
        "dev_inputs = torch.tensor(input_ids_dev)\n",
        "test_inputs = torch.tensor(input_ids_test)\n",
        "train_tags = torch.tensor(tags_train)\n",
        "dev_tags = torch.tensor(tags_dev)\n",
        "test_tags = torch.tensor(tags_test)\n",
        "train_masks = torch.tensor(attention_masks_train)\n",
        "dev_masks = torch.tensor(attention_masks_dev)\n",
        "test_masks = torch.tensor(attention_masks_test)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {},
      "id": "43649f73-0361-4c00-a7cb-8c02d851ce65"
    },
    {
      "cell_type": "code",
      "source": [
        "# We define the dataloaders. \n",
        "# Shuffle the data for training using RandomSampler\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "# Load dev and test data sequentially with SequentialSampler.\n",
        "dev_data = TensorDataset(dev_inputs, dev_masks, dev_tags)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=bs)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {},
      "id": "f511d5b9-d6fd-4080-8d4f-058aa2cffc92"
    },
    {
      "cell_type": "code",
      "source": [
        "# The BertForTokenClassification class is used for token-level predictions. \n",
        "# It includes the BERT model and carries out token-level classification in the last layer\n",
        "# We use the Adam optimizer\n",
        "from transformers import BertForTokenClassification, AdamW "
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {},
      "id": "cd412309-b4e5-4971-9e29-61d9689ec8b7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and use the pretrained BETO model (BERT for Spanish)\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"indolem/indobert-base-uncased\",\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at indolem/indobert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at indolem/indobert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 24,
      "metadata": {},
      "id": "798b6d06-e01a-4b1d-8e35-278226257211"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model to the GPU\n",
        "model.cuda();"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {},
      "id": "651511cc-46d3-4fb4-835e-c9a0e446932c"
    },
    {
      "cell_type": "code",
      "source": [
        "# weight_decay is a regularization procedure with regard to the weight matrices\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/jupyter-pfsa-id/.local/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n"
        }
      ],
      "execution_count": 26,
      "metadata": {},
      "id": "b2e87b61-e29d-4868-9765-a4af1c354dd9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a scheduler to reduce the learning rate \n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs; the BERT paper uses 4\n",
        "epochs = 4\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {},
      "id": "8399d3b9-12d3-4ae1-8a9d-90ab004a11d1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to measure the progression of training\n",
        "# Done\n",
        "# !pip install seqeval"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {},
      "id": "7fa09649-860e-4e9c-877c-cf9e6a355879"
    },
    {
      "cell_type": "code",
      "source": [
        "import seqeval\n",
        "#from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
        "from sklearn.metrics import f1_score, classification_report, precision_recall_fscore_support\n",
        "from tqdm import tqdm, trange"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {},
      "id": "8d8e3d8f-3ddd-464f-acff-eb8b05289712"
    },
    {
      "cell_type": "code",
      "source": [
        "#import wandb\n",
        "#wandb.login()"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {},
      "id": "87c9efc8-7864-444e-9943-5942be8ba67e"
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb.init(project=\"pfsa-id-gtx1080ti-bert-v1\",entity=\"sigitpurnomo\")"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {},
      "id": "7705caa1-a74a-4e02-a648-b5e3f5c21e4c"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# To measure execution time of this cell\n",
        "\n",
        "# Train the model for; the BERT paper uses 4\n",
        "## Store the average loss after each epoch; these values are used to plot the loss.\n",
        "loss_values, development_loss_values = [], []\n",
        "\n",
        "#data_seqeval = {\n",
        "#    \"predicted_tags\": [],\n",
        "#    \"true_tags\": [],\n",
        "#}\n",
        "#df_seqeval = None\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    #\n",
        "    # Training\n",
        "    #\n",
        "    # Set the model into training mode\n",
        "    model.train()\n",
        "    # Reset the total loss for each epoch\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Transfer batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Remove previous gradients before each backward pass\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        # This returns the loss (not the model output) since we have input the labels.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Get the loss\n",
        "        loss = outputs[0]\n",
        "        # Backward pass to compute the gradients\n",
        "        loss.backward()\n",
        "        # Train loss\n",
        "        total_loss += loss.item()\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "    \n",
        "\n",
        "    # Store each loss value for plotting the learning curve afterwards\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    # After each training epoch, measure performance on development set\n",
        "\n",
        "    # Set the model into evaluation mode\n",
        "    model.eval()\n",
        "    # Reset the development loss for this epoch\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in dev_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # The model must not compute or save gradients, in order to save memory and speed up this step\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, compute predictions\n",
        "            # This will return the logits (logarithm of the odds), not the loss (we do not provide labels)\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Transfer logits and labels to CPU\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Compute the accuracy for this batch of development sentences\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "        \n",
        "        #data_seqeval[\"batch\"].append(str(batch))\n",
        "        #data_seqeval[\"true_tags\"].append(str(label_ids))\n",
        "        #data_seqeval[\"predicted_tags\"].append(str([list(p) for p in np.argmax(logits, axis=2)]))\n",
        "\n",
        "    #df_seqeval = pd.DataFrame(data_seqeval)\n",
        "    #wandb.log({f\"dataframe_seqeval\": wandb.Table(dataframe=df_seqeval)})\n",
        "    \n",
        "    eval_loss = eval_loss / len(dev_dataloader)\n",
        "    development_loss_values.append(eval_loss)\n",
        "    print(\"Development loss: {}\".format(eval_loss))\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "    dev_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "    print(\"Development F1 score: {}\".format(f1_score(pred_tags, dev_tags, average='micro')))\n",
        "    #print(\"Development classification report:\\n{}\".format(classification_report(pred_tags, dev_tags,digits=4)))\n",
        "    print()\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch:   0%|          | 0/4 [00:02<?, ?it/s]\n"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 7.43 GiB total capacity; 6.58 GiB already allocated; 6.81 MiB free; 6.91 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f49ea474536 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7f49ea6bdf1e in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7f49ea6bef9e in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #3: THCStorage_resize + 0x96 (0x7f4999ab53d6 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: at::native::(anonymous namespace)::resize_cuda_(at::Tensor&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>) + 0x9aa (0x7f499b2d8e4a in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0x2887de3 (0x7f499b2d9de3 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf0ffe2 (0x7f4999961fe2 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x1041be6 (0x7f4999a93be6 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf65018 (0x7f49999b7018 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: <unknown function> + 0x10c2780 (0x7f49d6326780 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2c9b47e (0x7f49d7eff47e in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x10c2780 (0x7f49d6326780 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::Tensor c10::Dispatcher::callUnboxed<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&) const + 0xb3 (0x7f49eaf37e43 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #13: <unknown function> + 0x28ac327 (0x7f49d7b10327 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::autograd::generated::MmBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1c4 (0x7f49d7b4b9d4 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2d89705 (0x7f49d7fed705 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f49d7feaa03 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f49d7feb7e2 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f49d7fe3e59 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f49eb1fbac8 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #20: <unknown function> + 0xc819d (0x7f4a0620e19d in /opt/tljh/user/bin/../lib/libstdc++.so.6)\nframe #21: <unknown function> + 0x76db (0x7f4a098f66db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #22: clone + 0x3f (0x7f4a0961f61f in /lib/x86_64-linux-gnu/libc.so.6)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 7.43 GiB total capacity; 6.58 GiB already allocated; 6.81 MiB free; 6.91 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7f49ea474536 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7f49ea6bdf1e in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7f49ea6bef9e in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)\nframe #3: THCStorage_resize + 0x96 (0x7f4999ab53d6 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #4: at::native::(anonymous namespace)::resize_cuda_(at::Tensor&, c10::ArrayRef<long>, c10::optional<c10::MemoryFormat>) + 0x9aa (0x7f499b2d8e4a in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0x2887de3 (0x7f499b2d9de3 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf0ffe2 (0x7f4999961fe2 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x1041be6 (0x7f4999a93be6 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #8: <unknown function> + 0xf65018 (0x7f49999b7018 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so)\nframe #9: <unknown function> + 0x10c2780 (0x7f49d6326780 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x2c9b47e (0x7f49d7eff47e in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x10c2780 (0x7f49d6326780 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::Tensor c10::Dispatcher::callUnboxed<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::OperatorHandle const&, at::Tensor const&, at::Tensor const&) const + 0xb3 (0x7f49eaf37e43 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #13: <unknown function> + 0x28ac327 (0x7f49d7b10327 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::autograd::generated::MmBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1c4 (0x7f49d7b4b9d4 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2d89705 (0x7f49d7fed705 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7f49d7feaa03 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7f49d7feb7e2 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_init(int) + 0x39 (0x7f49d7fe3e59 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7f49eb1fbac8 in /opt/tljh/user/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #20: <unknown function> + 0xc819d (0x7f4a0620e19d in /opt/tljh/user/bin/../lib/libstdc++.so.6)\nframe #21: <unknown function> + 0x76db (0x7f4a098f66db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #22: clone + 0x3f (0x7f4a0961f61f in /lib/x86_64-linux-gnu/libc.so.6)\n"
          ]
        }
      ],
      "execution_count": 32,
      "metadata": {},
      "id": "fb45a631-fd1a-464e-8796-46280326bff9"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zip(pred_tags, dev_tags)),\n",
        "               columns =['Pred', 'True'])"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pred_tags' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-595c00b59ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = pd.DataFrame(list(zip(pred_tags, dev_tags)),\n\u001b[0m\u001b[1;32m      2\u001b[0m                columns =['Pred', 'True'])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pred_tags' is not defined"
          ]
        }
      ],
      "execution_count": 33,
      "metadata": {},
      "id": "310dcdc4-f5e5-4d52-b817-1ddf90188e2f"
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/home/jupyter-pfsa-id/corpus/train-val-result-bert.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "bd1f278d-2ddc-4e4f-9c6d-52e435941af8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training loss\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
        "plt.plot(development_loss_values, 'r-o', label=\"validation loss\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Learning curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "8e860ba8-a0da-4cc5-912d-59d24da75563"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the model to the test set\n",
        "# Set again the model into evaluation mode\n",
        "model.eval()\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "input_ids_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # The model must not compute or store gradients\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate predictions.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "    # Transfer logits and labels to CPU\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    input_ids_list.extend(b_input_ids)\n",
        "    \n",
        "    # Calculate the accuracy for this batch of test sentences\n",
        "    eval_loss += outputs[0].mean().item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "test_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "#print(str(pred_tags))\n",
        "#print(str(test_tags))\n",
        "print(\"Test F1 score: {}\".format(f1_score(pred_tags, test_tags,average='micro'),digits=4))\n",
        "#print(\"Test classification report: {}\".format(classification_report(pred_tags, test_tags,digits=4)))\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "0d64d31a-e5b6-40ae-bbfb-0bf300a7ffc5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "#print(\"Optimizer's state_dict:\")\n",
        "#for var_name in optimizer.state_dict():\n",
        "#    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "6cdb58de-aa8e-4bca-b481-c8708e43a28c"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zip(pred_tags, test_tags)),\n",
        "               columns =['Pred', 'True'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "fb3fd637-aa6a-4435-9030-20f6060e1629"
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/home/jupyter-pfsa-id/corpus/test-result-bert.csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "a09dd8d6-39bf-47a5-a09b-d1cf897f9cee"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "10aa1d62-6169-430a-9d43-c38b970dec49"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "custom_37",
      "language": "python",
      "display_name": "Python_37"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "custom_37"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}