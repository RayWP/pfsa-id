{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Already done\n",
        "# pip install transformers==2.9\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "125562b3-559c-46d6-a17e-fd827757b1aa"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "folder_name = \"result-bert-base\""
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1740749263445
        }
      },
      "id": "6de8e206-d2fa-4cfa-9596-414c48965711"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train = []\n",
        "labels_train = []\n",
        "\n",
        "tokens = []\n",
        "token_labels = []\n",
        "unique_labels_train = set()\n",
        "\n",
        "with open(\"corpus-raymond/train-full.txt\", newline = '') as lines:                                                                                          \n",
        "  \n",
        "    line_reader = csv.reader(lines, delimiter='\\t')\n",
        "\n",
        "    for line in line_reader:\n",
        "        \n",
        "        if line == []:\n",
        "\n",
        "            sentences_train.append(tokens)\n",
        "            labels_train.append(token_labels)           \n",
        "    \n",
        "            tokens = []\n",
        "            token_labels = []        \n",
        "\n",
        "        else: \n",
        "            #print(str(line[0]))\n",
        "            tokens.append(line[0])\n",
        "            token_labels.append(line[1])\n",
        "\n",
        "            unique_labels_train.add(line[1])"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1740749389702
        }
      },
      "id": "d56252cd-d718-4c80-8ccc-bc8ba88bc41a"
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "sentences_dev = []\n",
        "labels_dev = []\n",
        "unique_labels_dev = set()\n",
        "\n",
        "tokens = []\n",
        "token_labels = []\n",
        "\n",
        "with open(\"corpus-raymond/validation-full.txt\", newline='', encoding='utf-8') as file:\n",
        "    line_reader = csv.reader(file, delimiter='\\t')\n",
        "\n",
        "    for line in line_reader:\n",
        "        # Remove empty spaces and ensure valid parsing\n",
        "        line = [x.strip() for x in line if x.strip()]  # Strip whitespace & ignore empty columns\n",
        "        \n",
        "        if not line:  # If it's an empty line, treat it as a sentence separator\n",
        "            if tokens:  # Avoid adding empty lists\n",
        "                sentences_dev.append(tokens)\n",
        "                labels_dev.append(token_labels)\n",
        "                tokens, token_labels = [], []  # Reset for next sentence\n",
        "        else:\n",
        "            if len(line) == 1 and line[0] == '\\\"':  # Handle single double-quote case\n",
        "                tokens.append('\\\"')\n",
        "                token_labels.append('O')  # Assuming label should be 'O' if unknown\n",
        "            elif len(line) >= 2:  # Normal case (word, label)\n",
        "                tokens.append(line[0])\n",
        "                token_labels.append(line[1])\n",
        "                unique_labels_dev.add(line[1])\n",
        "\n",
        "# Ensure last collected sentence is added\n",
        "if tokens:\n",
        "    sentences_dev.append(tokens)\n",
        "    labels_dev.append(token_labels)\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1740749841455
        }
      },
      "id": "570990f1-c1d2-4db0-bfd4-de6c18e39c75"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_test = []\n",
        "labels_test = []\n",
        "\n",
        "tokens = []\n",
        "token_labels = []\n",
        "unique_labels_test = set()\n",
        "\n",
        "with open(\"corpus-raymond/validation-full.txt\", newline='', encoding='utf-8') as file:\n",
        "    line_reader = csv.reader(file, delimiter='\\t')\n",
        "\n",
        "    for line in line_reader:\n",
        "        # Remove empty spaces and ensure valid parsing\n",
        "        line = [x.strip() for x in line if x.strip()]  # Strip whitespace & ignore empty columns\n",
        "        \n",
        "        if not line:  # If it's an empty line, treat it as a sentence separator\n",
        "            if tokens:  # Avoid adding empty lists\n",
        "                sentences_test.append(tokens)\n",
        "                labels_test.append(token_labels)\n",
        "                tokens, token_labels = [], []  # Reset for next sentence\n",
        "        else:\n",
        "            if len(line) == 1 and line[0] == '\\\"':  # Handle single double-quote case\n",
        "                tokens.append('\\\"')\n",
        "                token_labels.append('O')  # Assuming label should be 'O' if unknown\n",
        "            elif len(line) >= 2:  # Normal case (word, label)\n",
        "                tokens.append(line[0])\n",
        "                token_labels.append(line[1])\n",
        "                unique_labels_test.add(line[1])\n",
        "\n",
        "# Ensure last collected sentence is added\n",
        "if tokens:\n",
        "    sentences_test.append(tokens)\n",
        "    labels_test.append(token_labels)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1740749912456
        }
      },
      "id": "af4f8038-a683-4249-bda7-4ef05387acba"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_test[0][:10] # First 10 elements of sentence 1"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "['the',\n 'clinic',\n 'basically',\n 'schedule',\n 'patient',\n ',',\n 'provide',\n 'service',\n 'for',\n 'they']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1740749915483
        }
      },
      "id": "059392ef-d305-4d6c-9aec-9172d08d38b1"
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels_test[0][:10]) # First 10 labels of sentence 1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['O', 'O', 'O', 'O', 'B-class', 'O', 'O', 'B-class', 'O', 'O']\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1740749918439
        }
      },
      "id": "819f9bc0-2329-47c6-b5e0-925c1d2089f0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Label values\n",
        "tag_values = list(unique_labels_train)\n",
        "tag_values.append(\"PAD\")\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1740749921023
        }
      },
      "id": "69da810f-3060-4e7d-bf63-f780fb4e3e8d"
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare data for BERT\n",
        "# Import pytorch and transformers library\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "\n",
        "from tensorflow import keras \n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "torch.__version__"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "'1.12.1'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1740750105670
        }
      },
      "id": "730e0067-57e8-43b2-a585-95a2e10cddd5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define BERT parameters\n",
        "# Sentence length\n",
        "MAX_LEN = 175\n",
        "# Batch size\n",
        "bs = 32 "
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1740750108091
        }
      },
      "id": "d691ae5e-60ad-4639-a3c7-31a923461d8e"
    },
    {
      "cell_type": "code",
      "source": [
        "# CUDA device (GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "# torch.cuda.get_device_name(0)\n",
        "# Print state of GPU\n",
        "!nvidia-smi"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\r\n\r\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1740750118797
        }
      },
      "id": "ba9e890b-5ba4-44a7-b580-480ec4481420"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import BERT tokenizer\n",
        "# Use the BETO model (BERT for Spanish), available in the Transformers library  \n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',use_fast=False)\n"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1740751832638
        }
      },
      "id": "c7f3b613-0e01-4c10-8861-45f4b1815be0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to tokenize and preserve labels\n",
        "def tokenize_and_keep_labels(sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize each word and count number of its subwords\n",
        "        # We force conversion to string to avoid errors with float elements\n",
        "        tokenized_word = tokenizer.tokenize(str(word))\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # The tokenized word is added to the resulting tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # The same label is added to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1740751834567
        }
      },
      "id": "2580c6f5-06b8-4b79-b453-5f90599f9b6f"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_and_labels_train = [\n",
        "    tokenize_and_keep_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_train, labels_train)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1740751840181
        }
      },
      "id": "2ea005fd-0f8e-4661-ab2e-444d33a3fe95"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_and_labels_dev = [\n",
        "    tokenize_and_keep_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_dev, labels_dev)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1740751840438
        }
      },
      "id": "98153f0b-0980-46cf-bd8e-707fff75f16e"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_and_labels_test = [\n",
        "    tokenize_and_keep_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_test, labels_test)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1740751840608
        }
      },
      "id": "d670d60e-805a-4369-ba34-21888f059303"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts_train = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_train]\n",
        "labels_train = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_train]\n",
        "\n",
        "tokenized_texts_dev = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_dev]\n",
        "labels_dev = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_dev]\n",
        "\n",
        "tokenized_texts_test = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_test]\n",
        "labels_test = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_test]"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1740751842144
        }
      },
      "id": "a4c5cbc6-c1f0-4a4e-9c20-1a7c0c7510b7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding of sentences according to desired input length\n",
        "input_ids_train = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_train],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids_dev = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_dev],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids_test = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1740751843084
        }
      },
      "id": "9e31ec4b-d77a-441d-8782-5a5cc2c18f67"
    },
    {
      "cell_type": "code",
      "source": [
        "# Paddding of labels with regard to input length\n",
        "tags_train = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_train],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "tags_dev = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_dev],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "tags_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_test],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1740751844261
        }
      },
      "id": "6b24cdb0-e082-436c-aa3d-96bd71e46c59"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the attention mask to ignore the padded elements in the sequences during training, development and testing\n",
        "attention_masks_train = [[float(i != 0.0) for i in ii] for ii in input_ids_train]\n",
        "attention_masks_dev = [[float(i != 0.0) for i in ii] for ii in input_ids_dev]\n",
        "attention_masks_test = [[float(i != 0.0) for i in ii] for ii in input_ids_test]"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1740751845654
        }
      },
      "id": "6e8eb718-1a6a-424d-9f2b-6722a0ff4346"
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset to torch tensors\n",
        "train_inputs = torch.tensor(input_ids_train)\n",
        "dev_inputs = torch.tensor(input_ids_dev)\n",
        "test_inputs = torch.tensor(input_ids_test)\n",
        "train_tags = torch.tensor(tags_train)\n",
        "dev_tags = torch.tensor(tags_dev)\n",
        "test_tags = torch.tensor(tags_test)\n",
        "train_masks = torch.tensor(attention_masks_train)\n",
        "dev_masks = torch.tensor(attention_masks_dev)\n",
        "test_masks = torch.tensor(attention_masks_test)"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1740751847113
        }
      },
      "id": "43649f73-0361-4c00-a7cb-8c02d851ce65"
    },
    {
      "cell_type": "code",
      "source": [
        "# We define the dataloaders. \n",
        "# Shuffle the data for training using RandomSampler\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "# Load dev and test data sequentially with SequentialSampler.\n",
        "dev_data = TensorDataset(dev_inputs, dev_masks, dev_tags)\n",
        "dev_sampler = SequentialSampler(dev_data)\n",
        "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=bs)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1740751848517
        }
      },
      "id": "f511d5b9-d6fd-4080-8d4f-058aa2cffc92"
    },
    {
      "cell_type": "code",
      "source": [
        "# The BertForTokenClassification class is used for token-level predictions. \n",
        "# It includes the BERT model and carries out token-level classification in the last layer\n",
        "# We use the Adam optimizer\n",
        "from transformers import BertForTokenClassification, AdamW "
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1740751850614
        }
      },
      "id": "cd412309-b4e5-4971-9e29-61d9689ec8b7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model and use the pretrained BETO model (BERT for Spanish)\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09d8038af6514e51ba651ee074dc7225"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1740752757555
        }
      },
      "id": "798b6d06-e01a-4b1d-8e35-278226257211"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model to the GPU\n",
        "#model.cuda();"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0f3212989af2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the model to the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/custom_37/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             raise AssertionError(\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1740751982548
        }
      },
      "id": "651511cc-46d3-4fb4-835e-c9a0e446932c"
    },
    {
      "cell_type": "code",
      "source": [
        "# weight_decay is a regularization procedure with regard to the weight matrices\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "# Adam optimizer\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/custom_37/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1740752758242
        }
      },
      "id": "b2e87b61-e29d-4868-9765-a4af1c354dd9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a scheduler to reduce the learning rate \n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs; the BERT paper uses 4\n",
        "epochs = 4\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1740752758421
        }
      },
      "id": "8399d3b9-12d3-4ae1-8a9d-90ab004a11d1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import modules to measure the progression of training\n",
        "# Done\n",
        "# !pip install seqeval"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {},
      "id": "7fa09649-860e-4e9c-877c-cf9e6a355879"
    },
    {
      "cell_type": "code",
      "source": [
        "import seqeval\n",
        "#from seqeval.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
        "from sklearn.metrics import f1_score, classification_report, precision_recall_fscore_support\n",
        "from tqdm import tqdm, trange"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1740752758576
        }
      },
      "id": "8d8e3d8f-3ddd-464f-acff-eb8b05289712"
    },
    {
      "cell_type": "code",
      "source": [
        "#import wandb\n",
        "#wandb.login()"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1740751997513
        }
      },
      "id": "87c9efc8-7864-444e-9943-5942be8ba67e"
    },
    {
      "cell_type": "code",
      "source": [
        "#wandb.init(project=\"pfsa-id-gtx1080ti-bert-v1\",entity=\"sigitpurnomo\")"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {},
      "id": "7705caa1-a74a-4e02-a648-b5e3f5c21e4c"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# To measure execution time of this cell\n",
        "\n",
        "# Train the model for; the BERT paper uses 4\n",
        "## Store the average loss after each epoch; these values are used to plot the loss.\n",
        "loss_values, development_loss_values = [], []\n",
        "\n",
        "#data_seqeval = {\n",
        "#    \"predicted_tags\": [],\n",
        "#    \"true_tags\": [],\n",
        "#}\n",
        "#df_seqeval = None\n",
        "\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    #\n",
        "    # Training\n",
        "    #\n",
        "    # Set the model into training mode\n",
        "    model.train()\n",
        "    # Reset the total loss for each epoch\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Transfer batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Remove previous gradients before each backward pass\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        # This returns the loss (not the model output) since we have input the labels.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Get the loss\n",
        "        loss = outputs[0]\n",
        "        # Backward pass to compute the gradients\n",
        "        loss.backward()\n",
        "        # Train loss\n",
        "        total_loss += loss.item()\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
        "    \n",
        "\n",
        "    # Store each loss value for plotting the learning curve afterwards\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    # After each training epoch, measure performance on development set\n",
        "\n",
        "    # Set the model into evaluation mode\n",
        "    model.eval()\n",
        "    # Reset the development loss for this epoch\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions , true_labels = [], []\n",
        "    for batch in dev_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # The model must not compute or save gradients, in order to save memory and speed up this step\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, compute predictions\n",
        "            # This will return the logits (logarithm of the odds), not the loss (we do not provide labels)\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "        # Transfer logits and labels to CPU\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Compute the accuracy for this batch of development sentences\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "        \n",
        "        #data_seqeval[\"batch\"].append(str(batch))\n",
        "        #data_seqeval[\"true_tags\"].append(str(label_ids))\n",
        "        #data_seqeval[\"predicted_tags\"].append(str([list(p) for p in np.argmax(logits, axis=2)]))\n",
        "\n",
        "    #df_seqeval = pd.DataFrame(data_seqeval)\n",
        "    #wandb.log({f\"dataframe_seqeval\": wandb.Table(dataframe=df_seqeval)})\n",
        "    \n",
        "    eval_loss = eval_loss / len(dev_dataloader)\n",
        "    development_loss_values.append(eval_loss)\n",
        "    print(\"Development loss: {}\".format(eval_loss))\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "    dev_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "    f1 = f1_score(pred_tags, dev_tags, average='micro')\n",
        "\n",
        "    # Format output with 4 decimal places\n",
        "    output_text = \"train-val F1 score: {:.4f}\\n\".format(f1)\n",
        "\n",
        "    # Print to console\n",
        "    print(output_text)\n",
        "\n",
        "    # Save to a text file\n",
        "    with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
        "        file.write(output_text)\n",
        "    #print(\"Development classification report:\\n{}\".format(classification_report(pred_tags, dev_tags,digits=4)))\n",
        "    print()\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Average train loss: 0.6496942809649876\nDevelopment loss: 0.22905113995075227\nDevelopment F1 score: 0.3909057572423909\n\nAverage train loss: 0.1298628912440368\nDevelopment loss: 0.1523367702960968\nDevelopment F1 score: 0.7572423909057573\n\nAverage train loss: 0.11312464784298625\nDevelopment loss: 0.1467743396759033\nDevelopment F1 score: 0.756142280894756\n\nAverage train loss: 0.09680819325149059\n"
        }
      ],
      "execution_count": 45,
      "metadata": {},
      "id": "fb45a631-fd1a-464e-8796-46280326bff9"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zip(pred_tags, dev_tags)),\n",
        "               columns =['Pred', 'True'])"
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1740754424911
        }
      },
      "id": "310dcdc4-f5e5-4d52-b817-1ddf90188e2f"
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(folder_name + '/train-val-result-bert.csv')"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1740754953474
        }
      },
      "id": "bd1f278d-2ddc-4e4f-9c6d-52e435941af8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training loss\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
        "plt.plot(development_loss_values, 'r-o', label=\"validation loss\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Learning curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 864x432 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1740755227726
        }
      },
      "id": "8e860ba8-a0da-4cc5-912d-59d24da75563"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the model to the test set\n",
        "# Set again the model into evaluation mode\n",
        "model.eval()\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "input_ids_list = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # The model must not compute or store gradients\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate predictions.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "    # Transfer logits and labels to CPU\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    input_ids_list.extend(b_input_ids)\n",
        "    \n",
        "    # Calculate the accuracy for this batch of test sentences\n",
        "    eval_loss += outputs[0].mean().item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "test_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "#print(str(pred_tags))\n",
        "#print(str(test_tags))\n",
        "f1 = f1_score(pred_tags, test_tags, average='micro')\n",
        "\n",
        "# Format output with 4 decimal places\n",
        "output_text = \"Test F1 score: {:.4f}\\n\".format(f1)\n",
        "\n",
        "# Print to console\n",
        "print(output_text)\n",
        "\n",
        "# Save to a text file\n",
        "with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
        "    file.write(output_text)\n",
        "#print(\"Test classification report: {}\".format(classification_report(pred_tags, test_tags,digits=4)))\n",
        "print()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test F1 score: 0.7565089842317565\n\n"
        }
      ],
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1740755280004
        }
      },
      "id": "0d64d31a-e5b6-40ae-bbfb-0bf300a7ffc5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "#print(\"Optimizer's state_dict:\")\n",
        "#for var_name in optimizer.state_dict():\n",
        "#    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model's state_dict:\nbert.embeddings.position_ids \t torch.Size([1, 512])\nbert.embeddings.word_embeddings.weight \t torch.Size([30522, 768])\nbert.embeddings.position_embeddings.weight \t torch.Size([512, 768])\nbert.embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\nbert.embeddings.LayerNorm.weight \t torch.Size([768])\nbert.embeddings.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.0.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.0.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.0.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.0.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.1.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.1.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.1.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.1.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.2.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.2.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.2.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.2.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.3.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.3.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.3.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.3.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.4.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.4.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.4.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.4.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.5.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.5.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.5.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.5.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.6.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.6.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.6.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.6.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.7.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.7.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.7.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.7.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.8.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.8.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.8.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.8.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.9.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.9.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.9.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.9.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.10.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.10.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.10.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.10.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\nbert.encoder.layer.11.attention.self.query.bias \t torch.Size([768])\nbert.encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\nbert.encoder.layer.11.attention.self.key.bias \t torch.Size([768])\nbert.encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\nbert.encoder.layer.11.attention.self.value.bias \t torch.Size([768])\nbert.encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\nbert.encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\nbert.encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\nbert.encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\nbert.encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\nbert.encoder.layer.11.output.dense.bias \t torch.Size([768])\nbert.encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\nbert.encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\nclassifier.weight \t torch.Size([6, 768])\nclassifier.bias \t torch.Size([6])\n"
        }
      ],
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1740757285071
        }
      },
      "id": "6cdb58de-aa8e-4bca-b481-c8708e43a28c"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(zip(pred_tags, test_tags)),\n",
        "               columns =['Pred', 'True'])"
      ],
      "outputs": [],
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1740757465671
        }
      },
      "id": "fb3fd637-aa6a-4435-9030-20f6060e1629"
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(folder_name + '/test-result-bert.csv')"
      ],
      "outputs": [],
      "execution_count": 55,
      "metadata": {
        "gather": {
          "logged": 1740757484695
        }
      },
      "id": "a09dd8d6-39bf-47a5-a09b-d1cf897f9cee"
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"bert-base-local\")\n",
        "tokenizer.save_pretrained(\"bert-base-local\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "10aa1d62-6169-430a-9d43-c38b970dec49"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_test_class = []\n",
        "labels_test_class = []\n",
        "\n",
        "tokens = []\n",
        "token_labels = []\n",
        "unique_labels_test_class = set()\n",
        "\n",
        "with open(\"corpus-raymond/validation-full-class-only.txt\", newline='', encoding='utf-8') as file:\n",
        "    line_reader = csv.reader(file, delimiter='\\t')\n",
        "\n",
        "    for line in line_reader:\n",
        "        # Remove empty spaces and ensure valid parsing\n",
        "        line = [x.strip() for x in line if x.strip()]  # Strip whitespace & ignore empty columns\n",
        "        \n",
        "        if not line:  # If it's an empty line, treat it as a sentence separator\n",
        "            if tokens:  # Avoid adding empty lists\n",
        "                sentences_test_class.append(tokens)\n",
        "                labels_test_class.append(token_labels)\n",
        "                tokens, token_labels = [], []  # Reset for next sentence\n",
        "        else:\n",
        "            if len(line) == 1 and line[0] == '\\\"':  # Handle single double-quote case\n",
        "                tokens.append('\\\"')\n",
        "                token_labels.append('O')  # Assuming label should be 'O' if unknown\n",
        "            elif len(line) >= 2:  # Normal case (word, label)\n",
        "                tokens.append(line[0])\n",
        "                token_labels.append(line[1])\n",
        "                unique_labels_test_class.add(line[1])\n",
        "\n",
        "# Ensure last collected sentence is added\n",
        "if tokens:\n",
        "    sentences_test_class.append(tokens)\n",
        "    labels_test_class.append(token_labels)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "3cbc659f-681e-450a-a235-0b544becfb0c"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the model to the test with only structure_focus set\n",
        "# Set again the model into evaluation mode\n",
        "\n",
        "tokenized_texts_and_labels_test_class = [\n",
        "    tokenize_and_keep_labels(sent, labs)\n",
        "    for sent, labs in zip(sentences_test_class, labels_test_class)\n",
        "]\n",
        "\n",
        "tokenized_texts_test = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels_test_class]\n",
        "labels_test = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels_test_class]\n",
        "\n",
        "input_ids_test_class = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_test],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags_test_class = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels_test],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "test_class_inputs = torch.tensor(input_ids_test_class)\n",
        "test_class_tags = torch.tensor(tags_test_class)\n",
        "\n",
        "attention_masks_test_class = [[float(i != 0.0) for i in ii] for ii in input_ids_test_class]\n",
        "test_class_masks = torch.tensor(attention_masks_test_class)\n",
        "\n",
        "test_class_data = TensorDataset(test_class_inputs, test_class_masks, test_class_tags)\n",
        "test_class_sampler = SequentialSampler(test_class_data)\n",
        "test_class_dataloader = DataLoader(test_class_data, sampler=test_class_sampler, batch_size=bs)\n",
        "\n",
        "model.eval()\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "input_ids_list = []\n",
        "\n",
        "for batch in test_class_dataloader:\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # The model must not compute or store gradients\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate predictions.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask, labels=b_labels)\n",
        "    # Transfer logits and labels to CPU\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    input_ids_list.extend(b_input_ids)\n",
        "    \n",
        "    # Calculate the accuracy for this batch of test sentences\n",
        "    eval_loss += outputs[0].mean().item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "test_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "#print(str(pred_tags))\n",
        "#print(str(test_tags))\n",
        "# Compute F1 score\n",
        "f1 = f1_score(pred_tags, test_tags, average='micro')\n",
        "\n",
        "# Format output with 4 decimal places\n",
        "output_text = \"Test with structure only F1 score: {:.4f}\\n\".format(f1)\n",
        "\n",
        "# Print to console\n",
        "print(output_text)\n",
        "\n",
        "# Save to a text file\n",
        "with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
        "    file.write(output_text)\n",
        "#print(\"Test classification report: {}\".format(classification_report(pred_tags, test_tags,digits=4)))\n",
        "\n",
        "\n",
        "print()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "2e573465-d20b-40b6-921f-56159a3209cc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "custom_37",
      "language": "python",
      "display_name": "Python_37"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "custom_37"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}