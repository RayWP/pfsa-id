{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926ee245-18e1-47ce-ba1f-320a271aebb4",
   "metadata": {},
   "source": [
    "## Use Case Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a7ac7e-575a-4068-8531-d3472e68c3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME = \"albert/albert-base-v2\"\n",
    "LABEL_COLUMN = \"usecase_focus\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_file = f\"{MODEL_NAME.replace('/', '-')}_{LABEL_COLUMN}_{timestamp}_bce_loss.txt\"\n",
    "\n",
    "# Define hyperparameter grid\n",
    "EPOCHS_LIST = [3, 4, 9, 12]\n",
    "BATCH_SIZES = [8]\n",
    "N_SPLITS_LIST = [5, 10]\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('type_classification-validation.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "data[LABEL_COLUMN] = label_encoder.fit_transform(data[LABEL_COLUMN])\n",
    "texts = data['sentence'].tolist()\n",
    "labels = data[LABEL_COLUMN].tolist()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def save_best_model(model, model_name, label_column, epochs, k_folds, batch_size):\n",
    "    save_path = os.path.join(model_save_dir, f\"{model_name}_{label_column}_epochs{epochs}_kfold{k_folds}_batch{batch_size}.bin\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Best model saved at {save_path}\")\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs, fold):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            preds = model(**inputs).logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Logging helper\n",
    "def log_result(log_path, text):\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "# Start hyperparameter tuning\n",
    "best_f1 = 0\n",
    "best_combo = \"\"\n",
    "best_result = \"\"\n",
    "\n",
    "for epochs, batch_size, n_splits in itertools.product(EPOCHS_LIST, BATCH_SIZES, N_SPLITS_LIST):\n",
    "    print(f\"\\nTuning Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_accuracies, all_precisions, all_recalls, all_f1s = [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        train_texts = [texts[i] for i in train_idx]\n",
    "        val_texts = [texts[i] for i in val_idx]\n",
    "        train_labels = [labels[i] for i in train_idx]\n",
    "        val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "        train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(set(labels)), ignore_mismatched_sizes=True)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        # criterion = torch.nn.BCELoss()\n",
    "\n",
    "        train_model(model, train_loader, optimizer, criterion, epochs, fold)\n",
    "        y_true, y_pred = evaluate_model(model, val_loader)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        all_accuracies.append(acc)\n",
    "        all_precisions.append(prec)\n",
    "        all_recalls.append(rec)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_f1 = np.mean(all_f1s)\n",
    "\n",
    "    combo_string = f\"Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\"\n",
    "    result_string = f\"{combo_string}\\naccuracy: {avg_accuracy:.4f}, precision: {avg_precision:.4f}, recall: {avg_recall:.4f}, f1-score: {avg_f1:.4f}\\n\"\n",
    "    print(result_string)\n",
    "    log_result(log_file, result_string)\n",
    "\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        best_combo = combo_string\n",
    "        best_result = result_string\n",
    "        best_model = model\n",
    "        best_epoch = epochs\n",
    "        best_batch = batch_size\n",
    "        best_fold = n_splits\n",
    "\n",
    "# Log best combination at the end\n",
    "footer = f\"\\nBest Combination:\\n{best_result}\"\n",
    "log_result(log_file, footer)\n",
    "print(footer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1dbdc-6a31-4dd5-bca7-7cbad894f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save the best models\n",
    "model_save_dir = \"all_model\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "if best_model is not None:\n",
    "    save_best_model(best_model, MODEL_NAME.replace('/', '-'), LABEL_COLUMN, best_epoch, best_fold, best_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db360b-1ec5-4c62-9046-2cde3f6da62c",
   "metadata": {},
   "source": [
    "## Structure Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a7feb-75f8-427d-a779-46d2bc18f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME = \"albert/albert-base-v2\"\n",
    "LABEL_COLUMN = \"structure_focus\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_file = f\"{MODEL_NAME.replace('/', '-')}_{LABEL_COLUMN}_{timestamp}_bce_loss.txt\"\n",
    "\n",
    "# Define hyperparameter grid\n",
    "EPOCHS_LIST = [3, 4, 9, 12]\n",
    "BATCH_SIZES = [8]\n",
    "N_SPLITS_LIST = [5, 10]\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('type_classification-validation.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "data[LABEL_COLUMN] = label_encoder.fit_transform(data[LABEL_COLUMN])\n",
    "texts = data['sentence'].tolist()\n",
    "labels = data[LABEL_COLUMN].tolist()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def save_best_model(model, model_name, label_column, epochs, k_folds, batch_size):\n",
    "    save_path = os.path.join(model_save_dir, f\"{model_name}_{label_column}_epochs{epochs}_kfold{k_folds}_batch{batch_size}.bin\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Best model saved at {save_path}\")\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs, fold):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            preds = model(**inputs).logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Logging helper\n",
    "def log_result(log_path, text):\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "# Start hyperparameter tuning\n",
    "best_f1 = 0\n",
    "best_combo = \"\"\n",
    "best_result = \"\"\n",
    "\n",
    "for epochs, batch_size, n_splits in itertools.product(EPOCHS_LIST, BATCH_SIZES, N_SPLITS_LIST):\n",
    "    print(f\"\\nTuning Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_accuracies, all_precisions, all_recalls, all_f1s = [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        train_texts = [texts[i] for i in train_idx]\n",
    "        val_texts = [texts[i] for i in val_idx]\n",
    "        train_labels = [labels[i] for i in train_idx]\n",
    "        val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "        train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(set(labels)), ignore_mismatched_sizes=True)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        # criterion = torch.nn.BCELoss()\n",
    "\n",
    "        train_model(model, train_loader, optimizer, criterion, epochs, fold)\n",
    "        y_true, y_pred = evaluate_model(model, val_loader)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        all_accuracies.append(acc)\n",
    "        all_precisions.append(prec)\n",
    "        all_recalls.append(rec)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_f1 = np.mean(all_f1s)\n",
    "\n",
    "    combo_string = f\"Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\"\n",
    "    result_string = f\"{combo_string}\\naccuracy: {avg_accuracy:.4f}, precision: {avg_precision:.4f}, recall: {avg_recall:.4f}, f1-score: {avg_f1:.4f}\\n\"\n",
    "    print(result_string)\n",
    "    log_result(log_file, result_string)\n",
    "\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        best_combo = combo_string\n",
    "        best_result = result_string\n",
    "        best_model = model\n",
    "        best_epoch = epochs\n",
    "        best_batch = batch_size\n",
    "        best_fold = n_splits\n",
    "\n",
    "# Log best combination at the end\n",
    "footer = f\"\\nBest Combination:\\n{best_result}\"\n",
    "log_result(log_file, footer)\n",
    "print(footer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab0eb5-9439-408f-877f-e4d655515c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save the best models\n",
    "model_save_dir = \"all_model\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "if best_model is not None:\n",
    "    save_best_model(best_model, MODEL_NAME.replace('/', '-'), LABEL_COLUMN, best_epoch, best_fold, best_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c68a08-4799-4d6a-b1eb-140009170e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf528d9-085a-4436-bd05-9e3b0709cc89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
