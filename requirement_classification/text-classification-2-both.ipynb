{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926ee245-18e1-47ce-ba1f-320a271aebb4",
   "metadata": {},
   "source": [
    "## Use Case Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a7ac7e-575a-4068-8531-d3472e68c3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Combination: EPOCHS=3, BATCH_SIZE=8, K-FOLD=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 15/15 [00:02<00:00,  6.69it/s, loss=0.619]\n",
      "Fold 1 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.34it/s, loss=0.513]\n",
      "Fold 1 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.33it/s, loss=0.477]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.38it/s, loss=0.596]\n",
      "Fold 2 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.39it/s, loss=0.499]\n",
      "Fold 2 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.39it/s, loss=0.455]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.602]\n",
      "Fold 3 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.33it/s, loss=0.486]\n",
      "Fold 3 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.34it/s, loss=0.495]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.69] \n",
      "Fold 4 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.519]\n",
      "Fold 4 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.34it/s, loss=0.455]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.582]\n",
      "Fold 5 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.33it/s, loss=0.441]\n",
      "Fold 5 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.425]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=3, BATCH_SIZE=8, K-FOLD=5\n",
      "accuracy: 0.8138, precision: 0.4069, recall: 0.5000, f1-score: 0.4486\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=3, BATCH_SIZE=8, K-FOLD=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.25it/s, loss=0.527]\n",
      "Fold 1 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.462]\n",
      "Fold 1 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s, loss=0.426]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.616]\n",
      "Fold 2 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.457]\n",
      "Fold 2 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.433]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.565]\n",
      "Fold 3 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.37it/s, loss=0.526]\n",
      "Fold 3 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.37it/s, loss=0.481]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.26it/s, loss=0.569]\n",
      "Fold 4 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.499]\n",
      "Fold 4 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.409]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.584]\n",
      "Fold 5 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.459]\n",
      "Fold 5 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.434]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 6 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.25it/s, loss=0.58] \n",
      "Fold 6 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.469]\n",
      "Fold 6 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.445]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 7 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.576]\n",
      "Fold 7 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.48] \n",
      "Fold 7 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.454]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 8 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.613]\n",
      "Fold 8 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.503]\n",
      "Fold 8 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.438]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 9 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.563]\n",
      "Fold 9 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.471]\n",
      "Fold 9 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.446]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 10 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.558]\n",
      "Fold 10 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.488]\n",
      "Fold 10 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.449]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=3, BATCH_SIZE=8, K-FOLD=10\n",
      "accuracy: 0.8143, precision: 0.4071, recall: 0.5000, f1-score: 0.4487\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=4, BATCH_SIZE=8, K-FOLD=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.569]\n",
      "Fold 1 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.518]\n",
      "Fold 1 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.452]\n",
      "Fold 1 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.407]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.609]\n",
      "Fold 2 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.38it/s, loss=0.462]\n",
      "Fold 2 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.37it/s, loss=0.481]\n",
      "Fold 2 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.444]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.528]\n",
      "Fold 3 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.464]\n",
      "Fold 3 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.446]\n",
      "Fold 3 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.33it/s, loss=0.34] \n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.569]\n",
      "Fold 4 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.45] \n",
      "Fold 4 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.437]\n",
      "Fold 4 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.324]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.622]\n",
      "Fold 5 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.488]\n",
      "Fold 5 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.438]\n",
      "Fold 5 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.389]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=4, BATCH_SIZE=8, K-FOLD=5\n",
      "accuracy: 0.8138, precision: 0.4069, recall: 0.5000, f1-score: 0.4486\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=4, BATCH_SIZE=8, K-FOLD=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.612]\n",
      "Fold 1 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.485]\n",
      "Fold 1 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.479]\n",
      "Fold 1 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.355]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.574]\n",
      "Fold 2 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.454]\n",
      "Fold 2 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.428]\n",
      "Fold 2 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.374]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.34it/s, loss=0.56] \n",
      "Fold 3 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.459]\n",
      "Fold 3 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.37it/s, loss=0.453]\n",
      "Fold 3 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.438]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.26it/s, loss=0.592]\n",
      "Fold 4 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.547]\n",
      "Fold 4 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.449]\n",
      "Fold 4 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.432]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.548]\n",
      "Fold 5 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.475]\n",
      "Fold 5 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.457]\n",
      "Fold 5 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.365]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 6 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.537]\n",
      "Fold 6 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.477]\n",
      "Fold 6 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.451]\n",
      "Fold 6 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.374]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 7 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.629]\n",
      "Fold 7 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.517]\n",
      "Fold 7 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.469]\n",
      "Fold 7 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.427]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 8 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.564]\n",
      "Fold 8 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.477]\n",
      "Fold 8 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.481]\n",
      "Fold 8 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.313]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 9 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.577]\n",
      "Fold 9 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.465]\n",
      "Fold 9 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.432]\n",
      "Fold 9 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.265]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 10 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.559]\n",
      "Fold 10 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.484]\n",
      "Fold 10 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.435]\n",
      "Fold 10 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.31] \n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=4, BATCH_SIZE=8, K-FOLD=10\n",
      "accuracy: 0.8143, precision: 0.4071, recall: 0.5000, f1-score: 0.4487\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=9, BATCH_SIZE=8, K-FOLD=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.27it/s, loss=0.574]\n",
      "Fold 1 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.491]\n",
      "Fold 1 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.442]\n",
      "Fold 1 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.359]\n",
      "Fold 1 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.197]\n",
      "Fold 1 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.107] \n",
      "Fold 1 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.529]\n",
      "Fold 1 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.125]\n",
      "Fold 1 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.0209]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.33it/s, loss=0.611]\n",
      "Fold 2 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.481]\n",
      "Fold 2 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.447]\n",
      "Fold 2 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.369]\n",
      "Fold 2 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.271]\n",
      "Fold 2 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.182]\n",
      "Fold 2 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.16] \n",
      "Fold 2 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.108]\n",
      "Fold 2 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.0836]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.28it/s, loss=0.599]\n",
      "Fold 3 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.28it/s, loss=0.492]\n",
      "Fold 3 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.467]\n",
      "Fold 3 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.464]\n",
      "Fold 3 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.366]\n",
      "Fold 3 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.222]\n",
      "Fold 3 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.169]\n",
      "Fold 3 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.124]\n",
      "Fold 3 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.28it/s, loss=0.0299]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.589]\n",
      "Fold 4 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.49] \n",
      "Fold 4 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.438]\n",
      "Fold 4 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.405]\n",
      "Fold 4 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.277]\n",
      "Fold 4 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.163]\n",
      "Fold 4 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0836]\n",
      "Fold 4 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.28it/s, loss=0.0243]\n",
      "Fold 4 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0122]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.27it/s, loss=0.574]\n",
      "Fold 5 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.516]\n",
      "Fold 5 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.444]\n",
      "Fold 5 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.45] \n",
      "Fold 5 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.331]\n",
      "Fold 5 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.29it/s, loss=0.301]\n",
      "Fold 5 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.177]\n",
      "Fold 5 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.129]\n",
      "Fold 5 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.0968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=9, BATCH_SIZE=8, K-FOLD=5\n",
      "accuracy: 0.8276, precision: 0.7537, recall: 0.7054, f1-score: 0.7114\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=9, BATCH_SIZE=8, K-FOLD=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.624]\n",
      "Fold 1 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.464]\n",
      "Fold 1 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.432]\n",
      "Fold 1 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.35] \n",
      "Fold 1 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.199]\n",
      "Fold 1 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0922]\n",
      "Fold 1 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.033] \n",
      "Fold 1 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.0112]\n",
      "Fold 1 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00528]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.576]\n",
      "Fold 2 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.495]\n",
      "Fold 2 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.453]\n",
      "Fold 2 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.399]\n",
      "Fold 2 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.239]\n",
      "Fold 2 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.124] \n",
      "Fold 2 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0265]\n",
      "Fold 2 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.0553]\n",
      "Fold 2 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00964]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.33it/s, loss=0.615]\n",
      "Fold 3 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.509]\n",
      "Fold 3 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.37it/s, loss=0.436]\n",
      "Fold 3 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.37it/s, loss=0.332]\n",
      "Fold 3 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.193]\n",
      "Fold 3 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.129]\n",
      "Fold 3 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.0474]\n",
      "Fold 3 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.0121]\n",
      "Fold 3 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.36it/s, loss=0.00527]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.64] \n",
      "Fold 4 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.561]\n",
      "Fold 4 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.45] \n",
      "Fold 4 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.428]\n",
      "Fold 4 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.351]\n",
      "Fold 4 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.188]\n",
      "Fold 4 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0847]\n",
      "Fold 4 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0283]\n",
      "Fold 4 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.00737]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.618]\n",
      "Fold 5 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.474]\n",
      "Fold 5 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.435]\n",
      "Fold 5 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.42] \n",
      "Fold 5 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.224]\n",
      "Fold 5 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.116]\n",
      "Fold 5 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0315]\n",
      "Fold 5 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0109]\n",
      "Fold 5 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0426] \n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 6 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.595]\n",
      "Fold 6 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.494]\n",
      "Fold 6 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.451]\n",
      "Fold 6 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.382]\n",
      "Fold 6 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.231]\n",
      "Fold 6 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0924]\n",
      "Fold 6 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0367]\n",
      "Fold 6 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0155] \n",
      "Fold 6 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.0133]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 7 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.552]\n",
      "Fold 7 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.49] \n",
      "Fold 7 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.472]\n",
      "Fold 7 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.383]\n",
      "Fold 7 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.235]\n",
      "Fold 7 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.213]\n",
      "Fold 7 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.176]\n",
      "Fold 7 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0395]\n",
      "Fold 7 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.019] \n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 8 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.581]\n",
      "Fold 8 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.474]\n",
      "Fold 8 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.448]\n",
      "Fold 8 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.331]\n",
      "Fold 8 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.259]\n",
      "Fold 8 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.141]\n",
      "Fold 8 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0773]\n",
      "Fold 8 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0248]\n",
      "Fold 8 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.00954]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 9 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.589]\n",
      "Fold 9 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.48] \n",
      "Fold 9 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.429]\n",
      "Fold 9 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.364]\n",
      "Fold 9 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.277]\n",
      "Fold 9 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.145]\n",
      "Fold 9 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0404]\n",
      "Fold 9 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0104]\n",
      "Fold 9 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0192] \n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 10 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.555]\n",
      "Fold 10 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.463]\n",
      "Fold 10 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.467]\n",
      "Fold 10 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.35] \n",
      "Fold 10 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.217]\n",
      "Fold 10 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.203]\n",
      "Fold 10 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.105]\n",
      "Fold 10 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0298]\n",
      "Fold 10 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=9, BATCH_SIZE=8, K-FOLD=10\n",
      "accuracy: 0.8067, precision: 0.6675, recall: 0.6042, f1-score: 0.6178\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=12, BATCH_SIZE=8, K-FOLD=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.26it/s, loss=0.584]\n",
      "Fold 1 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.478]\n",
      "Fold 1 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.442]\n",
      "Fold 1 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.325]\n",
      "Fold 1 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.227]\n",
      "Fold 1 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.149]\n",
      "Fold 1 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.0812]\n",
      "Fold 1 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0195]\n",
      "Fold 1 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.00548]\n",
      "Fold 1 Epoch 10: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.00352]\n",
      "Fold 1 Epoch 11: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.00248]\n",
      "Fold 1 Epoch 12: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.00173]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.33it/s, loss=0.676]\n",
      "Fold 2 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.493]\n",
      "Fold 2 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.479]\n",
      "Fold 2 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.479]\n",
      "Fold 2 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.37it/s, loss=0.369]\n",
      "Fold 2 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.297]\n",
      "Fold 2 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.136]\n",
      "Fold 2 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.37it/s, loss=0.0714]\n",
      "Fold 2 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.0182]\n",
      "Fold 2 Epoch 10: 100%|██████████| 15/15 [00:01<00:00,  9.36it/s, loss=0.00654]\n",
      "Fold 2 Epoch 11: 100%|██████████| 15/15 [00:01<00:00,  9.35it/s, loss=0.00485]\n",
      "Fold 2 Epoch 12: 100%|██████████| 15/15 [00:01<00:00,  9.38it/s, loss=0.00367]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.27it/s, loss=0.626]\n",
      "Fold 3 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.509]\n",
      "Fold 3 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.457]\n",
      "Fold 3 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.455]\n",
      "Fold 3 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.333]\n",
      "Fold 3 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.284]\n",
      "Fold 3 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.223]\n",
      "Fold 3 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0928]\n",
      "Fold 3 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0323]\n",
      "Fold 3 Epoch 10: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.00971]\n",
      "Fold 3 Epoch 11: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.00529]\n",
      "Fold 3 Epoch 12: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.00279]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.28it/s, loss=0.593]\n",
      "Fold 4 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.475]\n",
      "Fold 4 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.449]\n",
      "Fold 4 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.414]\n",
      "Fold 4 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.269]\n",
      "Fold 4 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.195]\n",
      "Fold 4 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.103] \n",
      "Fold 4 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0325]\n",
      "Fold 4 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.0234]\n",
      "Fold 4 Epoch 10: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.0698]\n",
      "Fold 4 Epoch 11: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0445]\n",
      "Fold 4 Epoch 12: 100%|██████████| 15/15 [00:01<00:00,  9.28it/s, loss=0.0222]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 15/15 [00:01<00:00,  9.28it/s, loss=0.554]\n",
      "Fold 5 Epoch 2: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.456]\n",
      "Fold 5 Epoch 3: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.454]\n",
      "Fold 5 Epoch 4: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.408]\n",
      "Fold 5 Epoch 5: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.261]\n",
      "Fold 5 Epoch 6: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.144]\n",
      "Fold 5 Epoch 7: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.0307]\n",
      "Fold 5 Epoch 8: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.00742]\n",
      "Fold 5 Epoch 9: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.0042] \n",
      "Fold 5 Epoch 10: 100%|██████████| 15/15 [00:01<00:00,  9.31it/s, loss=0.00291]\n",
      "Fold 5 Epoch 11: 100%|██████████| 15/15 [00:01<00:00,  9.30it/s, loss=0.0022] \n",
      "Fold 5 Epoch 12: 100%|██████████| 15/15 [00:01<00:00,  9.32it/s, loss=0.00186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=12, BATCH_SIZE=8, K-FOLD=5\n",
      "accuracy: 0.8276, precision: 0.7345, recall: 0.6493, f1-score: 0.6649\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=12, BATCH_SIZE=8, K-FOLD=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.26it/s, loss=0.581]\n",
      "Fold 1 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.455]\n",
      "Fold 1 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.441]\n",
      "Fold 1 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.364]\n",
      "Fold 1 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.206]\n",
      "Fold 1 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.172]\n",
      "Fold 1 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.173]\n",
      "Fold 1 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0787]\n",
      "Fold 1 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0296]\n",
      "Fold 1 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.00939]\n",
      "Fold 1 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00476]\n",
      "Fold 1 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00382]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.536]\n",
      "Fold 2 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.51] \n",
      "Fold 2 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.453]\n",
      "Fold 2 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.418]\n",
      "Fold 2 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.296]\n",
      "Fold 2 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.177]\n",
      "Fold 2 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.111]\n",
      "Fold 2 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0818]\n",
      "Fold 2 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0289]\n",
      "Fold 2 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00869]\n",
      "Fold 2 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0037] \n",
      "Fold 2 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00261]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.32it/s, loss=0.571]\n",
      "Fold 3 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.469]\n",
      "Fold 3 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.462]\n",
      "Fold 3 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.34it/s, loss=0.428]\n",
      "Fold 3 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.368]\n",
      "Fold 3 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.194]\n",
      "Fold 3 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.162]\n",
      "Fold 3 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.0251]\n",
      "Fold 3 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.00939]\n",
      "Fold 3 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.00576]\n",
      "Fold 3 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.34it/s, loss=0.00375]\n",
      "Fold 3 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.35it/s, loss=0.00304]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.619]\n",
      "Fold 4 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.498]\n",
      "Fold 4 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.44] \n",
      "Fold 4 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.371]\n",
      "Fold 4 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.217]\n",
      "Fold 4 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0855]\n",
      "Fold 4 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0139]\n",
      "Fold 4 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00518]\n",
      "Fold 4 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00363]\n",
      "Fold 4 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00266]\n",
      "Fold 4 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00202]\n",
      "Fold 4 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00176]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.26it/s, loss=0.54] \n",
      "Fold 5 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.46] \n",
      "Fold 5 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.437]\n",
      "Fold 5 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.367]\n",
      "Fold 5 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.255]\n",
      "Fold 5 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.139]\n",
      "Fold 5 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0862]\n",
      "Fold 5 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0671]\n",
      "Fold 5 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.0406]\n",
      "Fold 5 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0217]\n",
      "Fold 5 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.0134]\n",
      "Fold 5 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00802]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 6 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.24it/s, loss=0.542]\n",
      "Fold 6 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.491]\n",
      "Fold 6 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.444]\n",
      "Fold 6 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.375]\n",
      "Fold 6 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.28] \n",
      "Fold 6 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.12] \n",
      "Fold 6 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.119]\n",
      "Fold 6 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.0372]\n",
      "Fold 6 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0105]\n",
      "Fold 6 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00701]\n",
      "Fold 6 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00466]\n",
      "Fold 6 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00344]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 7 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.25it/s, loss=0.629]\n",
      "Fold 7 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.491]\n",
      "Fold 7 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.441]\n",
      "Fold 7 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.293]\n",
      "Fold 7 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.155]\n",
      "Fold 7 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.0729]\n",
      "Fold 7 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0678]\n",
      "Fold 7 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.0515]\n",
      "Fold 7 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0254]\n",
      "Fold 7 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00763]\n",
      "Fold 7 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.00395]\n",
      "Fold 7 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00271]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 8 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.586]\n",
      "Fold 8 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.473]\n",
      "Fold 8 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.448]\n",
      "Fold 8 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.391]\n",
      "Fold 8 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.208]\n",
      "Fold 8 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.0806]\n",
      "Fold 8 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0184]\n",
      "Fold 8 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.00551]\n",
      "Fold 8 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00368]\n",
      "Fold 8 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00239]\n",
      "Fold 8 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00203]\n",
      "Fold 8 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.00164]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 9 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.604]\n",
      "Fold 9 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.452]\n",
      "Fold 9 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.463]\n",
      "Fold 9 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.413]\n",
      "Fold 9 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.271]\n",
      "Fold 9 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.179]\n",
      "Fold 9 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.296]\n",
      "Fold 9 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.132]\n",
      "Fold 9 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0841]\n",
      "Fold 9 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0261]\n",
      "Fold 9 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00658]\n",
      "Fold 9 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0035] \n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 10 Epoch 1: 100%|██████████| 17/17 [00:01<00:00,  9.19it/s, loss=0.594]\n",
      "Fold 10 Epoch 2: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.465]\n",
      "Fold 10 Epoch 3: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.472]\n",
      "Fold 10 Epoch 4: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.346]\n",
      "Fold 10 Epoch 5: 100%|██████████| 17/17 [00:01<00:00,  9.31it/s, loss=0.273]\n",
      "Fold 10 Epoch 6: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.176]\n",
      "Fold 10 Epoch 7: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.136]\n",
      "Fold 10 Epoch 8: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.0398]\n",
      "Fold 10 Epoch 9: 100%|██████████| 17/17 [00:01<00:00,  9.28it/s, loss=0.0167]\n",
      "Fold 10 Epoch 10: 100%|██████████| 17/17 [00:01<00:00,  9.29it/s, loss=0.00501]\n",
      "Fold 10 Epoch 11: 100%|██████████| 17/17 [00:01<00:00,  9.30it/s, loss=0.00301]\n",
      "Fold 10 Epoch 12: 100%|██████████| 17/17 [00:01<00:00,  9.27it/s, loss=0.00232]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=12, BATCH_SIZE=8, K-FOLD=10\n",
      "accuracy: 0.8205, precision: 0.7587, recall: 0.6542, f1-score: 0.6676\n",
      "\n",
      "\n",
      "Best Combination:\n",
      "Combination: EPOCHS=9, BATCH_SIZE=8, K-FOLD=5\n",
      "accuracy: 0.8276, precision: 0.7537, recall: 0.7054, f1-score: 0.7114\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "LABEL_COLUMN = \"usecase_focus\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_file = f\"{MODEL_NAME.replace('/', '-')}_{LABEL_COLUMN}_{timestamp}.txt\"\n",
    "\n",
    "# Define hyperparameter grid\n",
    "EPOCHS_LIST = [3, 4, 9, 12]\n",
    "BATCH_SIZES = [8]\n",
    "N_SPLITS_LIST = [5, 10]\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('type_classification-validation.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "data[LABEL_COLUMN] = label_encoder.fit_transform(data[LABEL_COLUMN])\n",
    "texts = data['sentence'].tolist()\n",
    "labels = data[LABEL_COLUMN].tolist()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def save_best_model(model, model_name, label_column, epochs, k_folds, batch_size):\n",
    "    save_path = os.path.join(model_save_dir, f\"{model_name}_{label_column}_epochs{epochs}_kfold{k_folds}_batch{batch_size}.bin\")\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Best model saved at {save_path}\")\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs, fold):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move entire batch to device\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "            # Forward pass with labels included\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Automatic loss from model\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            preds = model(**inputs).logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Logging helper\n",
    "def log_result(log_path, text):\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "# Start hyperparameter tuning\n",
    "best_f1 = 0\n",
    "best_combo = \"\"\n",
    "best_result = \"\"\n",
    "\n",
    "for epochs, batch_size, n_splits in itertools.product(EPOCHS_LIST, BATCH_SIZES, N_SPLITS_LIST):\n",
    "    print(f\"\\nTuning Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_accuracies, all_precisions, all_recalls, all_f1s = [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        train_texts = [texts[i] for i in train_idx]\n",
    "        val_texts = [texts[i] for i in val_idx]\n",
    "        train_labels = [labels[i] for i in train_idx]\n",
    "        val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "        train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(set(labels)), ignore_mismatched_sizes=True)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        train_model(model, train_loader, optimizer, criterion, epochs, fold)\n",
    "        y_true, y_pred = evaluate_model(model, val_loader)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        all_accuracies.append(acc)\n",
    "        all_precisions.append(prec)\n",
    "        all_recalls.append(rec)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_f1 = np.mean(all_f1s)\n",
    "\n",
    "    combo_string = f\"Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\"\n",
    "    result_string = f\"{combo_string}\\naccuracy: {avg_accuracy:.4f}, precision: {avg_precision:.4f}, recall: {avg_recall:.4f}, f1-score: {avg_f1:.4f}\\n\"\n",
    "    print(result_string)\n",
    "    log_result(log_file, result_string)\n",
    "\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        best_combo = combo_string\n",
    "        best_result = result_string\n",
    "        best_model = model\n",
    "        best_epoch = epochs\n",
    "        best_batch = batch_size\n",
    "        best_fold = n_splits\n",
    "\n",
    "# Log best combination at the end\n",
    "footer = f\"\\nBest Combination:\\n{best_result}\"\n",
    "log_result(log_file, footer)\n",
    "print(footer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c1dbdc-6a31-4dd5-bca7-7cbad894f9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at all_model/microsoft-deberta-v3-base_usecase_focus_epochs9_kfold5_batch8.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save the best models\n",
    "model_save_dir = \"all_model\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "if best_model is not None:\n",
    "    save_best_model(best_model, MODEL_NAME.replace('/', '-'), LABEL_COLUMN, best_epoch, best_fold, best_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db360b-1ec5-4c62-9046-2cde3f6da62c",
   "metadata": {},
   "source": [
    "## Structure Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2a7feb-75f8-427d-a779-46d2bc18f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning Combination: EPOCHS=3, BATCH_SIZE=8, K-FOLD=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1: 100%|██████████| 15/15 [00:04<00:00,  3.56it/s, loss=0.633]\n",
      "Fold 1 Epoch 2: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, loss=0.525]\n",
      "Fold 1 Epoch 3: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, loss=0.442]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 2 Epoch 1: 100%|██████████| 15/15 [00:04<00:00,  3.60it/s, loss=0.638]\n",
      "Fold 2 Epoch 2: 100%|██████████| 15/15 [00:04<00:00,  3.60it/s, loss=0.491]\n",
      "Fold 2 Epoch 3: 100%|██████████| 15/15 [00:04<00:00,  3.61it/s, loss=0.351]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 3 Epoch 1: 100%|██████████| 15/15 [00:04<00:00,  3.58it/s, loss=0.603]\n",
      "Fold 3 Epoch 2: 100%|██████████| 15/15 [00:04<00:00,  3.58it/s, loss=0.531]\n",
      "Fold 3 Epoch 3: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, loss=0.468]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 4 Epoch 1: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, loss=0.584]\n",
      "Fold 4 Epoch 2: 100%|██████████| 15/15 [00:04<00:00,  3.58it/s, loss=0.468]\n",
      "Fold 4 Epoch 3: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, loss=0.291]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 5 Epoch 1: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, loss=0.582]\n",
      "Fold 5 Epoch 2: 100%|██████████| 15/15 [00:04<00:00,  3.58it/s, loss=0.522]\n",
      "Fold 5 Epoch 3: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, loss=0.515]\n",
      "/usr/local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination: EPOCHS=3, BATCH_SIZE=8, K-FOLD=5\n",
      "accuracy: 0.7586, precision: 0.4903, recall: 0.5584, f1-score: 0.5183\n",
      "\n",
      "\n",
      "Tuning Combination: EPOCHS=3, BATCH_SIZE=8, K-FOLD=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Fold 1 Epoch 1:   6%|▌         | 1/17 [00:00<00:08,  2.00it/s, loss=0.716]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 502.00 MiB. GPU 1 has a total capacity of 10.90 GiB of which 386.25 MiB is free. Process 452606 has 10.52 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m    120\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m--> 122\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader)\n\u001b[1;32m    125\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_true, y_pred)\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, epochs, fold)\u001b[0m\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 502.00 MiB. GPU 1 has a total capacity of 10.90 GiB of which 386.25 MiB is free. Process 452606 has 10.52 GiB memory in use. Of the allocated memory 9.27 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "LABEL_COLUMN = \"structure_focus\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_file = f\"{MODEL_NAME.replace('/', '-')}_{LABEL_COLUMN}_{timestamp}.txt\"\n",
    "\n",
    "# Define hyperparameter grid\n",
    "EPOCHS_LIST = [3, 4, 9, 12]\n",
    "BATCH_SIZES = [8]\n",
    "N_SPLITS_LIST = [5, 10]\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('type_classification-validation.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "data[LABEL_COLUMN] = label_encoder.fit_transform(data[LABEL_COLUMN])\n",
    "texts = data['sentence'].tolist()\n",
    "labels = data[LABEL_COLUMN].tolist()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs, fold):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\")\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move entire batch to device\n",
    "            batch = {key: val.to(device) for key, val in batch.items()}\n",
    "\n",
    "            # Forward pass with labels included\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # Automatic loss from model\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            preds = model(**inputs).logits.argmax(dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Logging helper\n",
    "def log_result(log_path, text):\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "# Start hyperparameter tuning\n",
    "best_f1 = 0\n",
    "best_combo = \"\"\n",
    "best_result = \"\"\n",
    "\n",
    "for epochs, batch_size, n_splits in itertools.product(EPOCHS_LIST, BATCH_SIZES, N_SPLITS_LIST):\n",
    "    print(f\"\\nTuning Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    all_accuracies, all_precisions, all_recalls, all_f1s = [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(texts, labels)):\n",
    "        train_texts = [texts[i] for i in train_idx]\n",
    "        val_texts = [texts[i] for i in val_idx]\n",
    "        train_labels = [labels[i] for i in train_idx]\n",
    "        val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "        train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "        val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(set(labels)), ignore_mismatched_sizes=True)\n",
    "        model.to(device)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        train_model(model, train_loader, optimizer, criterion, epochs, fold)\n",
    "        y_true, y_pred = evaluate_model(model, val_loader)\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        all_accuracies.append(acc)\n",
    "        all_precisions.append(prec)\n",
    "        all_recalls.append(rec)\n",
    "        all_f1s.append(f1)\n",
    "\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_f1 = np.mean(all_f1s)\n",
    "\n",
    "    combo_string = f\"Combination: EPOCHS={epochs}, BATCH_SIZE={batch_size}, K-FOLD={n_splits}\"\n",
    "    result_string = f\"{combo_string}\\naccuracy: {avg_accuracy:.4f}, precision: {avg_precision:.4f}, recall: {avg_recall:.4f}, f1-score: {avg_f1:.4f}\\n\"\n",
    "    print(result_string)\n",
    "    log_result(log_file, result_string)\n",
    "\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        best_combo = combo_string\n",
    "        best_result = result_string\n",
    "        best_model = model\n",
    "        best_epoch = epochs\n",
    "        best_batch = batch_size\n",
    "        best_fold = n_splits\n",
    "\n",
    "# Log best combination at the end\n",
    "footer = f\"\\nBest Combination:\\n{best_result}\"\n",
    "log_result(log_file, footer)\n",
    "print(footer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab0eb5-9439-408f-877f-e4d655515c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory to save the best models\n",
    "model_save_dir = \"all_model\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "if best_model is not None:\n",
    "    save_best_model(best_model, MODEL_NAME.replace('/', '-'), LABEL_COLUMN, best_epoch, best_fold, best_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
