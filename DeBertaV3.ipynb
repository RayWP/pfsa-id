{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:49.855955Z",
     "start_time": "2025-03-07T14:45:49.695025Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_dataset = pd.read_csv('corpus-raymond/train-full-hf-style.csv')\n",
    "val_dataset = pd.read_csv('corpus-raymond/val-full-hf-style.csv')\n",
    "test_dataset = pd.read_csv('corpus-raymond/test-full-hf-style.csv')\n",
    "#take column 'tokens' as list\n",
    "train_dataset['tokens'] = train_dataset['tokens'].apply(eval)\n",
    "val_dataset['tokens'] = val_dataset['tokens'].apply(eval)\n",
    "test_dataset['tokens'] = test_dataset['tokens'].apply(eval)\n",
    "\n",
    "#take column 'IOB_tag' as list\n",
    "train_dataset['IOB_tag'] = train_dataset['IOB_tag'].apply(eval)\n",
    "val_dataset['IOB_tag'] = val_dataset['IOB_tag'].apply(eval)\n",
    "test_dataset['IOB_tag'] = test_dataset['IOB_tag'].apply(eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f73d5f4016b9e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:49.896364Z",
     "start_time": "2025-03-07T14:45:49.867960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552deebe74c4634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.040212Z",
     "start_time": "2025-03-07T14:45:50.036210Z"
    }
   },
   "outputs": [],
   "source": [
    "iob_mapping = {\n",
    "    \"O\": 0,\n",
    "    \"B-class\": 1,\n",
    "    \"I-class\": 2,\n",
    "    \"B-attr\": 3,\n",
    "    \"I-attr\": 4\n",
    "}\n",
    "\n",
    "label_names = [ 'O', 'B-class', 'I-class', 'B-attr', 'I-attr' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d341879131b0e94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.241226Z",
     "start_time": "2025-03-07T14:45:50.236225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-class', 2: 'I-class', 3: 'B-attr', 4: 'I-attr', 5: '[PAD]'}\n",
      "{'O': 0, 'B-class': 1, 'I-class': 2, 'B-attr': 3, 'I-attr': 4, '[PAD]': 5}\n"
     ]
    }
   ],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "id2label[5] = \"[PAD]\"\n",
    "label2id[\"[PAD]\"] = 5\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1809149bf9d95f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.456197Z",
     "start_time": "2025-03-07T14:45:50.450198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". HTTPS with browser web button account customer new sdram MB 128 ; chip ram Flash ; GB : requirement hardware follow the to adhere 4 Intel XScale PXA270 a Register \n",
      "O O     O    O       O   O      I-class B-class  O   O     O  O   O O    O   O     O O  O O           O        O      O   O  O      O O     O      O      O O        \n"
     ]
    }
   ],
   "source": [
    "words = train_dataset.iloc[0][\"tokens\"]\n",
    "labels = train_dataset.iloc[0][\"IOB_tag\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0ad48192d4de0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:45:50.891213Z",
     "start_time": "2025-03-07T14:45:50.623559Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DebertaV2TokenizerFast\n",
    "import os\n",
    "from pathlib import Path\n",
    "tokenizer_checkpoint = \"microsoft/deberta-v3-large\"\n",
    "model_checkpoint = \"microsoft/deberta-v3-large\" \n",
    "folder_name = \"BERT-Style-result/microsoft/deberta-v3-large-20-epoch-4-bs\"\n",
    "model_name_save = \"BERT-Style-model/microsoft/deberta-v3-large-20-epoch-4-bs\"\n",
    "Path(folder_name).mkdir(parents=True, exist_ok=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint, add_prefix_space=True)\n",
    "\n",
    "max_length = 256\n",
    "epochs = 20\n",
    "bs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239159fe7bb3e643",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:47:40.251290Z",
     "start_time": "2025-03-07T14:47:40.247348Z"
    }
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = 5 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(5)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2a750b286ecd95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:47:41.349838Z",
     "start_time": "2025-03-07T14:47:41.344368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 323, 36145, 275, 2672, 967, 1946, 914, 1099, 353, 37455, 12288, 9354, 11600, 2600, 6288, 15359, 7144, 2600, 8817, 877, 4145, 3305, 1111, 262, 264, 11747, 453, 7450, 1477, 43571, 48995, 558, 22058, 266, 7256, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "------------\n",
      "[0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "------------\n",
      "[5, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(train_dataset.iloc[0][\"tokens\"],truncation=True, is_split_into_words=True, padding='max_length', max_length=max_length)\n",
    "labels = train_dataset.iloc[0][\"IOB_tag\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(inputs)\n",
    "print(\"------------\")\n",
    "print(labels)\n",
    "print(\"------------\")\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5efd2fcfe3798a38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:51.421549Z",
     "start_time": "2025-03-07T14:29:51.417726Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(df):\n",
    "    # Convert Pandas DataFrame to dictionary format (column-based)\n",
    "    examples = df.to_dict(orient=\"list\")\n",
    "\n",
    "    # Tokenize the input tokens\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding='max_length', max_length=max_length\n",
    "    )\n",
    "\n",
    "    all_labels = examples[\"IOB_tag\"]\n",
    "    rearranged_labels = []\n",
    "\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        rearranged_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = rearranged_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65afae5f293a13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:51.915266Z",
     "start_time": "2025-03-07T14:29:51.761477Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenize_and_align_labels(train_dataset)\n",
    "tokenized_val = tokenize_and_align_labels(val_dataset)\n",
    "tokenized_test = tokenize_and_align_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "849d07139f52d337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:52.481705Z",
     "start_time": "2025-03-07T14:29:52.126651Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# convert tokenized train to arrow dataset class\n",
    "train_dataset = Dataset.from_dict(tokenized_train)\n",
    "val_dataset = Dataset.from_dict(tokenized_val)\n",
    "test_dataset = Dataset.from_dict(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161be3fb74d6927f",
   "metadata": {},
   "source": [
    "# Data Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60b4279359a555e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:52.683957Z",
     "start_time": "2025-03-07T14:29:52.678957Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea37710ff2039e",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14f04c19330eeac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:54.825368Z",
     "start_time": "2025-03-07T14:29:53.158363Z"
    }
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a92b04db98538dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:55.027556Z",
     "start_time": "2025-03-07T14:29:55.021810Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536937e8af00143",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e201335ef1af87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:55.671299Z",
     "start_time": "2025-03-07T14:29:55.436307Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=len(id2label),\n",
    ")\n",
    "# weight_decay is a regularization procedure with regard to the weight matrices\n",
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19216f4f9f0ef1c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:56.596964Z",
     "start_time": "2025-03-07T14:29:55.819246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForTokenClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = \"cuda\"\n",
    "print(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438f940b2194e8a",
   "metadata": {},
   "source": [
    "# Preparing Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48f110bd1254115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:57.596407Z",
     "start_time": "2025-03-07T14:29:56.798046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to torch tensors\n",
    "train_inputs = torch.tensor(train_dataset[\"input_ids\"])\n",
    "dev_inputs = torch.tensor(val_dataset[\"input_ids\"])\n",
    "test_inputs = torch.tensor(test_dataset[\"input_ids\"])\n",
    "train_tags = torch.tensor(train_dataset[\"labels\"])\n",
    "dev_tags = torch.tensor(val_dataset[\"labels\"])\n",
    "test_tags = torch.tensor(test_dataset[\"labels\"])\n",
    "train_masks = torch.tensor(train_dataset[\"attention_mask\"])\n",
    "dev_masks = torch.tensor(val_dataset[\"attention_mask\"])\n",
    "test_masks = torch.tensor(test_dataset[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d7e090fa3de219e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:57.824872Z",
     "start_time": "2025-03-07T14:29:57.820878Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "\n",
    "# We define the dataloaders. \n",
    "# Shuffle the data for training using RandomSampler\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "# Load dev and test data sequentially with SequentialSampler.\n",
    "dev_data = TensorDataset(dev_inputs, dev_masks, dev_tags)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=bs)\n",
    "\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ef97bc0b87326df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:29:58.004324Z",
     "start_time": "2025-03-07T14:29:58.000310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import a scheduler to reduce the learning rate \n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs; the BERT paper uses 10\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68aa2ac29568db",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96e2008ecec57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T14:30:09.818329800Z",
     "start_time": "2025-03-07T14:30:03.696195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.04759164712643319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 1/20 [03:12<1:00:50, 192.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss: 0.031744941478555506\n",
      "train-val F1 score: 0.8904\n",
      "\n",
      "\n",
      "Average train loss: 0.14032678508714272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 2/20 [06:33<59:17, 197.62s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss: 0.9731289806518149\n",
      "train-val F1 score: 0.0018\n",
      "\n",
      "\n",
      "Average train loss: 0.09548963326281004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 3/20 [09:55<56:32, 199.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development loss: 1.275800187029737\n",
      "train-val F1 score: 0.0016\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm import trange\n",
    "\n",
    "# To measure execution time of this cell\n",
    "\n",
    "# Train the model for; the BERT paper uses 4\n",
    "## Store the average loss after each epoch; these values are used to plot the loss.\n",
    "loss_values, development_loss_values = [], []\n",
    "\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    #\n",
    "    # Training\n",
    "    #\n",
    "    # Set the model into training mode\n",
    "    model.train()\n",
    "    # Reset the total loss for each epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Transfer batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Remove previous gradients before each backward pass\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This returns the loss (not the model output) since we have input the labels.\n",
    "        outputs = model(b_input_ids,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Get the loss\n",
    "        loss = outputs[0]\n",
    "        # Backward pass to compute the gradients\n",
    "        loss.backward()\n",
    "        # Train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "\n",
    "    # Store each loss value for plotting the learning curve afterwards\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    # After each training epoch, measure performance on development set\n",
    "\n",
    "    # Set the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the development loss for this epoch\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in dev_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # The model must not compute or save gradients, in order to save memory and speed up this step\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, compute predictions\n",
    "            # This will return the logits (logarithm of the odds), not the loss (we do not provide labels)\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Transfer logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Compute the accuracy for this batch of development sentences\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "        #data_seqeval[\"batch\"].append(str(batch))\n",
    "        #data_seqeval[\"true_tags\"].append(str(label_ids))\n",
    "        #data_seqeval[\"predicted_tags\"].append(str([list(p) for p in np.argmax(logits, axis=2)]))\n",
    "\n",
    "    #df_seqeval = pd.DataFrame(data_seqeval)\n",
    "    #wandb.log({f\"dataframe_seqeval\": wandb.Table(dataframe=df_seqeval)})\n",
    "    \n",
    "    eval_loss = eval_loss / len(dev_dataloader)\n",
    "    development_loss_values.append(eval_loss)\n",
    "    print(\"Development loss: {}\".format(eval_loss))\n",
    "    pred_tags = [id2label[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if id2label[l_i] != \"[PAD]\"]\n",
    "    dev_tags = [id2label[l_i] for l in true_labels\n",
    "                                  for l_i in l if id2label[l_i] != \"[PAD]\"]\n",
    "    f1 = f1_score(pred_tags, dev_tags, average='micro')\n",
    "\n",
    "    # Format output with 4 decimal places\n",
    "    output_text = \"train-val F1 score: {:.4f}\\n\".format(f1)\n",
    "\n",
    "    # Print to console\n",
    "    print(output_text)\n",
    "\n",
    "    # Save to a text file\n",
    "    with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
    "        file.write(output_text)\n",
    "    #print(\"Development classification report:\\n{}\".format(classification_report(pred_tags, dev_tags,digits=4)))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f51cd3e2dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(pred_tags, dev_tags)),\n",
    "               columns =['Pred', 'True'])\n",
    "df.to_csv(folder_name + '/train-val-result-bert.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5b5bf726c34c1",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa54a7b1f8075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(development_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95241329f809f83f",
   "metadata": {},
   "source": [
    "# Testing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a911e1ce59aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the test set\n",
    "# Set again the model into evaluation mode\n",
    "model.eval()\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "input_ids_list = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # The model must not compute or store gradients\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate predictions.\n",
    "        outputs = model(b_input_ids,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "    # Transfer logits and labels to CPU\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    input_ids_list.extend(b_input_ids)\n",
    "    \n",
    "    # Calculate the accuracy for this batch of test sentences\n",
    "    eval_loss += outputs[0].mean().item()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "pred_tags = [id2label[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if id2label[l_i] != \"[PAD]\"]\n",
    "test_tags = [id2label[l_i] for l in true_labels\n",
    "                                  for l_i in l if id2label[l_i] != \"[PAD]\"]\n",
    "#print(str(pred_tags))\n",
    "#print(str(test_tags))\n",
    "f1 = f1_score(pred_tags, test_tags, average='micro')\n",
    "\n",
    "# Format output with 4 decimal places\n",
    "output_text = \"Test F1 score: {:.4f}\\n\".format(f1)\n",
    "\n",
    "# Print to console\n",
    "print(output_text)\n",
    "\n",
    "# Save to a text file\n",
    "with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
    "    file.write(output_text)\n",
    "#print(\"Test classification report: {}\".format(classification_report(pred_tags, test_tags,digits=4)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42cd76-91e2-4530-a62c-c3e32a937a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(pred_tags, test_tags)),\n",
    "               columns =['Pred', 'True'])\n",
    "df.to_csv(folder_name + '/test-result-bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc31b3-c79a-4066-bf98-bc1d9bc4186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(folder_name+\"/test-result-bert.csv\")\n",
    "\n",
    "# Extract true and predicted labels\n",
    "y_true = df[\"True\"]\n",
    "y_pred = df[\"Pred\"]\n",
    "\n",
    "# Define the target classes\n",
    "target_classes = [\"B-class\", \"I-class\", \"B-attr\", \"I-attr\", \"O\"]\n",
    "\n",
    "# Compute precision, recall, and F1-score for the specified classes\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=target_classes, zero_division=0)\n",
    "\n",
    "# Compute overall accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Create a results dictionary\n",
    "metrics = pd.DataFrame({\n",
    "    \"Class\": target_classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1-Score\": f1\n",
    "})\n",
    "\n",
    "# Add overall accuracy\n",
    "metrics.loc[len(metrics)] = [\"Overall Accuracy\", accuracy, accuracy, accuracy]\n",
    "\n",
    "# Save metrics to a text file\n",
    "with open(folder_name + \"/f1_score.txt\", \"a\") as file:\n",
    "    file.write(metrics.to_string(index=False) + \"\\n\")\n",
    "\n",
    "# Display results\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94be0102f039b7",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce2b962f166c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241bd72f66c1bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_name_save)\n",
    "tokenizer.save_pretrained(model_name_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ef6e6-011f-4bf7-98e5-7ba6bc68d4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
