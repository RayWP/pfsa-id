{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e528e393",
   "metadata": {},
   "source": [
    "# Use Case Diagram Extraction Pipeline\n",
    "\n",
    "This notebook reads text documents from the target_text directory and processes them to extract class diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a65797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from spacy import displacy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee013dbe",
   "metadata": {},
   "source": [
    "# Reading Files from target_text Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeea468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for PlantUML files\n",
    "output_dir = os.path.join(os.path.dirname(os.getcwd()), \"survey/class_diagrams_output_prosus\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Output directory already exists: {output_dir}\")\n",
    "\n",
    "# Define target directory path\n",
    "target_dir = os.path.join(os.path.dirname(os.getcwd()), \"survey/target_text\")\n",
    "print(f\"Reading files from: {target_dir}\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(target_dir):\n",
    "    print(f\"Error: Directory '{target_dir}' does not exist.\")\n",
    "    raise FileNotFoundError(f\"Directory '{target_dir}' does not exist.\")\n",
    "\n",
    "# Get list of text files from the directory\n",
    "text_files = [f for f in os.listdir(target_dir) if f.endswith('.txt')]\n",
    "\n",
    "if not text_files:\n",
    "    print(\"No text files found in the directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(text_files)} text file(s): {text_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6066cf",
   "metadata": {},
   "source": [
    "# Process Each Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document_text, filename):\n",
    "    \"\"\"Preprocess the document text and split it into sentences\"\"\"\n",
    "    print(f\"Processing document: {filename}\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    print(\"Step 1: Preprocessing\")\n",
    "    # Remove newlines and extra spaces\n",
    "    document = re.sub(r'\\n', ' ', document_text)\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    \n",
    "    # Separate document by sentences into a dataframe\n",
    "    document_sentence = pd.DataFrame(document.split('.'), columns=['sentence'])\n",
    "    document_sentence = document_sentence[document_sentence['sentence'].str.strip().str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    return document_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd00cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model for sentence classification\n",
    "def load_sentence_classifier_model():\n",
    "    \"\"\"Load the pre-trained BERT model for sentence classification\"\"\"\n",
    "    model_dir = os.path.join(os.path.dirname(os.getcwd()), \"requirement_classification\", \"all_model\")\n",
    "    \n",
    "    # If multiple models exist, use the most recently created one\n",
    "    model_file = \"microsoft-deberta-v3-large_usecase_focus_epochs12_kfold10_batch8.bin\"\n",
    "    model_path = os.path.join(model_dir, model_file)\n",
    "    print(f\"Loading sentence classifier model from: {model_path}\")\n",
    "    \n",
    "    # Define model name based on file name pattern\n",
    "    model_name = \"ProsusAI/finbert\"  # Default model base\n",
    "    \n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ada9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextDataset class for sentence classification\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentences(document_sentence, filename, output_dir):\n",
    "    \"\"\"Classify sentences to identify those useful for class diagram extraction using BERT model\"\"\"\n",
    "    print(\"Step 2: Categorizing sentences using pre-trained model\")\n",
    "    document_sentence['useful'] = 0\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    try:\n",
    "        model, tokenizer, device = load_sentence_classifier_model()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentence classifier model: {e}\")\n",
    "        print(\"Falling back to default classification method\")\n",
    "        # You could implement a fallback method here\n",
    "        return document_sentence\n",
    "    \n",
    "    # Create dataset from sentences\n",
    "    sentences = document_sentence['sentence'].tolist()\n",
    "    dataset = TextDataset(sentences, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Process batches and get predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Classifying sentences\"):\n",
    "            # Move inputs to device\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predicted class (0 = not useful, 1 = useful)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Update dataframe with predictions\n",
    "    document_sentence['useful'] = predictions\n",
    "    \n",
    "    # Log results\n",
    "    useful_count = sum(predictions)\n",
    "    print(f\"Found {useful_count} useful sentences out of {len(sentences)} total sentences\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_checkpoint.csv\")\n",
    "    document_sentence.to_csv(checkpoint_file)\n",
    "    print(f\"Saved checkpoint to {checkpoint_file}\")\n",
    "    \n",
    "    return document_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(document_sentence, filename, output_dir):\n",
    "    \"\"\"Extract entities from useful sentences\"\"\"\n",
    "    # Extract sentences marked as useful for class diagram extraction\n",
    "    print(\"Step 3: Extracting class diagram entities\")\n",
    "    sentence_class_diagram_only = document_sentence[document_sentence['useful'] == 1]\n",
    "    document_class = ' '.join(sentence_class_diagram_only['sentence'].tolist())\n",
    "    \n",
    "    # Entity extraction using the model\n",
    "    model_path = os.path.join(os.path.dirname(os.getcwd()), \"key-term-extraction\", \"BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\")\n",
    "    print(f\"Using NER model from: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\"ner\", model=model_path, aggregation_strategy=\"simple\")\n",
    "        entities = ner_pipeline(document_class)\n",
    "        \n",
    "        # Process entities\n",
    "        summary = {\n",
    "            \"class\": defaultdict(int),\n",
    "            \"attr\": defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for ent in entities:\n",
    "            entity_type = ent[\"entity_group\"].lower()\n",
    "            word = ent[\"word\"]\n",
    "            \n",
    "            if entity_type in summary:\n",
    "                summary[entity_type][word] += 1\n",
    "        \n",
    "        # Convert defaultdict to normal dict\n",
    "        summary = {key: list(value.keys()) for key, value in summary.items()}\n",
    "        \n",
    "        # Save entity data as CSV\n",
    "        entity_csv_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.csv\")\n",
    "        pd.DataFrame(entities).to_csv(entity_csv_file)\n",
    "        print(f\"Entities saved to CSV: {entity_csv_file}\")\n",
    "        \n",
    "        # Convert entities to JSON serializable format\n",
    "        serializable_entities = []\n",
    "        for ent in entities:\n",
    "            # Extract only serializable properties and convert non-serializable types\n",
    "            serializable_ent = {\n",
    "                \"entity_group\": ent[\"entity_group\"],\n",
    "                \"word\": ent[\"word\"],\n",
    "                \"score\": float(ent[\"score\"]),  # Convert tensor to float if needed\n",
    "                \"start\": ent[\"start\"],\n",
    "                \"end\": ent[\"end\"]\n",
    "            }\n",
    "            serializable_entities.append(serializable_ent)\n",
    "        \n",
    "        # Save entity data as JSON\n",
    "        import json\n",
    "        entity_json_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.json\")\n",
    "        with open(entity_json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"entities\": serializable_entities,  # Use serializable entities\n",
    "                \"summary\": summary,\n",
    "                \"document_class\": document_class\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Entities saved to JSON: {entity_json_file}\")\n",
    "        \n",
    "        # Save entity data as plain text\n",
    "        entity_txt_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.txt\")\n",
    "        with open(entity_txt_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Document: {filename}\\n\\n\")\n",
    "            f.write(f\"Classes identified:\\n{'-'*20}\\n\")\n",
    "            for cls in summary['class']:\n",
    "                f.write(f\"- {cls}\\n\")\n",
    "            f.write(f\"\\nAttributes identified:\\n{'-'*20}\\n\")\n",
    "            for attr in summary['attr']:\n",
    "                f.write(f\"- {attr}\\n\")\n",
    "            f.write(f\"\\nDetailed Entities:\\n{'-'*20}\\n\")\n",
    "            for ent in serializable_entities:  # Use serializable entities\n",
    "                f.write(f\"Type: {ent['entity_group']}, Word: {ent['word']}, Score: {ent['score']:.4f}\\n\")\n",
    "        print(f\"Entities saved to TXT: {entity_txt_file}\")\n",
    "        \n",
    "        # Return serializable entities for further processing\n",
    "        return {\n",
    "            \"entities\": serializable_entities,  # Use serializable entities\n",
    "            \"summary\": summary,\n",
    "            \"document_class\": document_class\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diagram(extraction_result, filename, output_dir):\n",
    "    \"\"\"Generate class diagram from extracted entities\"\"\"\n",
    "    print(\"Step 4: Generating PlantUML code\")\n",
    "    \n",
    "    # Check if we have valid extraction results\n",
    "    if \"error\" in extraction_result:\n",
    "        print(f\"Cannot generate diagram due to extraction error: {extraction_result['error']}\")\n",
    "        return {\"filename\": filename, \"error\": extraction_result['error']}\n",
    "    \n",
    "    try:\n",
    "        # Prepare summary for diagram generation\n",
    "        summary = extraction_result[\"summary\"]\n",
    "        document_class = extraction_result[\"document_class\"]\n",
    "        summary_string = f\"class: {summary['class']}, attribute: {summary['attr']}, description: {document_class}\"\n",
    "        \n",
    "        # Generate PlantUML using Azure OpenAI\n",
    "        chat_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You will be given a JSON of class names, attributes, and a system description. Your task is to generate plantuml script containing classes, attributes, and relationships according to the system description. Strictly produce only plantuml script\"\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": summary_string\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=chat_prompt,\n",
    "            max_tokens=800,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        plantuml_result = completion.choices[0].message.content\n",
    "        \n",
    "        # Clean up the result and save to file\n",
    "        plantuml_result = plantuml_result.strip('```plantuml')\n",
    "        plantuml_result = plantuml_result.strip('```')\n",
    "        \n",
    "        output_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_class_diagram.puml\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(plantuml_result)\n",
    "        print(f\"PlantUML diagram saved to {output_file}\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"entities\": extraction_result[\"entities\"],\n",
    "            \"plantuml_code\": plantuml_result,\n",
    "            \"output_file\": output_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating diagram: {e}\")\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document_text, filename):\n",
    "    \"\"\"Process a document through the entire pipeline\"\"\"\n",
    "    # Step 1: Preprocess document\n",
    "    document_sentence = preprocess_document(document_text, filename)\n",
    "    \n",
    "    # Step 2: Classify sentences\n",
    "    document_sentence = classify_sentences(document_sentence, filename, output_dir)\n",
    "    \n",
    "    # Step 3: Extract entities\n",
    "    extraction_result = extract_entities(document_sentence, filename, output_dir)\n",
    "\n",
    "    return \"true \" + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files in the target directory\n",
    "results = []\n",
    "\n",
    "for filename in text_files:\n",
    "    file_path = os.path.join(target_dir, filename)\n",
    "    print(f\"\\n{'='*50}\\nProcessing file: {filename}\\n{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            document_text = file.read()\n",
    "            \n",
    "            result = process_document(document_text, filename)\n",
    "            results.append(result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filename}: {e}\")\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        \n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"Processed {len(results)} files.\")\n",
    "print(f\"Output saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491cb9a",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f90122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4a591d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI endpoint: https://dewi.openai.azure.com/\n",
      "Azure OpenAI deployment: o3-mini\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URL_1\", \"\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_1\", \"\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY_1\", \"\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION_1\")\n",
    "\n",
    "print(f\"Azure OpenAI endpoint: {endpoint}\")\n",
    "print(f\"Azure OpenAI deployment: {deployment}\")\n",
    "\n",
    "# Initialize Azure OpenAI Service client with key-based authentication\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "138d4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_diagram_with_azure(classes, attributes, document_text, filename):\n",
    "    \"\"\"\n",
    "    Generate a class diagram using Azure OpenAI by sending a request with the extracted classes, attributes,\n",
    "    and document text.\n",
    "    \n",
    "    Args:\n",
    "        classes (list): List of extracted class names\n",
    "        attributes (list): List of extracted attributes\n",
    "        document_text (str): The full document text used for context\n",
    "        filename (str): The name of the file being processed (without extension)\n",
    "    \n",
    "    Returns:\n",
    "        str: PlantUML code for the class diagram\n",
    "    \"\"\"\n",
    "    # Prepare summary for diagram generation\n",
    "    summary_string = f\"class: {classes}, attribute: {attributes}, description: {document_text}\"\n",
    "    \n",
    "    mode = \"gen-nr-nent\"\n",
    "    # Create plantuml_result folder if it doesn't exist\n",
    "    plantuml_result_dir = \"./\"+ mode + \"-\" + deployment\n",
    "\n",
    "    if mode == \"gen-r-ent\": #with restrictions and entities\n",
    "        prompt = '''Given a description of software requirement: {document_text}\n",
    "                There are list of entities\n",
    "                List of classes:  {classes}\n",
    "                List of attributes: {attributes}\n",
    "\n",
    "                Generate a Class Diagram according to the above description and these factors:\n",
    "                - preferably use the class names and attributes provided, but you can also create new ones if necessary\n",
    "                - similar attribute could be merged as one\n",
    "                - a class could be and attribute for another class\n",
    "                - discover the relationships between classes and attributes as many as possible correctly\n",
    "                - discover method for each classes when possible\n",
    "                Set the output strictly to only PlantUML of the result diagram \n",
    "                '''.format(document_text=document_text, classes=classes, attributes=attributes)\n",
    "        \n",
    "    elif mode == \"gen-r-nent\": #with restrictions and entities\n",
    "        prompt = '''Given a description of software requirement: {document_text}\n",
    " \n",
    "                Generate a Class Diagram according these factors:\n",
    "                - a class could be and attribute for another class\n",
    "                - discover the relationships between classes and attributes as many as possible correctly\n",
    "                - discover method for each classes when possible\n",
    "                Set the output strictly to only PlantUML of the result diagram \n",
    "                '''.format(document_text=document_text, classes=classes, attributes=attributes)\n",
    "        \n",
    "    elif mode == \"gen-nr-nent\": #no resctrictions and no entities\n",
    "        prompt = '''Given a description of software requirement: {document_text}\n",
    "\n",
    "                Generate a Class Diagram according\n",
    "                Set the output strictly to only PlantUML of the result diagram \n",
    "                '''.format(document_text=document_text, classes=classes, attributes=attributes)\n",
    "    \n",
    "    elif mode == \"gen-nr-ent\": #no restrictions and entities\n",
    "        prompt = '''Given a description of software requirement: {document_text}\n",
    "                There are list of entities\n",
    "                List of classes:  {classes}\n",
    "                List of attributes: {attributes}\n",
    "\n",
    "                Generate a Class Diagram according\n",
    "                Set the output strictly to only PlantUML of the result diagram \n",
    "                '''.format(document_text=document_text, classes=classes, attributes=attributes)\n",
    "\n",
    "    \n",
    "    chat_prompt = [\n",
    "            {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [{\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                }]\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    print(chat_prompt)\n",
    "    # Use the existing client from previous cells\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=chat_prompt,\n",
    "        # max_tokens=1000,\n",
    "        # temperature=0,\n",
    "        # top_p=0.95,\n",
    "        # frequency_penalty=0,\n",
    "        # presence_penalty=0,\n",
    "        # stop=None,\n",
    "        # stream=False\n",
    "        max_completion_tokens=100000\n",
    "    )\n",
    "    \n",
    "    # Extract and clean up the result\n",
    "    plantuml_result = completion.choices[0].message.content\n",
    "    \n",
    "    # Clean up any markdown code block markers\n",
    "    plantuml_result = plantuml_result.strip()\n",
    "    plantuml_result = plantuml_result.replace(\"```plantuml\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "\n",
    "    if not os.path.exists(plantuml_result_dir):\n",
    "        os.makedirs(plantuml_result_dir)\n",
    "        print(f\"Created directory: {plantuml_result_dir}\")\n",
    "    \n",
    "    # Save PlantUML code to file\n",
    "    output_file = os.path.join(plantuml_result_dir, f\"{filename}_class_diagram.puml\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(plantuml_result)\n",
    "    print(f\"PlantUML diagram saved to {output_file}\")\n",
    "    \n",
    "    return plantuml_result, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb6e9fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 JSON files\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': 'Given a description of software requirement:  You will start by tracking persons, then add policies, then contact info and vehicles and eventually be able to generalize the model and increase its flexibility  We want to get to a point where you can retrieve a person, their policies, vehicles, dwellings, addresses, and contact info  Collections of Policies There is a link between Persons and Policies  A Person object can hold a collection of multiple policy objects List to your Person class to hold instances of Policy objects  Test this works in the main(): Create two policy objects for each Person Add a policy object to the Person by calling addPolicy() Retrieve the policy object and print its details for each customer Policy Details Policies have additional information the company wishes to keep  Policies have the contact information: Phone Numbers, Addresses, and Emails  Note that the contact classes have a method: contactPointAsString(), which returns a string representation of the contact information  Since the policy keeps track of these things, create separate list objects in the Policy class to track these items  Like the Person, you should create addEmail(), addPhone(), addAddress(), addVehicle() methods and getVehicles(), getPhones()â€¦ methods to store the objects in the policy and retrieve all objects of that type from the policy  Test that this works by adding some of the things you created to the policy and then printing: The Person info For Each policy, its info and a list of its contacts and vehicles  Generalize the Model You do business with companies and persons, so add a class to track Company information  Create a common superclass called a party representing those (persons or companies) with policies  Move the collection of policies up to the Party superclass, along with the addPolicy() and getPolicies() methods  Note that the party has a name() method that returns a String representation of the name  Create a new company, two polices and some contact info, and two vehicles for each policy  Now, add the Person and company objects to the Party Array  Dwellings are another type of thing  You will need to create these physical things in the main() and call a new addPhysicalObject() method to add the things to the list in the policy object  You should now be able to print out the policy, its contact points, and its physical things  When you print the physical things, print the description and names, which are the attributes common to the physical object class\\n\\n                Generate a Class Diagram according\\n                Set the output strictly to only PlantUML of the result diagram \\n                '}]}]\n",
      "PlantUML diagram saved to ./gen-nr-nent-o3-mini\\R39_Insurance_class_diagram.puml\n",
      "Successfully extracted data from 0 files\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import glob\n",
    "import os \n",
    "\n",
    "output_dir = \"./insurance\"\n",
    "# Get all JSON files from the output directory\n",
    "json_files = glob.glob(os.path.join(output_dir, \"*_entities.json\"))\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "# Parse each JSON file and extract the required information\n",
    "extracted_data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        filename = os.path.basename(json_file).replace('_entities.json', '')\n",
    "        \n",
    "        # Extract document text, classes, and attributes\n",
    "        document = data.get('document_class', '')\n",
    "        classes = data.get('summary', {}).get('class', [])\n",
    "        attributes = data.get('summary', {}).get('attr', [])\n",
    "        \n",
    "        generate_class_diagram_with_azure(classes, attributes, document, filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {json_file}: {e}\")\n",
    "\n",
    "print(f\"Successfully extracted data from {len(extracted_data)} files\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
