{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e528e393",
   "metadata": {},
   "source": [
    "# Class Diagram Extraction Pipeline\n",
    "\n",
    "This notebook reads text documents from the target_text directory and processes them to extract class diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a65797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from spacy import displacy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee013dbe",
   "metadata": {},
   "source": [
    "# Reading Files from target_text Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeea468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for PlantUML files\n",
    "output_dir = os.path.join(os.path.dirname(os.getcwd()), \"survey/class_diagrams_output_prosus\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Output directory already exists: {output_dir}\")\n",
    "\n",
    "# Define target directory path\n",
    "target_dir = os.path.join(os.path.dirname(os.getcwd()), \"survey/target_text\")\n",
    "print(f\"Reading files from: {target_dir}\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(target_dir):\n",
    "    print(f\"Error: Directory '{target_dir}' does not exist.\")\n",
    "    raise FileNotFoundError(f\"Directory '{target_dir}' does not exist.\")\n",
    "\n",
    "# Get list of text files from the directory\n",
    "text_files = [f for f in os.listdir(target_dir) if f.endswith('.txt')]\n",
    "\n",
    "if not text_files:\n",
    "    print(\"No text files found in the directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(text_files)} text file(s): {text_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6066cf",
   "metadata": {},
   "source": [
    "# Process Each Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document_text, filename):\n",
    "    \"\"\"Preprocess the document text and split it into sentences\"\"\"\n",
    "    print(f\"Processing document: {filename}\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    print(\"Step 1: Preprocessing\")\n",
    "    # Remove newlines and extra spaces\n",
    "    document = re.sub(r'\\n', ' ', document_text)\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    \n",
    "    # Separate document by sentences into a dataframe\n",
    "    document_sentence = pd.DataFrame(document.split('.'), columns=['sentence'])\n",
    "    document_sentence = document_sentence[document_sentence['sentence'].str.strip().str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    return document_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd00cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model for sentence classification\n",
    "def load_sentence_classifier_model():\n",
    "    \"\"\"Load the pre-trained BERT model for sentence classification\"\"\"\n",
    "    model_dir = os.path.join(os.path.dirname(os.getcwd()), \"requirement_classification\", \"all_model\")\n",
    "    \n",
    "    # If multiple models exist, use the most recently created one\n",
    "    model_file = \"ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\"\n",
    "    model_path = os.path.join(model_dir, model_file)\n",
    "    print(f\"Loading sentence classifier model from: {model_path}\")\n",
    "    \n",
    "    # Define model name based on file name pattern\n",
    "    model_name = \"ProsusAI/finbert\"  # Default model base\n",
    "    \n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ada9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextDataset class for sentence classification\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentences(document_sentence, filename, output_dir):\n",
    "    \"\"\"Classify sentences to identify those useful for class diagram extraction using BERT model\"\"\"\n",
    "    print(\"Step 2: Categorizing sentences using pre-trained model\")\n",
    "    document_sentence['useful'] = 0\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    try:\n",
    "        model, tokenizer, device = load_sentence_classifier_model()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentence classifier model: {e}\")\n",
    "        print(\"Falling back to default classification method\")\n",
    "        # You could implement a fallback method here\n",
    "        return document_sentence\n",
    "    \n",
    "    # Create dataset from sentences\n",
    "    sentences = document_sentence['sentence'].tolist()\n",
    "    dataset = TextDataset(sentences, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Process batches and get predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Classifying sentences\"):\n",
    "            # Move inputs to device\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predicted class (0 = not useful, 1 = useful)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Update dataframe with predictions\n",
    "    document_sentence['useful'] = predictions\n",
    "    \n",
    "    # Log results\n",
    "    useful_count = sum(predictions)\n",
    "    print(f\"Found {useful_count} useful sentences out of {len(sentences)} total sentences\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_checkpoint.csv\")\n",
    "    document_sentence.to_csv(checkpoint_file)\n",
    "    print(f\"Saved checkpoint to {checkpoint_file}\")\n",
    "    \n",
    "    return document_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(document_sentence, filename, output_dir):\n",
    "    \"\"\"Extract entities from useful sentences\"\"\"\n",
    "    # Extract sentences marked as useful for class diagram extraction\n",
    "    print(\"Step 3: Extracting class diagram entities\")\n",
    "    sentence_class_diagram_only = document_sentence[document_sentence['useful'] == 1]\n",
    "    document_class = ' '.join(sentence_class_diagram_only['sentence'].tolist())\n",
    "    \n",
    "    # Entity extraction using the model\n",
    "    model_path = os.path.join(os.path.dirname(os.getcwd()), \"key-term-extraction\", \"BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\")\n",
    "    print(f\"Using NER model from: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\"ner\", model=model_path, aggregation_strategy=\"simple\")\n",
    "        entities = ner_pipeline(document_class)\n",
    "        \n",
    "        # Process entities\n",
    "        summary = {\n",
    "            \"class\": defaultdict(int),\n",
    "            \"attr\": defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for ent in entities:\n",
    "            entity_type = ent[\"entity_group\"].lower()\n",
    "            word = ent[\"word\"]\n",
    "            \n",
    "            if entity_type in summary:\n",
    "                summary[entity_type][word] += 1\n",
    "        \n",
    "        # Convert defaultdict to normal dict\n",
    "        summary = {key: list(value.keys()) for key, value in summary.items()}\n",
    "        \n",
    "        # Save entity data as CSV\n",
    "        entity_csv_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.csv\")\n",
    "        pd.DataFrame(entities).to_csv(entity_csv_file)\n",
    "        print(f\"Entities saved to CSV: {entity_csv_file}\")\n",
    "        \n",
    "        # Convert entities to JSON serializable format\n",
    "        serializable_entities = []\n",
    "        for ent in entities:\n",
    "            # Extract only serializable properties and convert non-serializable types\n",
    "            serializable_ent = {\n",
    "                \"entity_group\": ent[\"entity_group\"],\n",
    "                \"word\": ent[\"word\"],\n",
    "                \"score\": float(ent[\"score\"]),  # Convert tensor to float if needed\n",
    "                \"start\": ent[\"start\"],\n",
    "                \"end\": ent[\"end\"]\n",
    "            }\n",
    "            serializable_entities.append(serializable_ent)\n",
    "        \n",
    "        # Save entity data as JSON\n",
    "        import json\n",
    "        entity_json_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.json\")\n",
    "        with open(entity_json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"entities\": serializable_entities,  # Use serializable entities\n",
    "                \"summary\": summary,\n",
    "                \"document_class\": document_class\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Entities saved to JSON: {entity_json_file}\")\n",
    "        \n",
    "        # Save entity data as plain text\n",
    "        entity_txt_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.txt\")\n",
    "        with open(entity_txt_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Document: {filename}\\n\\n\")\n",
    "            f.write(f\"Classes identified:\\n{'-'*20}\\n\")\n",
    "            for cls in summary['class']:\n",
    "                f.write(f\"- {cls}\\n\")\n",
    "            f.write(f\"\\nAttributes identified:\\n{'-'*20}\\n\")\n",
    "            for attr in summary['attr']:\n",
    "                f.write(f\"- {attr}\\n\")\n",
    "            f.write(f\"\\nDetailed Entities:\\n{'-'*20}\\n\")\n",
    "            for ent in serializable_entities:  # Use serializable entities\n",
    "                f.write(f\"Type: {ent['entity_group']}, Word: {ent['word']}, Score: {ent['score']:.4f}\\n\")\n",
    "        print(f\"Entities saved to TXT: {entity_txt_file}\")\n",
    "        \n",
    "        # Return serializable entities for further processing\n",
    "        return {\n",
    "            \"entities\": serializable_entities,  # Use serializable entities\n",
    "            \"summary\": summary,\n",
    "            \"document_class\": document_class\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diagram(extraction_result, filename, output_dir):\n",
    "    \"\"\"Generate class diagram from extracted entities\"\"\"\n",
    "    print(\"Step 4: Generating PlantUML code\")\n",
    "    \n",
    "    # Check if we have valid extraction results\n",
    "    if \"error\" in extraction_result:\n",
    "        print(f\"Cannot generate diagram due to extraction error: {extraction_result['error']}\")\n",
    "        return {\"filename\": filename, \"error\": extraction_result['error']}\n",
    "    \n",
    "    try:\n",
    "        # Prepare summary for diagram generation\n",
    "        summary = extraction_result[\"summary\"]\n",
    "        document_class = extraction_result[\"document_class\"]\n",
    "        summary_string = f\"class: {summary['class']}, attribute: {summary['attr']}, description: {document_class}\"\n",
    "        \n",
    "        # Generate PlantUML using Azure OpenAI\n",
    "        chat_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You will be given a JSON of class names, attributes, and a system description. Your task is to generate plantuml script containing classes, attributes, and relationships according to the system description. Strictly produce only plantuml script\"\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": summary_string\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=chat_prompt,\n",
    "            max_tokens=800,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        plantuml_result = completion.choices[0].message.content\n",
    "        \n",
    "        # Clean up the result and save to file\n",
    "        plantuml_result = plantuml_result.strip('```plantuml')\n",
    "        plantuml_result = plantuml_result.strip('```')\n",
    "        \n",
    "        output_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_class_diagram.puml\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(plantuml_result)\n",
    "        print(f\"PlantUML diagram saved to {output_file}\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"entities\": extraction_result[\"entities\"],\n",
    "            \"plantuml_code\": plantuml_result,\n",
    "            \"output_file\": output_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating diagram: {e}\")\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document_text, filename):\n",
    "    \"\"\"Process a document through the entire pipeline\"\"\"\n",
    "    # Step 1: Preprocess document\n",
    "    document_sentence = preprocess_document(document_text, filename)\n",
    "    \n",
    "    # Step 2: Classify sentences\n",
    "    document_sentence = classify_sentences(document_sentence, filename, output_dir)\n",
    "    \n",
    "    # Step 3: Extract entities\n",
    "    extraction_result = extract_entities(document_sentence, filename, output_dir)\n",
    "\n",
    "    return \"true \" + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ecd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files in the target directory\n",
    "results = []\n",
    "\n",
    "for filename in text_files:\n",
    "    file_path = os.path.join(target_dir, filename)\n",
    "    print(f\"\\n{'='*50}\\nProcessing file: {filename}\\n{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            document_text = file.read()\n",
    "            \n",
    "            result = process_document(document_text, filename)\n",
    "            results.append(result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filename}: {e}\")\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        \n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"Processed {len(results)} files.\")\n",
    "print(f\"Output saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491cb9a",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f90122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4a591d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI endpoint: https://dewi.openai.azure.com/\n",
      "Azure OpenAI deployment: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URL_1\", \"\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_1\", \"\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY_1\", \"\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION_1\")\n",
    "\n",
    "print(f\"Azure OpenAI endpoint: {endpoint}\")\n",
    "print(f\"Azure OpenAI deployment: {deployment}\")\n",
    "\n",
    "# Initialize Azure OpenAI Service client with key-based authentication\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "138d4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_class_diagram_with_azure(classes, attributes, document_text, filename):\n",
    "    \"\"\"\n",
    "    Generate a class diagram using Azure OpenAI by sending a request with the extracted classes, attributes,\n",
    "    and document text.\n",
    "    \n",
    "    Args:\n",
    "        classes (list): List of extracted class names\n",
    "        attributes (list): List of extracted attributes\n",
    "        document_text (str): The full document text used for context\n",
    "        filename (str): The name of the file being processed (without extension)\n",
    "    \n",
    "    Returns:\n",
    "        str: PlantUML code for the class diagram\n",
    "    \"\"\"\n",
    "    # Prepare summary for diagram generation\n",
    "    summary_string = f\"class: {classes}, attribute: {attributes}, description: {document_text}\"\n",
    "    \n",
    "    # Generate PlantUML using Azure OpenAI\n",
    "    chat_prompt = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"type\": \"text\",\n",
    "                \"text\":    '''Given a description of software requirement: {document_text}\n",
    "There are list of entities\n",
    "List of classes:  {classes}\n",
    "List of attributes: {attributes}\n",
    "\n",
    "Generate a Class Diagram based on the description and entities with following factor:\n",
    "-\tprioritize using entities given\n",
    "-\tDiscover relationship between classes according to the description\n",
    "-\tAssign attributes to the classes according to the description\n",
    "-   Since a class could be an attribute of another class, it is allowed when necessary\n",
    "Set the output strictly to only PlantUML of the result diagram \n",
    "'''.format(document_text=document_text, classes=classes, attributes=attributes)\n",
    "            }]\n",
    "        },\n",
    "    ]\n",
    "    print(chat_prompt)\n",
    "    # Use the existing client from previous cells\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=chat_prompt,\n",
    "        max_tokens=800,\n",
    "        temperature=0,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    # Extract and clean up the result\n",
    "    plantuml_result = completion.choices[0].message.content\n",
    "    \n",
    "    # Clean up any markdown code block markers\n",
    "    plantuml_result = plantuml_result.strip()\n",
    "    plantuml_result = plantuml_result.replace(\"```plantuml\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    # Create plantuml_result folder if it doesn't exist\n",
    "    plantuml_result_dir = \"./plantuml_result_combine\"\n",
    "    if not os.path.exists(plantuml_result_dir):\n",
    "        os.makedirs(plantuml_result_dir)\n",
    "        print(f\"Created directory: {plantuml_result_dir}\")\n",
    "    \n",
    "    # Save PlantUML code to file\n",
    "    output_file = os.path.join(plantuml_result_dir, f\"{filename}_class_diagram.puml\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(plantuml_result)\n",
    "    print(f\"PlantUML diagram saved to {output_file}\")\n",
    "    \n",
    "    return plantuml_result, output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb6e9fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 JSON files\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': \"Given a description of software requirement: The clinic basically schedules patients, provides services for them, and bills them for those services  New patients fill out a form listing their name, address, telephone numbers, allergies, and state of mind prior to scheduling their first appointment  Schedules are entered into a central appointment book; patient records (including contact information) are kept in paper files  Appointments are for one of three procedures: dental hygiene, cavities and fillings, and oral surgery (including root canals and tooth extractions)  For each procedure the patient needs to be prepared and supplies need to be collected (e , probes, drill bits, cements, resins, etc  Billing is always done by the month, and bills are always sent by mail to patients' contact addresses  Each patient also generates a reimbursement request to an insurance company  The clinic maintains a supplies inventory file that a worker fills out once a week by physically inspecting each of the three procedures rooms\\nThere are list of entities\\nList of classes:  ['clinic', 'patients', 'services', 'form', 'first appointment', 'central appointment book', 'patient records', 'Appointments', 'procedures', 'procedure', 'patient', 'bills', 'reimbursement request', 'insurance', 'supplies inventory file', 'rooms']\\nList of attributes: ['name', 'address', 'telephone numbers', 'allergies', 'state of mind', 'contact information', 'supplies', 'contact addresses']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n\"}]}]\n",
      "Created directory: ./plantuml_result_combine\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\dental-clinic_class_diagram.puml\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': \"Given a description of software requirement: Geological samples are retrieved from the field and then processed in the laboratory to determine various properties, including chemistry, mineralogy, age, and petrophysical properties like density, porosity, permeability  Samples obtained as part of economic activities, such as mineral exploration, are usually processed in commercial assay and chemistry labs  For confidentiality reasons the location information associated with each sample is not provided to the lab, but must be re-attached during the interpretation phase  During processing, many derived samples will be generated by various physical and chemical procedures  In some cases the derived samples are strict sub-samples, whose intensive properties are intended to be the same as the parent  In other cases, the split is 'biased', with the derived sample intended to select a specific sub-sample, defined by a particular particle size, density, magnetic properties, etc  The link from the derived sample to the parent sample must be preserved, and the link from the parent to the location from which it was obtained also  In some cases the location is associated with another sampling artifact, such as a drill-hole or traverse or cruise, with the latter carrying the detailed location information  In a research context some samples have a particularly high-value, having been obtained by an expensive process (involving drilling or ships or spacecraft) or from a location that is hard to visit (remote, offshore, in space)  Each lab will run its own LIMS system, which will usually assign a local identifier for the sample  When the results of these observations are reported, it is necessary that observations from different labs can be correlated with each other, so that the complete picture around each sample can be assembled  These stories focus on sensing applications involving ex-situ sampling, where a location is visited and a specimen obtained using some sampling process, then transported to one or more laboratories where it is processed into one or more sub-samples and various observations made  Sample identity is usually key, and the relationships between samples, between samples and other artifacts of the sampling process, and also with other geographic features or locations  The sampling time and analysis and reporting time are all different  Similar process apply to botanical sampling, and to environmental sampling (water, air, dust)\\nThere are list of entities\\nList of classes:  ['Geological samples', 'field', 'sample', 'lab', 'derived', 'samples', 'sub-samples', 'sub-sample', 'parent', 'LIMS system', 'observations', 'labs', 'location', 'specimen']\\nList of attributes: ['properties', 'chemistry', 'mineralogy', 'age', 'petrophysical properties', 'density', 'porosity', 'permeability', 'location information', 'particle size', 'magnetic properties', 'parent', 'location', 'local identifier', 'Sample', 'geographic', 'sampling time', 'analysis', 'time']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n\"}]}]\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\geological-samples-observation_class_diagram.puml\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': \"Given a description of software requirement:  Sometimes, addresses start with a house name or number, but some overseas clients put the city or town first  Imagine the confusion of sending something to Paris which we thought was the city but was in fact the house name! The other issue is that overseas zip codes or postal codes, as they are sometimes called, are not always numeric—they can be a combination of numbers and letters  Analyst: When it’s an organization, do you always need to know the contact? AP: Yes, and sometimes there are more than one  For each client or representative of an organization, I need to write down their full name and how they prefer their honorific: Mr  And of course, their email address, phone number, and postal address, and sometimes we have a primary mailing address that could be different from the billing address  And I must record the information about their involvement as well  We also need to indicate if this event is billable or not\\nThere are list of entities\\nList of classes:  ['clients', 'organization', 'client', 'event']\\nList of attributes: ['addresses', 'house name', 'number', 'city', 'town', 'zip codes', 'postal codes', 'contact', 'representative', 'full name', 'honorific', 'email address', 'phone number', 'postal address', 'primary mailing address', 'billing address']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n\"}]}]\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\law-firm_class_diagram.puml\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': 'Given a description of software requirement:  There will be two separate applications for the mobile platforms, which are Android and iOS  They should have the same function and features for the account creation process  The project will also contain the back-end communication system for the kiosk and branches  Requirements Menu Placement The \"Block Card\" process should be under the \"Cards\" menu  It should be added after the \"My Cards\" page  Customer Users can have four types of cards: debit, credit, virtual and supplementary  Software System The system should check the card status when the user blocks the desired card  The user can initiate the card-blocking process if the card status is available  The blocking reasons can be stolen, missing, and others\\nThere are list of entities\\nList of classes:  [\\'account\\', \\'kiosk\\', \\'Customer\\', \\'Users\\', \\'system\\', \\'user\\', \\'card\\']\\nList of attributes: [\\'Cards\\', \\'card status\\']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n'}]}]\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\R26_BlockCard_class_diagram.puml\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': 'Given a description of software requirement:  A unique storeID identifies each store  Additional information about each store is the store address and phone number  An employee ID identifies each employee  Additional information about an employee is the employee\\'s name, address, and phone number  Each employee has at least one and could have many phone numbers  Each employee\\'s phone number is classified by a phone number type, such as an office phone number, home phone number, beeper number, etc  Each employee is classified as one type of employee: \"manager,\" \"cashier,\" or \"stocker  An employee type such as \"manager,\" \"cashier,\" or \"stocker\" can classify many employees  SchlockBuster maintains a listing of titles by a title ID  Additional information for each title includes the title\\'s name, run time(such as 120 minutes), and rating (G, PG, R)  SchlockBuster maintains a listing of distributors by distributor ID  Additional information about a distributor includes the distributor\\'s name and phone number  Many stores can carry a title  A store can stock multiple copies (the physical cassette cartridge) of the same title \" A cartridge ID number identifies each cartridge  A cartridge utilization count is also maintained for each cartridge  A cartridge status classifies each cartridge  The domain of cartridge status is Available\", \"Rented,\" and \"Broken \" A status type can apply to many cartridges  A unique customer ID identifies each customer  Additional information maintained about each customer is the customer\\'s name and the customer\\'s phone number  Each customer is associated with the store where they enrolled as members to receive their SchlockBuster Video card  A rental ID number identifies each rental transaction  Each rental transaction is associated with (rented from) one and only one store  Each rental transaction also captures the rental date and total rental amount  Each rental transaction is associated with one or more rental transaction detail lines  Each rental transaction detail line captures the id of the cartridge being rented, the due date of the cartridge being rented, and the actual return date\\nThere are list of entities\\nList of classes:  [\\'store\\', \\'employee\\', \\'manager\\', \\'cashier\\', \\'stock\\', \\'stocker\\', \\'employees\\', \\'distributors\\', \\'distributor\\', \\'stores\\', \\'cartridge\\', \\'cartridges\\', \\'customer\\', \\'rental transaction\\', \\'rental\\', \\'transaction\\', \\'rental transaction detail lines\\']\\nList of attributes: [\\'storeID\\', \\'store\\', \\'address\\', \\'phone number\\', \\'employee ID\\', \\'employee\\', \\'name\\', \\'phone numbers\\', \\'phone number type\\', \\'office phone number\\', \\'home phone number\\', \\'beeper number\\', \\'type\\', \\'employee type\\', \\'titles\\', \\'title ID\\', \\'title\\', \\'run time\\', \\'rating\\', \\'distributor ID\\', \\'distributor\\', \\'cartridge ID number\\', \\'cartridge\\', \\'utilization count\\', \\'status\\', \\'cartridge status\\', \\'Available\\', \\'Rented\\', \\'Broken\\', \\'status type\\', \\'customer ID\\', \\'customer\\', \\'rental ID number\\', \\'rental date\\', \\'total rental amount\\', \\'transaction detail\\', \\'id\\', \\'rented\\', \\'due date\\', \\'actual return date\\']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n'}]}]\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\R36_Video Rental_class_diagram.puml\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': 'Given a description of software requirement: System Features Torrent Search Description and Priority The user will give a search term that will search through a database of compatible torrent websites added by the development team  This is one of the software\\'s main features and therefore has a high priority for development  Results for this query will then be sent to the torrent tab in the program, where they will display information such as website, seeds, peers, size, date posted, and a link for the webpage  Functional Requirements Torrent search will share the same search bar as the streaming search  of seeds and peers, the file size, the date posted, and a link to the webpage \" Results will be arranged in size/date/alphabetical order by clicking on the column headers  There will be a page button for the user to navigate the results\\nThere are list of entities\\nList of classes:  [\\'user\\', \\'search\\', \\'torrent websites\\', \\'Results\\', \\'query\\', \\'torrent\\', \\'tab\\', \\'program\\', \\'Torrent search\\', \\'search bar\\', \\'streaming search\\', \\'results\\']\\nList of attributes: [\\'Description\\', \\'Priority\\', \\'term\\', \\'compatible\\', \\'website\\', \\'seeds\\', \\'peers\\', \\'size\\', \\'date posted\\', \\'link\\', \\'file\\', \\'link to\\', \\'webpage\\', \\'date\\', \\'alphabetical order\\']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n'}]}]\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\R81_VideoSearch_class_diagram.puml\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': \"Given a description of software requirement:  Our rental stock includes a total of 5,780 vehicles including various types of trucks and trailers  We need to implement a system to track our rental agreements and our vehicle assignments  Each rental office rents vehicles that they have in stock to customers ready to take possession of the vehicle  The central office oversees the vehicle distribution, and directs transfers of vehicles from one rental office to another  Each rental office has an office name like “Littleton Right-Way”  Each office also has a unique three digit office number  We also keep each office’s address  Each office is a home office for some of our vehicles, and each vehicle is based out of a single home office  Each vehicle has a vehicle id, state of registration, and a license plate registration number  We have five different types of vehicles: 36 trucks, 24’ trucks, 10’ trucks, 8’ covered trailers, and 6’ open trailers  For all our vehicles, we need to track the last maintenance date, and expiration date of its registration  For our trucks, we need to know the current odometer reading, the gas tank capacity, and whether or not it has a working radio  Most of our rental agreements are for individual customers, but a rental agreement can either be for an individual or for a company  We assign each company an identifying company number and track the company’s name and address  Our corporate sales group handles all that information separately  For each individual customer, we record the customer’s name, home phone, address, and driver’s license state, number, and expiration date  We only allow a single individual or company for a given rental agreement, and we write a separate rental agreement for each vehicle  Each rental agreement is identified by the originating rental office number and a rental agreement number  We also need to track the rental date, the anticipated duration of the rental, the originating rental office, the drop-off rental office, the amount of the deposit paid, the quoted daily rental rate, and the quoted rate per mile\\nThere are list of entities\\nList of classes:  ['vehicles', 'system', 'rental agreements', 'vehicle', 'rental office', 'customers', 'central', 'office', 'home office', 'trucks', 'radio', 'rental agreement', 'company', 'sales', 'customer', '-']\\nList of attributes: ['rental stock', 'trailers', 'assignments', 'office', 'name', 'office number', 'address', 'vehicle id', 'state of registration', 'license plate', 'registration number', 'covered', 'last', 'maintenance date', 'expiration date', 'registration', 'odometer reading', 'gas tank capacity', 'identifying', 'company number', 'company', 'customer', 'home phone', 'driver', 's license', 'state', 'number', 'originating rental office number', 'rental agreement number', 'rental date', 'anticipated', 'duration', 'rental', 'originating rental office', 'drop', 'amount', 'deposit paid', 'quoted', 'daily rental rate', 'quoted rate per mile']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n\"}]}]\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\rental-truck-company_class_diagram.puml\n",
      "[{'role': 'user', 'content': [{'type': 'text', 'text': 'Given a description of software requirement:  Unless you are a celebrity or a good friend of Romano you will need a reservation  A reservation is made for a specific time, date and number of people  The reservation also captures the name and phone number of the person making the reservation  Each reservation is assigned a unique reservation number  There are two categories of reservations at Romano\\'s: individual reservations and banquet reservations  Additional reservation information captured when an individual makes a reservation includes seating preference (inside or patio) and smoking preference (smoking or nonsmoking)  Additional reservation information captured for banquet reservations includes the group name and the method of payment  Each table is identified by a unique table number  Each of the tables is further described by a unique free form description such as \"located by the North window\", \"located in front of the fountain\", \"by the kitchen door\"  Each table is classified as a 2-person, 4-person or 6-person table  When a reservation is made, Romano associates a specific number to the reservation  There are several restaurant managers who report to Romano  The managers are responsible for managing the Maitre\\'d and the chefs as well as ensuring that the guests have a pleasant dining experience  The Maitre\\'d is responsible for managing the waiters, bartenders and bus personnel  The Chefs are responsible for managing the cooks and dishwashers  Each person working for Romano\\'s must be classified as either a manager, Maitre\\'d, waiter, bartender, chef, cook, bus person or dishwasher  Additional information maintained by Romano\\'s for each person includes the persons name, date of birth and drivers license number  A waiter can be assigned to many reservations during the course of the evening\"  There are many exciting and exotic items  Each menu item is identified by a unique menu item number  Information maintained by Romano\\'s for each menu item includes an item description of (e ), and item prep time  Each menu item is classified by Romano\\'s as \"appetizer\", \"entree\", \"dessert\" or \"beverage\"  The price of each menu item can vary based on the time of day  For example, some of the menu items have different lunch and dinner prices  In order to calculate the check at the end of the dinner, the waiter maintains a list, by reservation number, of the menu items ordered and the time that the menu item was ordered  In other words, each reservation can be associated with many menu items and a menu item can be associated with many reservations  In addition to menu items, Romano\\'s maintains a list of the food items that are utilized by the restaurant such as chicken, mushrooms, bread sticks, red sauce, cream sauce, etc  Food items are utilized in the preparation of menu items  Each food item is identified by a unique food item number\\nThere are list of entities\\nList of classes:  [\\'reservation\\', \\'person\\', \\'reservations\\', \\'banquet reservations\\', \\'table\\', \\'tables\\', \\'fountain\\', \\'kitchen\\', \\'restaurant managers\\', \\'managers\\', \\'tre\\', \\'chefs\\', \\'guests\\', \\'Chefs\\', \\'cooks\\', \\'dishwashers\\', \\'manager\\', \\'chef\\', \\'dishwasher\\', \\'waiter\\', \\'menu\\', \\'menu item\\', \\'appe\\', \\'izer\\', \\'menu items\\', \\'restaurant\\', \\'food\\']\\nList of attributes: [\\'time\\', \\'date\\', \\'number of people\\', \\'name\\', \\'phone number\\', \\'reservation number\\', \\'categories\\', \\'reservation\\', \\'seating preference\\', \\'patio\\', \\'smoking preference\\', \\'group name\\', \\'method of payment\\', \\'table number\\', \\'number\\', \\'persons\\', \\'date of\\', \\'birth\\', \\'drivers license number\\', \\'item\\', \\'menu item number\\', \\'item description\\', \\'item prep time\\', \\'price\\', \\'menu items\\', \\'ordered\\', \\'food item number\\']\\n\\nGenerate a Class Diagram based on the description and entities with following factor:\\n-\\tprioritize using entities given\\n-\\tDiscover relationship between classes according to the description\\n-\\tAssign attributes to the classes according to the description\\n-   Since a class could be an attribute of another class, it is allowed when necessary\\nSet the output strictly to only PlantUML of the result diagram \\n'}]}]\n",
      "PlantUML diagram saved to ./plantuml_result_combine\\restaurant_class_diagram.puml\n",
      "Successfully extracted data from 0 files\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import glob\n",
    "import os \n",
    "\n",
    "output_dir = \"./class_diagrams_output\"\n",
    "# Get all JSON files from the output directory\n",
    "json_files = glob.glob(os.path.join(output_dir, \"*_entities.json\"))\n",
    "print(f\"Found {len(json_files)} JSON files\")\n",
    "\n",
    "# Parse each JSON file and extract the required information\n",
    "extracted_data = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        filename = os.path.basename(json_file).replace('_entities.json', '')\n",
    "        \n",
    "        # Extract document text, classes, and attributes\n",
    "        document = data.get('document_class', '')\n",
    "        classes = data.get('summary', {}).get('class', [])\n",
    "        attributes = data.get('summary', {}).get('attr', [])\n",
    "        \n",
    "        generate_class_diagram_with_azure(classes, attributes, document, filename)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {json_file}: {e}\")\n",
    "\n",
    "print(f\"Successfully extracted data from {len(extracted_data)} files\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
