{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e528e393",
   "metadata": {},
   "source": [
    "# Class Diagram Extraction Pipeline\n",
    "\n",
    "This notebook reads text documents from the target_text directory and processes them to extract class diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2a65797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from spacy import displacy\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee013dbe",
   "metadata": {},
   "source": [
    "# Reading Files from target_text Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6eeea468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory already exists: /work/pfsa-id/survey/class_diagrams_output_prosus\n",
      "Reading files from: /work/pfsa-id/survey/target_text\n",
      "Found 8 text file(s): ['R26_BlockCard.txt', 'R36_Video Rental.txt', 'R81_VideoSearch.txt', 'dental-clinic.txt', 'geological-samples-observation.txt', 'law-firm.txt', 'rental-truck-company.txt', 'restaurant.txt']\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for PlantUML files\n",
    "output_dir = os.path.join(os.path.dirname(os.getcwd()), \"survey/class_diagrams_output_prosus\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"Created output directory: {output_dir}\")\n",
    "else:\n",
    "    print(f\"Output directory already exists: {output_dir}\")\n",
    "\n",
    "# Define target directory path\n",
    "target_dir = os.path.join(os.path.dirname(os.getcwd()), \"survey/target_text\")\n",
    "print(f\"Reading files from: {target_dir}\")\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(target_dir):\n",
    "    print(f\"Error: Directory '{target_dir}' does not exist.\")\n",
    "    raise FileNotFoundError(f\"Directory '{target_dir}' does not exist.\")\n",
    "\n",
    "# Get list of text files from the directory\n",
    "text_files = [f for f in os.listdir(target_dir) if f.endswith('.txt')]\n",
    "\n",
    "if not text_files:\n",
    "    print(\"No text files found in the directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(text_files)} text file(s): {text_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "609e1f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI endpoint: https://dewi.openai.azure.com/\n",
      "Azure OpenAI deployment: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT_URL_1\", \"\")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME_1\", \"\")\n",
    "subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY_1\", \"\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION_1\")\n",
    "\n",
    "print(f\"Azure OpenAI endpoint: {endpoint}\")\n",
    "print(f\"Azure OpenAI deployment: {deployment}\")\n",
    "\n",
    "# Initialize Azure OpenAI Service client with key-based authentication\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6066cf",
   "metadata": {},
   "source": [
    "# Process Each Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bcdcdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(document_text, filename):\n",
    "    \"\"\"Preprocess the document text and split it into sentences\"\"\"\n",
    "    print(f\"Processing document: {filename}\")\n",
    "    \n",
    "    # Data preprocessing\n",
    "    print(\"Step 1: Preprocessing\")\n",
    "    # Remove newlines and extra spaces\n",
    "    document = re.sub(r'\\n', ' ', document_text)\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    \n",
    "    # Separate document by sentences into a dataframe\n",
    "    document_sentence = pd.DataFrame(document.split('.'), columns=['sentence'])\n",
    "    document_sentence = document_sentence[document_sentence['sentence'].str.strip().str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    return document_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4dd00cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model for sentence classification\n",
    "def load_sentence_classifier_model():\n",
    "    \"\"\"Load the pre-trained BERT model for sentence classification\"\"\"\n",
    "    model_dir = os.path.join(os.path.dirname(os.getcwd()), \"requirement_classification\", \"all_model\")\n",
    "    \n",
    "    # If multiple models exist, use the most recently created one\n",
    "    model_file = \"ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\"\n",
    "    model_path = os.path.join(model_dir, model_file)\n",
    "    print(f\"Loading sentence classifier model from: {model_path}\")\n",
    "    \n",
    "    # Define model name based on file name pattern\n",
    "    model_name = \"ProsusAI/finbert\"  # Default model base\n",
    "    \n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, ignore_mismatched_sizes=True)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    model.to(device)\n",
    "    \n",
    "    return model, tokenizer, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8b8ada9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextDataset class for sentence classification\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f814552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentences(document_sentence, filename, output_dir):\n",
    "    \"\"\"Classify sentences to identify those useful for class diagram extraction using BERT model\"\"\"\n",
    "    print(\"Step 2: Categorizing sentences using pre-trained model\")\n",
    "    document_sentence['useful'] = 0\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    try:\n",
    "        model, tokenizer, device = load_sentence_classifier_model()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading sentence classifier model: {e}\")\n",
    "        print(\"Falling back to default classification method\")\n",
    "        # You could implement a fallback method here\n",
    "        return document_sentence\n",
    "    \n",
    "    # Create dataset from sentences\n",
    "    sentences = document_sentence['sentence'].tolist()\n",
    "    dataset = TextDataset(sentences, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "    \n",
    "    # Process batches and get predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Classifying sentences\"):\n",
    "            # Move inputs to device\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            \n",
    "            # Get model outputs\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predicted class (0 = not useful, 1 = useful)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Update dataframe with predictions\n",
    "    document_sentence['useful'] = predictions\n",
    "    \n",
    "    # Log results\n",
    "    useful_count = sum(predictions)\n",
    "    print(f\"Found {useful_count} useful sentences out of {len(sentences)} total sentences\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_checkpoint.csv\")\n",
    "    document_sentence.to_csv(checkpoint_file)\n",
    "    print(f\"Saved checkpoint to {checkpoint_file}\")\n",
    "    \n",
    "    return document_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7356a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(document_sentence, filename, output_dir):\n",
    "    \"\"\"Extract entities from useful sentences\"\"\"\n",
    "    # Extract sentences marked as useful for class diagram extraction\n",
    "    print(\"Step 3: Extracting class diagram entities\")\n",
    "    sentence_class_diagram_only = document_sentence[document_sentence['useful'] == 1]\n",
    "    document_class = ' '.join(sentence_class_diagram_only['sentence'].tolist())\n",
    "    \n",
    "    # Entity extraction using the model\n",
    "    model_path = os.path.join(os.path.dirname(os.getcwd()), \"key-term-extraction\", \"BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\")\n",
    "    print(f\"Using NER model from: {model_path}\")\n",
    "    \n",
    "    try:\n",
    "        ner_pipeline = pipeline(\"ner\", model=model_path, aggregation_strategy=\"simple\")\n",
    "        entities = ner_pipeline(document_class)\n",
    "        \n",
    "        # Process entities\n",
    "        summary = {\n",
    "            \"class\": defaultdict(int),\n",
    "            \"attr\": defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        for ent in entities:\n",
    "            entity_type = ent[\"entity_group\"].lower()\n",
    "            word = ent[\"word\"]\n",
    "            \n",
    "            if entity_type in summary:\n",
    "                summary[entity_type][word] += 1\n",
    "        \n",
    "        # Convert defaultdict to normal dict\n",
    "        summary = {key: list(value.keys()) for key, value in summary.items()}\n",
    "        \n",
    "        # Save entity data as CSV\n",
    "        entity_csv_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.csv\")\n",
    "        pd.DataFrame(entities).to_csv(entity_csv_file)\n",
    "        print(f\"Entities saved to CSV: {entity_csv_file}\")\n",
    "        \n",
    "        # Convert entities to JSON serializable format\n",
    "        serializable_entities = []\n",
    "        for ent in entities:\n",
    "            # Extract only serializable properties and convert non-serializable types\n",
    "            serializable_ent = {\n",
    "                \"entity_group\": ent[\"entity_group\"],\n",
    "                \"word\": ent[\"word\"],\n",
    "                \"score\": float(ent[\"score\"]),  # Convert tensor to float if needed\n",
    "                \"start\": ent[\"start\"],\n",
    "                \"end\": ent[\"end\"]\n",
    "            }\n",
    "            serializable_entities.append(serializable_ent)\n",
    "        \n",
    "        # Save entity data as JSON\n",
    "        import json\n",
    "        entity_json_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.json\")\n",
    "        with open(entity_json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"entities\": serializable_entities,  # Use serializable entities\n",
    "                \"summary\": summary,\n",
    "                \"document_class\": document_class\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Entities saved to JSON: {entity_json_file}\")\n",
    "        \n",
    "        # Save entity data as plain text\n",
    "        entity_txt_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_entities.txt\")\n",
    "        with open(entity_txt_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Document: {filename}\\n\\n\")\n",
    "            f.write(f\"Classes identified:\\n{'-'*20}\\n\")\n",
    "            for cls in summary['class']:\n",
    "                f.write(f\"- {cls}\\n\")\n",
    "            f.write(f\"\\nAttributes identified:\\n{'-'*20}\\n\")\n",
    "            for attr in summary['attr']:\n",
    "                f.write(f\"- {attr}\\n\")\n",
    "            f.write(f\"\\nDetailed Entities:\\n{'-'*20}\\n\")\n",
    "            for ent in serializable_entities:  # Use serializable entities\n",
    "                f.write(f\"Type: {ent['entity_group']}, Word: {ent['word']}, Score: {ent['score']:.4f}\\n\")\n",
    "        print(f\"Entities saved to TXT: {entity_txt_file}\")\n",
    "        \n",
    "        # Return serializable entities for further processing\n",
    "        return {\n",
    "            \"entities\": serializable_entities,  # Use serializable entities\n",
    "            \"summary\": summary,\n",
    "            \"document_class\": document_class\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5be1265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diagram(extraction_result, filename, output_dir):\n",
    "    \"\"\"Generate class diagram from extracted entities\"\"\"\n",
    "    print(\"Step 4: Generating PlantUML code\")\n",
    "    \n",
    "    # Check if we have valid extraction results\n",
    "    if \"error\" in extraction_result:\n",
    "        print(f\"Cannot generate diagram due to extraction error: {extraction_result['error']}\")\n",
    "        return {\"filename\": filename, \"error\": extraction_result['error']}\n",
    "    \n",
    "    try:\n",
    "        # Prepare summary for diagram generation\n",
    "        summary = extraction_result[\"summary\"]\n",
    "        document_class = extraction_result[\"document_class\"]\n",
    "        summary_string = f\"class: {summary['class']}, attribute: {summary['attr']}, description: {document_class}\"\n",
    "        \n",
    "        # Generate PlantUML using Azure OpenAI\n",
    "        chat_prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You will be given a JSON of class names, attributes, and a system description. Your task is to generate plantuml script containing classes, attributes, and relationships according to the system description. Strictly produce only plantuml script\"\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": summary_string\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment,\n",
    "            messages=chat_prompt,\n",
    "            max_tokens=800,\n",
    "            temperature=0,\n",
    "            top_p=0.95,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            stream=False\n",
    "        )\n",
    "        plantuml_result = completion.choices[0].message.content\n",
    "        \n",
    "        # Clean up the result and save to file\n",
    "        plantuml_result = plantuml_result.strip('```plantuml')\n",
    "        plantuml_result = plantuml_result.strip('```')\n",
    "        \n",
    "        output_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_class_diagram.puml\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(plantuml_result)\n",
    "        print(f\"PlantUML diagram saved to {output_file}\")\n",
    "        \n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"entities\": extraction_result[\"entities\"],\n",
    "            \"plantuml_code\": plantuml_result,\n",
    "            \"output_file\": output_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating diagram: {e}\")\n",
    "        return {\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f803e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(document_text, filename):\n",
    "    \"\"\"Process a document through the entire pipeline\"\"\"\n",
    "    # Step 1: Preprocess document\n",
    "    document_sentence = preprocess_document(document_text, filename)\n",
    "    \n",
    "    # Step 2: Classify sentences\n",
    "    document_sentence = classify_sentences(document_sentence, filename, output_dir)\n",
    "    \n",
    "    # Step 3: Extract entities\n",
    "    extraction_result = extract_entities(document_sentence, filename, output_dir)\n",
    "\n",
    "    return \"true \" + filename\n",
    "    # # Step 4: Generate diagram\n",
    "    # diagram_result = generate_diagram(extraction_result, filename, output_dir)\n",
    "    \n",
    "    # return diagram_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54ecd44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing file: R26_BlockCard.txt\n",
      "==================================================\n",
      "Processing document: R26_BlockCard.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 3/3 [00:00<00:00, 37.31it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 useful sentences out of 19 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/R26_BlockCard_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/R26_BlockCard_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/R26_BlockCard_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/R26_BlockCard_entities.txt\n",
      "\n",
      "==================================================\n",
      "Processing file: R36_Video Rental.txt\n",
      "==================================================\n",
      "Processing document: R36_Video Rental.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 6/6 [00:00<00:00, 70.47it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 47 useful sentences out of 48 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/R36_Video Rental_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/R36_Video Rental_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/R36_Video Rental_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/R36_Video Rental_entities.txt\n",
      "\n",
      "==================================================\n",
      "Processing file: R81_VideoSearch.txt\n",
      "==================================================\n",
      "Processing document: R81_VideoSearch.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 2/2 [00:00<00:00, 56.82it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 useful sentences out of 16 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/R81_VideoSearch_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/R81_VideoSearch_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/R81_VideoSearch_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/R81_VideoSearch_entities.txt\n",
      "\n",
      "==================================================\n",
      "Processing file: dental-clinic.txt\n",
      "==================================================\n",
      "Processing document: dental-clinic.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 3/3 [00:00<00:00, 77.37it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 useful sentences out of 20 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/dental-clinic_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/dental-clinic_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/dental-clinic_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/dental-clinic_entities.txt\n",
      "\n",
      "==================================================\n",
      "Processing file: geological-samples-observation.txt\n",
      "==================================================\n",
      "Processing document: geological-samples-observation.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 3/3 [00:00<00:00, 70.45it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 useful sentences out of 17 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/geological-samples-observation_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/geological-samples-observation_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/geological-samples-observation_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/geological-samples-observation_entities.txt\n",
      "\n",
      "==================================================\n",
      "Processing file: law-firm.txt\n",
      "==================================================\n",
      "Processing document: law-firm.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 4/4 [00:00<00:00, 67.98it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 useful sentences out of 26 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/law-firm_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/law-firm_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/law-firm_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/law-firm_entities.txt\n",
      "\n",
      "==================================================\n",
      "Processing file: rental-truck-company.txt\n",
      "==================================================\n",
      "Processing document: rental-truck-company.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 4/4 [00:00<00:00, 56.92it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 useful sentences out of 32 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/rental-truck-company_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/rental-truck-company_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/rental-truck-company_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/rental-truck-company_entities.txt\n",
      "\n",
      "==================================================\n",
      "Processing file: restaurant.txt\n",
      "==================================================\n",
      "Processing document: restaurant.txt\n",
      "Step 1: Preprocessing\n",
      "Step 2: Categorizing sentences using pre-trained model\n",
      "Loading sentence classifier model from: /work/pfsa-id/requirement_classification/all_model/ProsusAI-finbert_structure_focus_epochs9_kfold10_batch8.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Classifying sentences: 100%|██████████| 6/6 [00:00<00:00, 79.36it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 useful sentences out of 42 total sentences\n",
      "Saved checkpoint to /work/pfsa-id/survey/class_diagrams_output_prosus/restaurant_checkpoint.csv\n",
      "Step 3: Extracting class diagram entities\n",
      "Using NER model from: /work/pfsa-id/key-term-extraction/BERT-Style-model/microsoft/deberta-v3-large-4-epoch-8-bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to CSV: /work/pfsa-id/survey/class_diagrams_output_prosus/restaurant_entities.csv\n",
      "Entities saved to JSON: /work/pfsa-id/survey/class_diagrams_output_prosus/restaurant_entities.json\n",
      "Entities saved to TXT: /work/pfsa-id/survey/class_diagrams_output_prosus/restaurant_entities.txt\n",
      "\n",
      "Processing complete!\n",
      "Processed 8 files.\n",
      "Output saved to: /work/pfsa-id/survey/class_diagrams_output_prosus\n"
     ]
    }
   ],
   "source": [
    "# Process all files in the target directory\n",
    "results = []\n",
    "\n",
    "for filename in text_files:\n",
    "    file_path = os.path.join(target_dir, filename)\n",
    "    print(f\"\\n{'='*50}\\nProcessing file: {filename}\\n{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            document_text = file.read()\n",
    "            \n",
    "            result = process_document(document_text, filename)\n",
    "            results.append(result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filename}: {e}\")\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        \n",
    "print(\"\\nProcessing complete!\")\n",
    "print(f\"Processed {len(results)} files.\")\n",
    "print(f\"Output saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491cb9a",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "02b633d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize entities for a selected file\n",
    "def visualize_entities(file_index=0):\n",
    "    if file_index < 0 or file_index >= len(results) or 'entities' not in results[file_index]:\n",
    "        print(\"Invalid file index or no entities found for this file.\")\n",
    "        return\n",
    "    \n",
    "    filename = results[file_index]['filename']\n",
    "    entities = results[file_index]['entities']\n",
    "    \n",
    "    # Read file content again\n",
    "    file_path = os.path.join(target_dir, filename)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        document_text = file.read()\n",
    "    \n",
    "    # Preprocess text\n",
    "    document = re.sub(r'\\n', ' ', document_text)\n",
    "    document = re.sub(r'\\s+', ' ', document)\n",
    "    \n",
    "    # Read checkpoint file to get useful sentences\n",
    "    checkpoint_file = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_checkpoint.csv\")\n",
    "    document_sentence = pd.read_csv(checkpoint_file)\n",
    "    sentence_class_diagram_only = document_sentence[document_sentence['useful'] == 1]\n",
    "    document_class = ' '.join(sentence_class_diagram_only['sentence'].tolist())\n",
    "    \n",
    "    # Visualize entities\n",
    "    colors = {\n",
    "        \"CLASS\": \"#ffff00\",\n",
    "        \"ATTR\": \"#9932cc\",\n",
    "    }\n",
    "    options = {\"ents\": [\"CLASS\", \"ATTR\"], \"colors\": colors}\n",
    "    \n",
    "    # Convert to spaCy format\n",
    "    spacy_ents = {\n",
    "        \"text\": document_class,\n",
    "        \"ents\": [\n",
    "            {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "            for ent in entities\n",
    "        ],\n",
    "        \"title\": f\"Named Entity Recognition - {filename}\",\n",
    "    }\n",
    "    \n",
    "    # Render the visualization\n",
    "    displacy.render(spacy_ents, style=\"ent\", manual=True, jupyter=True, options=options)\n",
    "    \n",
    "    print(f\"PlantUML file: {results[file_index]['output_file']}\")\n",
    "    \n",
    "# Example usage: visualize_entities(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ad766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Display PlantUML code for a selected file\n",
    "def show_plantuml(file_index=0):\n",
    "    if file_index < 0 or file_index >= len(results) or 'plantuml_code' not in results[file_index]:\n",
    "        print(\"Invalid file index or no PlantUML code found for this file.\")\n",
    "        return\n",
    "    \n",
    "    filename = results[file_index]['filename']\n",
    "    plantuml_code = results[file_index]['plantuml_code']\n",
    "    \n",
    "    print(f\"PlantUML code for {filename}:\\n\")\n",
    "    print(plantuml_code)\n",
    "    \n",
    "# Example usage: show_plantuml(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
